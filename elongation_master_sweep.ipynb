{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Master File w/ hyperparameter sweeping across multiple architectures\"\"\"\n",
    "\"\"\"\n",
    "Restart kernel after running\n",
    "Only need to run once\n",
    "\"\"\"\n",
    "!pip install scikit-learn matplotlib seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mhassett\u001b[0m (\u001b[33melongation-net\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  seqnames     start       end strand  ensembl_gene_id  score      ctcf  \\\n",
      "0       15  88623545  88623545      +  ENSG00000181026    0.0 -0.079992   \n",
      "1       15  88623546  88623546      +  ENSG00000181026    0.0 -0.079942   \n",
      "2       15  88623547  88623547      +  ENSG00000181026    0.0 -0.079893   \n",
      "3       15  88623548  88623548      +  ENSG00000181026    0.0 -0.079844   \n",
      "4       15  88623549  88623549      +  ENSG00000181026    0.0 -0.079796   \n",
      "\n",
      "   h3k36me3   h3k4me1  h3k79me2   h3k9me1   h3k9me3  h4k20me1       sj5  \\\n",
      "0 -0.000099  0.348531  4.423451  0.446508 -0.168099  3.232475 -0.028916   \n",
      "1  0.001638  0.352677  4.460072  0.453024 -0.169218  3.259194 -0.028916   \n",
      "2  0.003360  0.356807  4.496664  0.459491 -0.170339  3.285849 -0.028916   \n",
      "3  0.005065  0.360919  4.533223  0.465908 -0.171461  3.312435 -0.028916   \n",
      "4  0.006754  0.365013  4.569743  0.472274 -0.172584  3.338952 -0.028916   \n",
      "\n",
      "        sj3       dms  wgbs      rpts  lambda_alphaj      zeta  \n",
      "0 -0.057178 -0.307549   0.0  0.249626       0.052255  0.993283  \n",
      "1 -0.057178 -0.307549   0.0  0.249626       0.052255  0.992642  \n",
      "2 -0.057178 -0.307549   0.0  0.249626       0.052255  0.992008  \n",
      "3 -0.057178 -0.307549   0.0  0.249626       0.052255  0.991381  \n",
      "4 -0.057178 -0.307549   0.0  0.249626       0.052255  0.990762  \n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.utils.data as td\n",
    "import torch.nn as nn\n",
    "import torch.nn.init as init\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torch.utils.data import DataLoader, Dataset, Subset\n",
    "import wandb\n",
    "\n",
    "froot = './data/k562_samp_epft_norm_test_1.csv'\n",
    "df = pd.read_csv(froot)\n",
    "\n",
    "wandb.login()\n",
    "\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_names = np.array(df.columns)\n",
    "feature_names = column_names[6:-2]\n",
    "num_features = len(feature_names)\n",
    "print(feature_names)\n",
    "num_samples = df.shape[0]\n",
    "\n",
    "# process read counts per gene j, site i\n",
    "X_ji = df['score'].values\n",
    "\n",
    "# process GLM simulated elongation rates\n",
    "Z_ji = df['zeta'].values\n",
    "\n",
    "print(\"Number of Samples: \" + str(num_samples))\n",
    "print(\"Number of Features: \" + str(num_features))\n",
    "\n",
    "#Y_ji is a list of samples containing lists of their feature values\n",
    "    # [   \n",
    "    #   sample_1: [feat_1, feat_2,...,feat_n],\n",
    "    #   sample_2: [feat_1, feat_2,...,feat_n],\n",
    "    # ]\n",
    "\n",
    "Y_ji = df.iloc[:, 6:-2].values\n",
    "Y_ji_shape = Y_ji.shape\n",
    "print(Y_ji.shape)\n",
    "\n",
    "# read depth * initiation rate values per gene j\n",
    "C_j = df['lambda_alphaj'].values\n",
    "\n",
    "gene_ids = df['ensembl_gene_id'].values\n",
    "\n",
    "cuda_available = torch.cuda.is_available()\n",
    "print(\"CUDA (GPU support) is available:\", cuda_available)\n",
    "num_gpus = torch.cuda.device_count()\n",
    "print(\"Number of GPUs available:\", num_gpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sweep_config = {\n",
    "    'method': 'grid'\n",
    "}\n",
    "metric = {\n",
    "    'name': 'valid_neural_net_loss',\n",
    "    'goal': 'minimize'   \n",
    "    }\n",
    "\n",
    "sweep_config['metric'] = metric\n",
    "\n",
    "parameters_dict = {\n",
    "    'optimizer': {\n",
    "        'values': ['adam']\n",
    "    },\n",
    "    'learn_rate': {\n",
    "        'values': [1e-3]\n",
    "    },\n",
    "    'momentum': {\n",
    "        'values': [0]#, 0.9]\n",
    "    },\n",
    "    'train_batch_size': {\n",
    "        'values': [16000000]\n",
    "    },\n",
    "    'train_use_sliding_window': {\n",
    "        'values': [False]#, True]\n",
    "    },\n",
    "    'train_window_size': {\n",
    "        'values': [500]\n",
    "    },\n",
    "    'train_stride': {\n",
    "        'values': [500]\n",
    "    },\n",
    "    'val_batch_size': {\n",
    "        'values': [16000000]\n",
    "    },\n",
    "    'val_use_sliding_window': {\n",
    "        'values': [False]#, True]\n",
    "    },\n",
    "    'val_window_size': {\n",
    "        'values': [500]\n",
    "    },\n",
    "    'val_stride': {\n",
    "        'values': [500]\n",
    "    },\n",
    "    'model_type': {\n",
    "        'values': ['linear', 'lstm']\n",
    "    },\n",
    "    'bidirectional': {\n",
    "        'values': [False]#, True]\n",
    "    },\n",
    "    'hidden_layer_size': {\n",
    "        'values': [100]#, 50, 200, 500]\n",
    "    },\n",
    "    'num_layers': {\n",
    "        'values': [1]#, 2]\n",
    "    },\n",
    "    'weight_init': {\n",
    "        'values': [None]#, 'zero']\n",
    "    }\n",
    "}\n",
    "\n",
    "parameters_dict.update({\n",
    "    'epochs': {\n",
    "        'value': 20}\n",
    "    })\n",
    "\n",
    "sweep_config['parameters'] = parameters_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sweep_id = wandb.sweep(sweep_config, project=\"elongation-net\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GeneDataset(Dataset):\n",
    "    def __init__(self, grouped_data, use_sliding_window=False, window_size=None, stride=None):\n",
    "        self.grouped_data = grouped_data\n",
    "        self.use_sliding_window = use_sliding_window\n",
    "        self.window_size = window_size\n",
    "        self.stride = stride\n",
    "        self.segments = []\n",
    "\n",
    "        if self.use_sliding_window and window_size is not None and stride is not None:\n",
    "            self._create_segments()\n",
    "        else:\n",
    "            self._prepare_full_genes()\n",
    "        \n",
    "    def _create_segments(self):\n",
    "        for gene_id, group in self.grouped_data:\n",
    "            gene_length = len(group)\n",
    "            for start_idx in range(0, gene_length - self.window_size + 1, self.stride):\n",
    "                end_idx = start_idx + self.window_size\n",
    "                segment = group.iloc[start_idx:end_idx]\n",
    "                self.segments.append((gene_id, segment))\n",
    "    \n",
    "    def _prepare_full_genes(self):\n",
    "        for gene_id, group in self.grouped_data:\n",
    "            self.segments.append((gene_id, group))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.segments)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        gene_id, segment = self.segments[idx]\n",
    "        \n",
    "        y_ji_array = np.array(segment['Y_ji'].tolist()).reshape(-1, 12)\n",
    "        y_ji_tensor = torch.tensor(y_ji_array, dtype=torch.float64)\n",
    "        \n",
    "        data = segment.drop(columns=['GeneId', 'dataset', 'Y_ji'])\n",
    "        tensor_data = torch.tensor(data.values, dtype=torch.float64)\n",
    "        \n",
    "        result = {\n",
    "            'GeneId': gene_id,\n",
    "            'Y_ji': y_ji_tensor,\n",
    "            'gene_length': len(segment)\n",
    "        }\n",
    "        for col in data.columns:\n",
    "            result[col] = tensor_data[:, data.columns.get_loc(col)]\n",
    "\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data = pd.DataFrame({\n",
    "    'GeneId': gene_ids,\n",
    "    'Y_ji': [row for row in Y_ji],\n",
    "    'X_ji': X_ji,\n",
    "    'C_j': C_j,\n",
    "    'Z_ji': Z_ji\n",
    "})\n",
    "\n",
    "grouped = data.groupby('GeneId')\n",
    "\n",
    "# split by gene into train, val, test sets\n",
    "train_idx, temp_idx = train_test_split(list(grouped.groups.keys()), test_size=0.2, random_state=42)\n",
    "val_idx, test_idx = train_test_split(temp_idx, test_size=0.5, random_state=42)\n",
    "\n",
    "# create dictionary mapping each gene id to its assigned train, val, test dataset labels\n",
    "dataset_mapping = {gene_id: 'train' for gene_id in train_idx}\n",
    "dataset_mapping.update({gene_id: 'val' for gene_id in val_idx})\n",
    "dataset_mapping.update({gene_id: 'test' for gene_id in test_idx})\n",
    "\n",
    "# filter rows based on assigned dataset field\n",
    "data['dataset'] = data['GeneId'].map(dataset_mapping)\n",
    "train_data = data[data['dataset'] == 'train']\n",
    "valid_data = data[data['dataset'] == 'val']\n",
    "test_data = data[data['dataset'] == 'test']\n",
    "\n",
    "\n",
    "print(\"train data size: \" + str(len(train_data)))\n",
    "print(\"val data size: \" + str(len(valid_data)))\n",
    "print(\"test data size: \" + str(len(test_data)))\n",
    "\n",
    "train_data = train_data.groupby('GeneId')\n",
    "valid_data = valid_data.groupby('GeneId')\n",
    "test_data = test_data.groupby('GeneId')\n",
    "print(\"train # genes: \" + str(len(train_data)))\n",
    "print(\"val # genes: \" + str(len(valid_data)))\n",
    "print(\"test # genes: \" + str(len(test_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dataset(train_data, batch_size, use_sliding_window=False, window_size=None, stride=None):\n",
    "    dataset = GeneDataset(train_data, use_sliding_window, window_size, stride)\n",
    "    loader = DataLoader(dataset, batch_size=batch_size, num_workers=7, shuffle=False, pin_memory=True)\n",
    "    return loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def build_model(model_type, num_layers, hidden_layer_size, bidirectional, weight_init):\n",
    "    \n",
    "    class LSTMModel(nn.Module):\n",
    "        def __init__(self, input_size, hidden_layer_size, output_size, num_layers, bidirectional):\n",
    "            super(LSTMModel, self).__init__()\n",
    "            self.lstm = nn.LSTM(input_size, hidden_layer_size, num_layers, bidirectional=bidirectional, batch_first=True)\n",
    "            self.bidirectional_linear = nn.Linear(2 * hidden_layer_size, output_size)\n",
    "            self.linear = nn.Linear(hidden_layer_size, output_size)\n",
    "\n",
    "        def forward(self, x):\n",
    "            x, _ = self.lstm(x)\n",
    "            if self.bidirectional:\n",
    "                x = self.bidirectional_linear(x)\n",
    "            else:\n",
    "                x = self.linear(x)\n",
    "            return x\n",
    "    \n",
    "    if model_type == 'lstm':\n",
    "        model = LSTMModel(num_features, hidden_layer_size, 1, num_layers, bidirectional)\n",
    "    elif model_type == 'linear':\n",
    "        model = nn.Linear(num_features, 1, bias=False)\n",
    "    \n",
    "    if cuda_available:\n",
    "        if num_gpus > 1:\n",
    "            print(\"Using\", num_gpus, \"GPUs\")\n",
    "            model = torch.nn.DataParallel(model)\n",
    "        model = model.to('cuda')\n",
    "\n",
    "    print(model)\n",
    "\n",
    "    first_param_device = next(model.parameters()).device\n",
    "    print(\"Model is on device:\", first_param_device)\n",
    "    \n",
    "    # expected weights are close to 0 which is why 0 initializing weights converges much quicker\n",
    "    if weight_init == 'zero':\n",
    "        with torch.no_grad():\n",
    "            for param in model.parameters():\n",
    "                param.zero_()\n",
    "    \n",
    "    model.double()\n",
    "\n",
    "    return model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_optimizer(network, optimizer, learning_rate, momentum):\n",
    "    if optimizer == \"sgd\":\n",
    "        optimizer = optim.SGD(network.parameters(),\n",
    "                              lr=learning_rate, momentum=momentum)\n",
    "        \n",
    "    # Adam optimizer adapts the learning rate for each parameter individually\n",
    "    elif optimizer == \"adam\":\n",
    "        optimizer = optim.Adam(network.parameters(),\n",
    "                               lr=learning_rate)\n",
    "    return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def valid_epoch(model, loader, loss_fn):\n",
    "    model.eval()\n",
    "    total_neural_net_loss = 0\n",
    "    total_glm_loss = 0\n",
    "    neural_net_zeta = []\n",
    "    glm_zeta = []\n",
    "    with torch.no_grad():\n",
    "        for idx, batch in enumerate(loader):\n",
    "            Y_ji_batch = batch['Y_ji'].to(device)\n",
    "            X_ji_batch = batch['X_ji'].to(device)\n",
    "            C_j_batch = batch['C_j'].to(device)\n",
    "            Z_ji_batch = batch['Z_ji'].to(device)\n",
    "            lengths = batch['gene_length'].to(device)\n",
    "            \n",
    "            outputs = model(Y_ji_batch)\n",
    "            neural_net_loss = loss_fn(X_ji_batch, C_j_batch, outputs.squeeze(2), lengths)\n",
    "            glm_loss = loss_fn(X_ji_batch, C_j_batch, Z_ji_batch, lengths)\n",
    "\n",
    "            total_neural_net_loss +=  neural_net_loss.item()\n",
    "            total_glm_loss += glm_loss.item()\n",
    "            \n",
    "            # store all predictions in list\n",
    "            neural_net_zeta.append(torch.exp(outputs.cpu()[0]))\n",
    "            glm_zeta.append(batch['Z_ji'][0])\n",
    "    \n",
    "    # calculate average loss across all batches\n",
    "    avg_neural_net_loss = total_neural_net_loss / len(loader)\n",
    "    avg_glm_loss = total_glm_loss / len(loader)\n",
    "    \n",
    "    neural_net_zeta = torch.cat(neural_net_zeta, dim=0)\n",
    "    glm_zeta = torch.cat(glm_zeta, dim=0)\n",
    "    \n",
    "    return avg_neural_net_loss, avg_glm_loss, neural_net_zeta, glm_zeta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, loader, optimizer, loss_fn):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for idx, batch in enumerate(loader):\n",
    "        optimizer.zero_grad()\n",
    "        Y_ji_batch = batch['Y_ji'].to(device)\n",
    "        X_ji_batch = batch['X_ji'].to(device)\n",
    "        C_j_batch = batch['C_j'].to(device)\n",
    "        lengths = batch['gene_length'].to(device)\n",
    "        \n",
    "        outputs = model(Y_ji_batch)\n",
    "        loss = loss_fn(X_ji_batch, C_j_batch, outputs.squeeze(2), lengths)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # calculate average loss across all batches\n",
    "        total_loss += loss.item()\n",
    "    avg_train_loss = total_loss / len(loader)\n",
    "    \n",
    "    return avg_train_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CustomLoss, self).__init__()\n",
    "\n",
    "    def forward(self, X_ji, C_j, rho_ji, lengths):\n",
    "        C_j_value = C_j[0]\n",
    "        loss = X_ji * rho_ji + C_j_value * torch.exp(-rho_ji) - X_ji * torch.log(C_j_value)\n",
    "        \n",
    "        # normalize loss by sequence length\n",
    "        loss_sum = loss.sum(dim=1)\n",
    "        normalized_loss = loss_sum / lengths.float()\n",
    "        \n",
    "        # calculate average loss within each batch\n",
    "        return (normalized_loss).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "increase_cut=1\n",
    "learning_rate_decreased = False\n",
    "\n",
    "def train(config=None):\n",
    "    with wandb.init(config=config):\n",
    "        config = wandb.config\n",
    "        model = build_model(config.model_type, config.num_layers, config.hidden_layer_size, \n",
    "                            config.bidirectional, config.weight_init)\n",
    "        train_loader = build_dataset(train_data, config.train_batch_size, config.train_use_sliding_window, \n",
    "                                     config.train_window_size, config.train_stride)\n",
    "        valid_loader = build_dataset(valid_data, config.val_batch_size, config.val_use_sliding_window, \n",
    "                                     config.val_window_size, config.val_stride)\n",
    "        optimizer = build_optimizer(model, config.optimizer, config.learn_rate, config.momentum)\n",
    "        \n",
    "        loss_fn = CustomLoss()\n",
    "        loss_neural_net_train = [0] * epochs\n",
    "        loss_neural_net_valid = [0] * epochs\n",
    "        loss_glm_valid = [0] * epochs\n",
    "        \n",
    "        # scheduler to reduce learning rate by half when new validation loss > old validation loss\n",
    "        old_neural_net_valid_loss = float('inf')\n",
    "        scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=1, verbose=True)\n",
    "\n",
    "        for epoch in range(config.epochs):\n",
    "            print(f'Epoch {epoch+1}')\n",
    "            \n",
    "            train_loss = train_epoch(model, train_loader, optimizer, loss_fn)\n",
    "            loss_neural_net_train[epoch] = train_loss\n",
    "            print(\"train loss: \"+ str(train_loss))\n",
    "            \n",
    "            valid_neural_net_loss, valid_glm_loss, neural_net_zeta, glm_zeta = valid_epoch(model, valid_loader, loss_fn)\n",
    "            loss_neural_net_valid[epoch] = valid_neural_net_loss\n",
    "            loss_glm_valid[epoch] = valid_glm_loss\n",
    "            print(\"valid neural net loss: \"+ str(valid_neural_net_loss))\n",
    "            print(\"valid glm loss: \"+ str(valid_glm_loss))\n",
    "            \n",
    "            # calculate metrics\n",
    "            mae = F.l1_loss(net_zeta.squeeze(), glm_zeta)\n",
    "            mse = F.mse_loss(net_zeta.squeeze(), glm_zeta)\n",
    "            correlation_coefficient = np.corrcoef(glm_zeta, net_zeta.squeeze())[0, 1]\n",
    "            print(\"Correlation Coefficient:\", correlation_coefficient)\n",
    "            print(f\"Mean Absolute Error: {mae.item():.4f}\")\n",
    "            print(f\"Mean Squared Error: {mse.item():.4f}\")\n",
    "            \n",
    "            \n",
    "            wandb.log({\"epoch\": epoch, \"train_loss\": train_loss, \"valid_neural_net_loss\": valid_neural_net_loss,\n",
    "                       \"valid_glm_loss\": valid_glm_loss, \"corr_coeff\": correlation_coefficient, \"mae\": mae.item(), \n",
    "                       \"mse\": mse.item()})\n",
    "            \n",
    "            # early stopping if loss is not improving after reducing learning rate\n",
    "            if learning_rate_decreased and valid_neural_net_loss - old_neural_net_valid_loss < increase_cut:\n",
    "                break\n",
    "                \n",
    "            # reduce learning rate if new loss > old loss\n",
    "            learning_rate_decreased = False\n",
    "            if valid_neural_net_loss > old_neural_net_valid_loss:\n",
    "                optimizer.param_groups[0]['lr'] *= 0.5\n",
    "                print(f\"Reduced learning rate to {optimizer.param_groups[0]['lr']}\")\n",
    "                learning_rate_decreased=True\n",
    "            old_train_loss = train_loss\n",
    "            scheduler.step(train_loss)\n",
    "            \n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.agent(sweep_id, train)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Anaconda3 (Python 3.7.6)",
   "language": "python",
   "name": "anaconda3_2020.02"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
