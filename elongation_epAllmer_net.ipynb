{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Testing/Debugging File \"\"\"\n",
    "\n",
    "\"\"\"\n",
    "Restart kernel after running\n",
    "Only need to run once\n",
    "\"\"\"\n",
    "!pip install scikit-learn matplotlib seaborn pybigwig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "cell_type = \"k562\"\n",
    "config_file_name = \"cnn_2\"\n",
    "use_wandb=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Configure path to config file and select whether hyperparameter sweeping or not \"\"\"\n",
    "config_folder_path = \"./configs/\"\n",
    "results_folder_path = \"./results/\"\n",
    "sweep_config = {\n",
    "    'method': 'grid'\n",
    "}\n",
    "metric = {\n",
    "    'name': 'valid_neural_net_loss',\n",
    "    'goal': 'minimize'   \n",
    "}\n",
    "sweep_config['metric'] = metric\n",
    "\n",
    "input_data_file = f'./data/{cell_type}_datasets.pkl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "import math \n",
    "import json\n",
    "import pyBigWig\n",
    "import wandb\n",
    "from datetime import datetime\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.init as init\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torch.utils.data import DataLoader, Dataset, BatchSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seqnames                         1\n",
      "start                      1002760\n",
      "end                        1002760\n",
      "strand                           +\n",
      "ensembl_gene_id    ENSG00000187608\n",
      "score                          0.0\n",
      "ctcf                      -0.07771\n",
      "h4k20me1                 -0.429997\n",
      "h3k79me2                   -0.2804\n",
      "h3k4me1                  -0.217665\n",
      "h3k9me3                  -0.333359\n",
      "h3k36me3                 -0.801406\n",
      "sj5                      -0.039619\n",
      "sj3                      -0.059131\n",
      "rpts                     -0.187111\n",
      "wgbs                           0.0\n",
      "lambda_alphaj             0.026377\n",
      "zeta                      1.133344\n",
      "A                                0\n",
      "T                                0\n",
      "G                                1\n",
      "C                                0\n",
      "combined_zeta             1.020207\n",
      "dataset                      train\n",
      "Name: 0, dtype: object\n",
      "['ctcf' 'h4k20me1' 'h3k79me2' 'h3k4me1' 'h3k9me3' 'h3k36me3' 'sj5' 'sj3'\n",
      " 'rpts' 'wgbs']\n",
      "Number of Samples: 136927782\n",
      "Number of Features: 10\n",
      "CUDA (GPU support) is available: False\n",
      "Number of GPUs available: 0\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Load datasets \"\"\"\n",
    "with open(input_data_file, 'rb') as file:\n",
    "    combined_datasets = pickle.load(file)\n",
    "    \n",
    "nucleotides = ['A', 'T', 'G', 'C']\n",
    "\n",
    "train_data = combined_datasets['train']\n",
    "valid_data = combined_datasets['valid']\n",
    "test_data = combined_datasets['test']\n",
    "\n",
    "print(train_data.iloc[0])\n",
    "\n",
    "column_names = np.array(train_data.columns)\n",
    "feature_names = column_names[6:16]\n",
    "num_features = len(feature_names)\n",
    "print(feature_names)\n",
    "num_samples = train_data.shape[0]\n",
    "num_seq_features = len(nucleotides)\n",
    "\n",
    "print(\"Number of Samples: \" + str(num_samples))\n",
    "print(\"Number of Features: \" + str(num_features))\n",
    "\n",
    "cuda_available = torch.cuda.is_available()\n",
    "print(\"CUDA (GPU support) is available:\", cuda_available)\n",
    "num_gpus = torch.cuda.device_count()\n",
    "print(\"Number of GPUs available:\", num_gpus)\n",
    "\n",
    "torch.backends.cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Process data using a sliding window approach \"\"\"\n",
    "class GeneDataset(Dataset):\n",
    "    def __init__(self, dataframe, use_sliding_window=False, window_size=100):\n",
    "        self.dataframe = dataframe\n",
    "        self.grouped_data = dataframe.groupby('ensembl_gene_id')\n",
    "        self.use_sliding_window = use_sliding_window\n",
    "        self.window_size = window_size\n",
    "        self.cache = {}\n",
    "        self.windows = []\n",
    "\n",
    "        # use subsequence windows from genes\n",
    "        if self.use_sliding_window and window_size is not None:\n",
    "            self._create_windows()\n",
    "        # use full-length genes\n",
    "        else:\n",
    "            self._prepare_full_genes()\n",
    "    \n",
    "    def _create_windows(self):\n",
    "        for gene_id, group in self.grouped_data:\n",
    "            gene_length = len(group)\n",
    "            for start_idx in range(0, gene_length - self.window_size + 1, self.window_size):\n",
    "                end_idx = start_idx + self.window_size\n",
    "                if end_idx > gene_length:\n",
    "                    break\n",
    "                window = group.iloc[start_idx:end_idx]\n",
    "                self.windows.append((gene_id, window))\n",
    "    \n",
    "    def _prepare_full_genes(self):\n",
    "        for gene_id, group in self.grouped_data:\n",
    "            self.windows.append((gene_id, group))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.windows)\n",
    "\n",
    "    # prepare single window or gene\n",
    "    def __getitem__(self, idx):\n",
    "        gene_id, window = self.windows[idx]\n",
    "        \n",
    "        if gene_id in self.cache:\n",
    "            return self.cache[gene_id]\n",
    "        \n",
    "        strand_encoded = window['strand'].map({'-': 0, '+': 1}).values\n",
    "        strand_tensor = torch.tensor(strand_encoded, dtype=torch.int64)\n",
    "         \n",
    "        result = {\n",
    "            'GeneId': gene_id,\n",
    "            'Chr': window['seqnames'].values,\n",
    "            'Start': torch.tensor(window['start'].values, dtype=torch.int64),\n",
    "            'End': torch.tensor(window['end'].values, dtype=torch.int64),\n",
    "            'Strand': strand_tensor,\n",
    "            \n",
    "            # epigenomic features per gene j, site i\n",
    "            'Y_ji':  torch.tensor(window[feature_names].values, dtype=torch.float64),\n",
    "            \n",
    "            # read counts per gene j, site i\n",
    "            'X_ji': torch.tensor(window['score'].values, dtype=torch.float64),\n",
    "            \n",
    "            # read depth * initiation rate values per gene j\n",
    "            'C_j': torch.tensor(window['lambda_alphaj'].iloc[0], dtype=torch.float64),\n",
    "            \n",
    "            # GLM elongation rate predictions per gene j, site i\n",
    "            'Z_ji': torch.tensor(window['combined_zeta'].values, dtype=torch.float64),\n",
    "            \n",
    "            # one-hot encoded sequences\n",
    "            'N_ji': torch.tensor(window[nucleotides].values, dtype=torch.float64)\n",
    "        }\n",
    "    \n",
    "        self.cache[gene_id] = result\n",
    "\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Batch subsequences with same gene id together \"\"\"\n",
    "class GeneIdBatchSampler(BatchSampler):\n",
    "    def __init__(self, dataset):\n",
    "        self.dataset = dataset\n",
    "        self.batches = self._create_batches()\n",
    "\n",
    "    def _create_batches(self):\n",
    "        # Group indices by GeneId\n",
    "        gene_id_to_indices = {}\n",
    "        for idx in range(len(self.dataset)):\n",
    "            gene_id = self.dataset[idx]['GeneId']\n",
    "            if gene_id not in gene_id_to_indices:\n",
    "                gene_id_to_indices[gene_id] = []\n",
    "            gene_id_to_indices[gene_id].append(idx)\n",
    "\n",
    "        return list(gene_id_to_indices.values())\n",
    "\n",
    "    def __iter__(self):\n",
    "        for batch in self.batches:\n",
    "            yield batch\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_collate_fn(batch):\n",
    "    gene_ids = [item['GeneId'] for item in batch]\n",
    "    start = torch.stack([item['Start'] for item in batch])\n",
    "    end = torch.stack([item['End'] for item in batch])\n",
    "    strand = torch.stack([item['Strand'] for item in batch])\n",
    "    Y_ji = torch.stack([item['Y_ji'] for item in batch])\n",
    "    X_ji = torch.stack([item['X_ji'] for item in batch])\n",
    "    C_j = torch.stack([item['C_j'] for item in batch])\n",
    "    Z_ji = torch.stack([item['Z_ji'] for item in batch])\n",
    "    N_ji = torch.stack([item['N_ji'] for item in batch])\n",
    "    \n",
    "    # Handling lists of strings\n",
    "    chrs = [item['Chr'] for item in batch]\n",
    "    \n",
    "    batched_data = {\n",
    "        'GeneId': gene_ids,\n",
    "        'Chr': chrs,\n",
    "        'Start': start,\n",
    "        'End': end,\n",
    "        'Strand': strand,\n",
    "        'Y_ji': Y_ji,\n",
    "        'X_ji': X_ji,\n",
    "        'C_j': C_j,\n",
    "        'Z_ji': Z_ji,\n",
    "        'N_ji': N_ji\n",
    "    }\n",
    "    \n",
    "    return batched_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dataset(data, batch_size, use_sliding_window, window_size=None):\n",
    "    dataset = GeneDataset(data, use_sliding_window, window_size)\n",
    "    #batch_sampler = GeneIdBatchSampler(dataset)\n",
    "    loader = DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=7, collate_fn=custom_collate_fn) #batch_sampler=batch_sampler, \n",
    "    return loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def match_list_lengths(list1, list2):\n",
    "    len1 = len(list1)\n",
    "    len2 = len(list2)\n",
    "    \n",
    "    # If the first list is shorter, extend it\n",
    "    if len1 < len2:\n",
    "        list1.extend([list1[-1]] * (len2 - len1))\n",
    "    # If the second list is shorter, extend it\n",
    "    elif len2 < len1:\n",
    "        list2.extend([list2[-1]] * (len1 - len2))\n",
    "    \n",
    "    return list1, list2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def build_model(config):\n",
    "\n",
    "    class EpLinearModel(nn.Module):\n",
    "        def __init__(self, input_size):\n",
    "            super(EpLinearModel, self).__init__()\n",
    "            self.name = \"ep_linear\"\n",
    "            self.linear = nn.Linear(input_size, 1)\n",
    "\n",
    "        def forward(self, Y_ji):\n",
    "            x = self.linear(Y_ji)\n",
    "            return x.squeeze(-1)   \n",
    "    \n",
    "    class EpSeqLinearModel(nn.Module):\n",
    "        def __init__(self, num_ep_features, num_seq_features):\n",
    "            super(EpSeqLinearModel, self).__init__()\n",
    "            self.name = \"ep_seq_linear\"\n",
    "            self.y_linear = nn.Linear(num_ep_features, 1)\n",
    "            self.n_linear = nn.Linear(num_seq_features, 1)\n",
    "            self.final_linear = nn.Linear(2, 1)\n",
    "\n",
    "        def forward(self, Y_ji, N_ji):\n",
    "            y = self.y_linear(Y_ji)\n",
    "            n = self.n_linear(N_ji)\n",
    "            x = torch.cat((y, n), axis=-1)\n",
    "            x = self.final_linear(x)\n",
    "            return x.squeeze(-1)\n",
    "        \n",
    "    class CNN(nn.Module):\n",
    "        def __init__(self, num_ep_features, num_seq_features, \n",
    "                     y_channels, y_kernel_sizes,\n",
    "                     n_channels, n_kernel_sizes, dropout, \n",
    "                     lstm_layer_size, num_lstm_layers=None, bidirectional=False):\n",
    "            \n",
    "            super(CNN, self).__init__()\n",
    "            self.name = \"cnn\"            \n",
    "\n",
    "            self.y_convs = nn.ModuleList()\n",
    "            y_in_channels = num_ep_features\n",
    "            \n",
    "            y_channels, y_kernel_sizes = match_list_lengths(y_channels, y_kernel_sizes)\n",
    "\n",
    "            # Y_ji convolutional layers\n",
    "            for idx, out_channels in enumerate(y_channels):\n",
    "                self.y_convs.append(\n",
    "                    nn.Conv1d(y_in_channels, out_channels, y_kernel_sizes[idx], stride=1, padding='same')\n",
    "                )\n",
    "                y_in_channels = out_channels\n",
    "            \n",
    "            \n",
    "            self.n_convs = nn.ModuleList()\n",
    "            n_in_channels = num_seq_features\n",
    "            \n",
    "            n_channels, n_kernel_sizes = match_list_lengths(n_channels, n_kernel_sizes)\n",
    "            \n",
    "            for idx, out_channels in enumerate(n_channels):\n",
    "                self.n_convs.append(\n",
    "                    nn.Conv1d(n_in_channels, out_channels, n_kernel_sizes[idx], stride=1, padding='same')\n",
    "                )\n",
    "                n_in_channels = out_channels\n",
    "\n",
    "            self.relu = nn.ReLU()\n",
    "            self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "            # Final convolutional layer to map to a single output channel\n",
    "            # Since the output needs to be (batch_size, seq_len), we map the final features to 1\n",
    "            self.final_conv = nn.Conv1d(y_channels[-1] + n_channels[-1], 1, 1)  # 1x1 convolution\n",
    "            \n",
    "            self.num_lstm_layers = num_lstm_layers\n",
    "            if num_lstm_layers != 0 and num_lstm_layers != None:\n",
    "                self.gru = nn.GRU(input_size=y_channels[-1] + n_channels[-1], hidden_size=lstm_layer_size, num_layers=num_lstm_layers, bidirectional=bidirectional, batch_first=True)\n",
    "            \n",
    "            self.final_linear = nn.Linear(lstm_layer_size, 1)\n",
    "            self.bidirectional = True\n",
    "            self.final_bidirectional_linear = nn.Linear(lstm_layer_size*2, 1)\n",
    "            \n",
    "        def forward(self, Y_ji, N_ji):\n",
    "            Y_ji = Y_ji.permute(0, 2, 1)  \n",
    "            N_ji = N_ji.permute(0, 2, 1)\n",
    "            \n",
    "            for conv in self.y_convs:\n",
    "                Y_ji = conv(Y_ji)\n",
    "                Y_ji = self.relu(Y_ji)\n",
    "                Y_ji = self.dropout(Y_ji)\n",
    "            \n",
    "            for conv in self.n_convs:\n",
    "                N_ji = conv(N_ji)\n",
    "                N_ji = self.relu(N_ji)\n",
    "                N_ji = self.dropout(N_ji)\n",
    "\n",
    "            x = torch.cat((Y_ji, N_ji), 1)\n",
    "                        \n",
    "            if self.num_lstm_layers != 0 and self.num_lstm_layers != None:\n",
    "                x = x.permute(0,2,1)\n",
    "                x, (h_n, c_n) = self.gru(x)\n",
    "                if self.bidirectional:\n",
    "                    x = self.final_bidirectional_linear(x)\n",
    "                else:\n",
    "                    x = self.final_linear(x)\n",
    "                x = x.squeeze(-1)\n",
    "            \n",
    "            else:\n",
    "                x = self.final_conv(x)\n",
    "                x = x.squeeze(1)  \n",
    "                \n",
    "            return x\n",
    "    \n",
    "    if config[\"model_type\"] == 'ep_seq_linear':\n",
    "        model = EpSeqLinearModel(num_features, num_seq_features)\n",
    "    elif config[\"model_type\"] == 'ep_linear':\n",
    "        model = EpLinearModel(num_features)\n",
    "    elif config[\"model_type\"] == 'cnn':\n",
    "        lstm_layer_size = None\n",
    "        bidirectional = None\n",
    "        if config[\"num_lstm_layers\"] != 0 and config[\"num_lstm_layers\"] != None:\n",
    "            lstm_layer_size = config[\"lstm_layer_size\"]\n",
    "            bidirectional = config[\"bidirectional\"]\n",
    "        model = CNN(num_features, num_seq_features, config[\"y_channels\"], config[\"y_kernel_sizes\"], \n",
    "                    config[\"n_channels\"], config[\"n_kernel_sizes\"], config[\"dropout\"], \n",
    "                    config[\"num_lstm_layers\"], lstm_layer_size, bidirectional)\n",
    "    \n",
    "    if cuda_available:\n",
    "        if num_gpus > 1:\n",
    "            print(\"Using\", num_gpus, \"GPUs\")\n",
    "            model = torch.nn.DataParallel(model)\n",
    "        model = model.to('cuda')\n",
    "\n",
    "    print(model)\n",
    "\n",
    "    \n",
    "    model.double()\n",
    "\n",
    "    return model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_optimizer(network, learning_rate, l2_lambda):\n",
    "    optimizer = optim.Adam(network.parameters(), lr=learning_rate, weight_decay=l2_lambda)\n",
    "    return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def valid_epoch(model, loader, loss_fn):\n",
    "    model.eval()\n",
    "    total_neural_net_loss = 0\n",
    "    total_glm_loss = 0\n",
    "    neural_net_zeta = []\n",
    "    glm_zeta = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for idx, batch in enumerate(loader):\n",
    "            Y_ji_batch = batch['Y_ji'].to(device)\n",
    "            X_ji_batch = batch['X_ji'].to(device)\n",
    "            N_ji_batch = batch['N_ji'].to(device) \n",
    "            C_j_batch = batch['C_j'].to(device).unsqueeze(1)\n",
    "            Z_ji_batch = batch['Z_ji'].to(device)\n",
    "            \n",
    "            if model.name == \"ep_linear\":\n",
    "                outputs = model(Y_ji_batch)\n",
    "            else:\n",
    "                outputs = model(Y_ji_batch, N_ji_batch)\n",
    "                \n",
    "            neural_net_loss = loss_fn(X_ji_batch, C_j_batch, outputs)\n",
    "            glm_loss = loss_fn(X_ji_batch, C_j_batch, torch.log(Z_ji_batch))\n",
    "\n",
    "            total_neural_net_loss +=  neural_net_loss.item()\n",
    "            total_glm_loss += glm_loss.item()\n",
    "            \n",
    "            # store all predictions in list\n",
    "            neural_net_zeta.append(torch.exp(outputs.cpu().flatten()))\n",
    "            glm_zeta.append(batch['Z_ji'].flatten())\n",
    "    \n",
    "    # calculate average loss across all batches\n",
    "    avg_neural_net_loss = total_neural_net_loss / len(loader)\n",
    "    avg_glm_loss = total_glm_loss / len(loader)\n",
    "    \n",
    "    neural_net_zeta = torch.cat(neural_net_zeta)\n",
    "    glm_zeta = torch.cat(glm_zeta)\n",
    "    \n",
    "    return avg_neural_net_loss, avg_glm_loss, neural_net_zeta, glm_zeta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, loader, optimizer, loss_fn, l1_lambda):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for idx, batch in enumerate(loader):\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        Y_ji_batch = batch['Y_ji'].to(device) \n",
    "        X_ji_batch = batch['X_ji'].to(device)\n",
    "        N_ji_batch = batch['N_ji'].to(device) \n",
    "        C_j_batch = batch['C_j'].to(device).unsqueeze(1)\n",
    "        \n",
    "        if model.name == \"ep_linear\":\n",
    "            outputs = model(Y_ji_batch)\n",
    "        else:\n",
    "            outputs = model(Y_ji_batch, N_ji_batch)\n",
    "\n",
    "        loss = loss_fn(X_ji_batch, C_j_batch, outputs)\n",
    "\n",
    "        if l1_lambda != 0:\n",
    "            l1_norm = sum(torch.abs(p).sum() for p in model.parameters())\n",
    "            loss = loss + l1_lambda * l1_norm\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # calculate average loss across all batches\n",
    "        total_loss += loss.item()\n",
    "    avg_train_loss = total_loss / len(loader)\n",
    "    \n",
    "    return avg_train_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CustomLoss, self).__init__()\n",
    "\n",
    "    def forward(self, X_ji, C_j, rho_ji):\n",
    "        loss = X_ji * rho_ji + C_j * torch.exp(-rho_ji) - X_ji * torch.log(C_j)\n",
    "        \n",
    "        # calculate average loss per site\n",
    "        return (loss).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cnn_2\n"
     ]
    }
   ],
   "source": [
    "with open(config_folder_path + config_file_name + \".json\", 'r') as file:\n",
    "    config = json.load(file)\n",
    "    sweep_config['parameters'] = config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "increase_cut=0.00001\n",
    "patience=5\n",
    "\n",
    "def train(config=None):\n",
    "    if use_wandb:\n",
    "        wandb.init(config=config)\n",
    "        config=wandb.config\n",
    "        \n",
    "    model = build_model(config)\n",
    "    \n",
    "    train_window_size = None\n",
    "    if config[\"train_use_sliding_window\"]:\n",
    "        train_window_size = config[\"train_window_size\"]\n",
    "    train_loader = build_dataset(train_data, 32, config[\"train_use_sliding_window\"], train_window_size)\n",
    "    \n",
    "    valid_window_size = None\n",
    "    if config[\"valid_use_sliding_window\"]:\n",
    "        valid_window_size = config[\"valid_window_size\"]\n",
    "    valid_loader = build_dataset(valid_data, 1, config[\"valid_use_sliding_window\"], valid_window_size)\n",
    "    \n",
    "    optimizer = build_optimizer(model, config['learning_rate'], config['l2_lambda'])\n",
    "    \n",
    "    loss_fn = torch.jit.script(CustomLoss())\n",
    "\n",
    "    # track loss curves\n",
    "    loss_neural_net_train = [0] * config[\"epochs\"]\n",
    "    loss_neural_net_valid = [0] * config[\"epochs\"]\n",
    "    loss_glm_valid = [0] * config[\"epochs\"]\n",
    "    \n",
    "    # scheduler to reduce learning rate by half when new validation loss > old validation loss\n",
    "    old_neural_net_valid_loss = float('inf')\n",
    "    epochs_no_improve = 0\n",
    "    scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=1, verbose=True)\n",
    "\n",
    "    for epoch in range(config[\"epochs\"]):\n",
    "        print(f'Epoch {epoch+1}')\n",
    "        \n",
    "        train_loss = train_epoch(model, train_loader, optimizer, loss_fn, config['l1_lambda'])\n",
    "        loss_neural_net_train[epoch] = train_loss\n",
    "        print(f\"train loss: {train_loss: .5f}\")\n",
    "        \n",
    "        valid_neural_net_loss, valid_glm_loss, neural_net_zeta, glm_zeta = valid_epoch(model, valid_loader, loss_fn)\n",
    "        loss_neural_net_valid[epoch] = valid_neural_net_loss\n",
    "        loss_glm_valid[epoch] = valid_glm_loss\n",
    "        print(f\"valid neural net loss: {valid_neural_net_loss: .5f}\")\n",
    "        print(f\"valid glm loss: {valid_glm_loss: .5f}\")\n",
    "        \n",
    "        # compute metrics\n",
    "        mae = F.l1_loss(neural_net_zeta.squeeze(), glm_zeta)\n",
    "        mse = F.mse_loss(neural_net_zeta.squeeze(), glm_zeta)\n",
    "        correlation_coefficient = np.corrcoef(glm_zeta, neural_net_zeta.squeeze())[0, 1]\n",
    "        print(\"Correlation Coefficient:\", correlation_coefficient)\n",
    "        print(f\"Mean Absolute Error: {mae.item():.4f}\")\n",
    "        print(f\"Mean Squared Error: {mse.item():.4f}\")\n",
    "        \n",
    "        if use_wandb:\n",
    "            wandb.log({\"epoch\": epoch, \"train_loss\": train_loss, \"valid_neural_net_loss\": valid_neural_net_loss,\n",
    "           \"valid_glm_loss\": valid_glm_loss, \"correlation_coefficient\": correlation_coefficient,\n",
    "           \"mae\": mae, \"mse\": mse})\n",
    "        \n",
    "        # early stopping\n",
    "        if valid_neural_net_loss < old_neural_net_valid_loss - increase_cut:\n",
    "            epochs_no_improve = 0\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "            \n",
    "        if epochs_no_improve == patience:\n",
    "            print(\"Early Stopping\")\n",
    "            break\n",
    "        \n",
    "        # reduce learning rate if new loss > old loss\n",
    "        if valid_neural_net_loss > old_neural_net_valid_loss:\n",
    "            optimizer.param_groups[0]['lr'] *= 0.5\n",
    "            print(f\"Reduced learning rate to {optimizer.param_groups[0]['lr']}\")\n",
    "            \n",
    "        old_neural_net_valid_loss = valid_neural_net_loss\n",
    "        scheduler.step(valid_neural_net_loss)\n",
    "        \n",
    "    return model, loss_neural_net_train, loss_neural_net_valid, loss_glm_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_wandb:\n",
    "    sweep_id = wandb.sweep(sweep_config, project=config_file_name)\n",
    "    wandb.agent(sweep_id, train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_wandb == False:\n",
    "    model, loss_neural_net_train, loss_neural_net_valid, loss_glm_valid = train(config)\n",
    "\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    filename = f\"models/{config_file_name}.pth\"\n",
    "    torch.save(model.state_dict(), filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN(\n",
      "  (y_convs): ModuleList(\n",
      "    (0): Conv1d(10, 16, kernel_size=(9,), stride=(1,), padding=same)\n",
      "  )\n",
      "  (n_convs): ModuleList(\n",
      "    (0): Conv1d(4, 32, kernel_size=(3,), stride=(1,), padding=same)\n",
      "    (1): Conv1d(32, 64, kernel_size=(5,), stride=(1,), padding=same)\n",
      "    (2): Conv1d(64, 128, kernel_size=(9,), stride=(1,), padding=same)\n",
      "  )\n",
      "  (relu): ReLU()\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      "  (final_conv): Conv1d(144, 1, kernel_size=(1,), stride=(1,))\n",
      "  (final_linear): Linear(in_features=0, out_features=1, bias=True)\n",
      "  (final_bidirectional_linear): Linear(in_features=0, out_features=1, bias=True)\n",
      ")\n",
      "CUDA (GPU support) is available: False\n",
      "Number of GPUs available: 0\n",
      "Model is on device: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/grid/siepel/home_norepl/hassett/.local/lib/python3.7/site-packages/torch/nn/init.py:388: UserWarning: Initializing zero-element tensors is a no-op\n",
      "  warnings.warn(\"Initializing zero-element tensors is a no-op\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CNN(\n",
       "  (y_convs): ModuleList(\n",
       "    (0): Conv1d(10, 16, kernel_size=(9,), stride=(1,), padding=same)\n",
       "  )\n",
       "  (n_convs): ModuleList(\n",
       "    (0): Conv1d(4, 32, kernel_size=(3,), stride=(1,), padding=same)\n",
       "    (1): Conv1d(32, 64, kernel_size=(5,), stride=(1,), padding=same)\n",
       "    (2): Conv1d(64, 128, kernel_size=(9,), stride=(1,), padding=same)\n",
       "  )\n",
       "  (relu): ReLU()\n",
       "  (dropout): Dropout(p=0.5, inplace=False)\n",
       "  (final_conv): Conv1d(144, 1, kernel_size=(1,), stride=(1,))\n",
       "  (final_linear): Linear(in_features=0, out_features=1, bias=True)\n",
       "  (final_bidirectional_linear): Linear(in_features=0, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load model state\n",
    "\n",
    "model = build_model(config)\n",
    "\n",
    "model.load_state_dict(torch.load(f\"./model_checkpoints/{config_file_name}.pth\", map_location=torch.device('cpu')))\n",
    "\n",
    "cuda_available = torch.cuda.is_available()\n",
    "print(\"CUDA (GPU support) is available:\", cuda_available)\n",
    "num_gpus = torch.cuda.device_count()\n",
    "print(\"Number of GPUs available:\", num_gpus)\n",
    "if cuda_available:\n",
    "    if num_gpus > 1:\n",
    "        print(\"Using\", num_gpus, \"GPUs\")\n",
    "        model = torch.nn.DataParallel(model)\n",
    "    model = model.to('cuda')\n",
    "\n",
    "first_param_device = next(model.parameters()).device\n",
    "print(\"Model is on device:\", first_param_device)\n",
    "\n",
    "model.double()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "results_dir = results_folder_path + config_file_name\n",
    "os.makedirs(results_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = range(1, len(loss_neural_net_train) + 1)\n",
    "plt.plot(epochs, loss_neural_net_train, label='train_neural_net_loss')\n",
    "plt.plot(epochs, loss_neural_net_valid, label='valid_neural_net_loss')\n",
    "plt.plot(epochs, loss_glm_valid, label='valid_glm_loss')\n",
    "\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.savefig(results_folder_path + config_file_name + \"/loss_curves.png\")\n",
    "plt.show "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "\n",
    "tstdl = build_dataset(test_data, 1, False, None)\n",
    "data_iter = iter(tstdl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_data(glm_zeta, net_zeta, file_name, plot_first_window, window_size):\n",
    "    if plot_first_window:\n",
    "        indices = range(window_size)\n",
    "        net_zeta = net_zeta[0:window_size]\n",
    "        glm_zeta = glm_zeta[0:window_size]\n",
    "    else:\n",
    "        indices = range(len(glm_zeta))\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(10, 5))\n",
    "        \n",
    "    ax.scatter(indices, net_zeta, color='blue', label='Neural Net Zeta', s=10, alpha=0.5)\n",
    "    ax.scatter(indices, glm_zeta, color='orange', label='GLM Zeta', s=10, alpha=0.5)\n",
    "    \n",
    "    ax.set_title('Neural Net vs GLM Elongation Rate')\n",
    "    ax.set_xlabel('Index')\n",
    "    ax.set_ylabel('Elongation Rate')\n",
    "    ax.legend()\n",
    "    \n",
    "    plt.ylim(0, 6.5)\n",
    "\n",
    "    plt.savefig(file_name)\n",
    "    plt.close()\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute metrics for test dataset\n",
    "\n",
    "loss_fn = CustomLoss()\n",
    "\n",
    "total_net_loss = 0\n",
    "total_glm_loss = 0\n",
    "net_zeta = []\n",
    "glm_zeta = []\n",
    "with torch.no_grad():\n",
    "    for batch in tstdl:\n",
    "        Y_ji = batch['Y_ji'].to(device)\n",
    "        N_ji = batch['N_ji'].to(device)\n",
    "        X_ji = batch['X_ji'].to(device)\n",
    "        C_j = batch['C_j'].to(device).unsqueeze(1)\n",
    "        Z_ji = batch['Z_ji'].to(device)\n",
    "        \n",
    "        if model.name == \"ep_linear\":\n",
    "            rho_ji = model(Y_ji)\n",
    "        else:\n",
    "            rho_ji = model(Y_ji, N_ji)\n",
    "        \n",
    "        net_loss = loss_fn(X_ji, C_j, rho_ji)\n",
    "        glm_loss = loss_fn(X_ji, C_j, torch.log(Z_ji))\n",
    "        \n",
    "        total_net_loss += net_loss.item()\n",
    "        total_glm_loss += glm_loss.item()\n",
    "        \n",
    "        net_zeta.append(torch.exp(rho_ji.cpu()).flatten())\n",
    "        glm_zeta.append(batch['Z_ji'].flatten())\n",
    "\n",
    "        \n",
    "net_zeta = torch.cat(net_zeta)\n",
    "glm_zeta = torch.cat(glm_zeta)\n",
    "\n",
    "mae = F.l1_loss(net_zeta, glm_zeta)\n",
    "mse = F.mse_loss(net_zeta, glm_zeta)\n",
    "\n",
    "correlation_coefficient = np.corrcoef(glm_zeta, net_zeta)[0, 1]\n",
    "\n",
    "file_path = f\"{results_dir}/metrics.txt\"\n",
    "\n",
    "with open(file_path, \"w\") as file:\n",
    "    file.write(f\"Neural Net Loss: {total_net_loss/len(tstdl):.4f}\\n\")\n",
    "    file.write(f\"GLM Loss: {total_glm_loss/len(tstdl):.4f}\\n\")\n",
    "    file.write(f\"Correlation Coefficient: {correlation_coefficient:.4f}\\n\")\n",
    "    file.write(f\"Mean Absolute Error: {mae.item():.4f}\\n\")\n",
    "    file.write(f\"Mean Squared Error: {mse.item():.4f}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "\n",
    "# plot for subset of genes in test dataset\n",
    "for i in range(0, 3):\n",
    "    inputs = next(data_iter) \n",
    "    print(\"number of samples: \" + str(len(inputs)))\n",
    "\n",
    "    with torch.no_grad():\n",
    "        Y_ji = inputs['Y_ji'].to(device)\n",
    "        N_ji = inputs['N_ji'].to(device)\n",
    "        if model.name == \"ep_linear\":\n",
    "            rho_ji = model(Y_ji)\n",
    "        else:\n",
    "            rho_ji = model(Y_ji, N_ji)\n",
    "\n",
    "    glm_zeta = inputs['Z_ji'][0]\n",
    "    # convert log(Z) outputs to Z\n",
    "    net_zeta = torch.exp(rho_ji.cpu().squeeze())\n",
    "\n",
    "    plot_data(glm_zeta, net_zeta, f\"{results_dir}/plot_{i}\", False, 0)\n",
    "    plot_data(glm_zeta, net_zeta, f\"{results_dir}/plot_w100_{i}\", True, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Convert results to bigwig format \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_data = pd.concat([train_data, valid_data, test_data], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "\n",
    "full_dl = build_dataset(full_data, 1, False, None)\n",
    "full_data_iter = iter(full_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nbw = pyBigWig.open(f\"{cell_type}_neuralNetEpAllmerPredZeta.bw\", \"w\")\\nchrom_sizes = {\\'chr1\\': 248956422, \\'chr2\\': 242193529}\\nbw.addHeader(list(chrom_sizes.items()))\\n# Add entries from DataFrame\\nfor index, row in df.iterrows():\\n    bw.addEntries([row[\\'chrom\\']], [row[\\'start\\']], ends=[row[\\'end\\']], values=[row[\\'value\\']])\\n\\n# Close the BigWig file\\nbw.close()\\n'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "bw = pyBigWig.open(f\"{cell_type}_neuralNetEpAllmerPredZeta.bw\", \"w\")\n",
    "chrom_sizes = {'chr1': 248956422, 'chr2': 242193529}\n",
    "bw.addHeader(list(chrom_sizes.items()))\n",
    "# Add entries from DataFrame\n",
    "for index, row in df.iterrows():\n",
    "    bw.addEntries([row['chrom']], [row['start']], ends=[row['end']], values=[row['value']])\n",
    "\n",
    "# Close the BigWig file\n",
    "bw.close()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['Chr', 'Start', 'End', 'Value', 'Strand']\n",
    "bw_df = df = pd.DataFrame(columns=columns)\n",
    "\n",
    "bw_items = []\n",
    "with torch.no_grad():\n",
    "    for batch in full_dl:\n",
    "        Y_ji = batch['Y_ji'].to(device)\n",
    "        N_ji = batch['N_ji'].to(device)\n",
    "        Z_ji = batch['Z_ji'].to(device)\n",
    "        \n",
    "        if model.name == \"ep_linear\":\n",
    "            rho_ji = model(Y_ji)\n",
    "        else:\n",
    "            rho_ji = model(Y_ji, N_ji)    \n",
    "        \n",
    "        data_dict = {\n",
    "            \"Chr\": batch[\"Chr\"][0].tolist(), # stored as string\n",
    "            \"Start\": batch[\"Start\"][0].tolist(),\n",
    "            \"End\": batch[\"End\"][0].tolist(),\n",
    "            \"Value\": torch.exp(rho_ji.squeeze().cpu()).tolist(), #test squeeze?\n",
    "            \"Strand\": batch[\"Strand\"][0].tolist()\n",
    "        }\n",
    "        bw_items.append(pd.DataFrame(data_dict))\n",
    "\n",
    "bw_df = pd.concat(bw_items, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check that minus strand has only positive values\n",
    "# Filter the DataFrame to include only rows where Strand is 0\n",
    "minus_strand_df = bw_df[bw_df[\"Strand\"] == 0]\n",
    "\n",
    "# Check if all values in the Value column of this subset are greater than 0\n",
    "all_values_positive = (minus_strand_df[\"Value\"] > 0).all()\n",
    "\n",
    "if not all_values_positive:\n",
    "    print(\"Minus strand contains negative values before being processed\")\n",
    "# throw error if all_values_positive is false"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove minus strand when positive strand at same position\n",
    "def filter_strands(group):\n",
    "    # Mask for rows with Strand == 1\n",
    "    mask_strand_1 = group['Strand'] == 1\n",
    "    # Identify unique Start positions for Strand == 1 within the group\n",
    "    starts_with_strand_1 = group.loc[mask_strand_1, 'Start'].unique()\n",
    "    # Mask for rows with Strand == 0 and Start not in starts_with_strand_1, within the group\n",
    "    mask_strand_0_unique_starts = (group['Strand'] == 0) & (~group['Start'].isin(starts_with_strand_1))\n",
    "    # Combine masks to filter the group\n",
    "    filtered_group = group[mask_strand_1 | mask_strand_0_unique_starts]\n",
    "    return filtered_group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bw_df = bw_df.groupby('Chr').apply(filter_strands).reset_index(drop=True)\n",
    "\n",
    "# negate zeta value for minus strand\n",
    "bw_df.loc[bw_df[\"Strand\"] == 0, \"Value\"] = bw_df.loc[bw_df[\"Strand\"] == 0, \"Value\"] * -1\n",
    "\n",
    "# remove strand column when storing to bigwig file\n",
    "del bw_df[\"Strand\"]\n",
    "\n",
    "# update Chr column values\n",
    "bw_df['Chr'] = 'chr' + bw_df['Chr'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup header\n",
    "epAllmer_bw = pyBigWig.open(\"./data/k562_epAllmerPredZeta.bw\")\n",
    "chrom_lengths = epAllmer_bw.chroms()\n",
    "chrom_lengths = list(chrom_lengths.items())\n",
    "\n",
    "bw = pyBigWig.open(\"./data/k562_epAllmerNeuralNetZeta.bw\", \"w\")\n",
    "bw.addHeader(chrom_lengths)\n",
    "\n",
    "bw.addEntries(bw_df['Chr'].tolist(), bw_df['Start'].tolist(), ends=bw_df['End'].tolist(), values=bw_df['Value'].tolist())\n",
    "\n",
    "bw.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Anaconda3 (Python 3.7.6)",
   "language": "python",
   "name": "anaconda3_2020.02"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
