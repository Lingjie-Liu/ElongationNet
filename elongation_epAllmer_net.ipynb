{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Testing/Debugging File \"\"\"\n",
    "\n",
    "\"\"\"\n",
    "Restart kernel after running\n",
    "Only need to run once\n",
    "\"\"\"\n",
    "!pip install scikit-learn matplotlib seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Configure path to config file and select whether hyperparameter sweeping or not \"\"\"\n",
    "config_folder_path = \"./configs/\"\n",
    "config_file_name = \"cnn_1\"\n",
    "results_folder_path = \"./results/\"\n",
    "sweep_config = {\n",
    "    'method': 'grid'\n",
    "}\n",
    "\n",
    "input_data_file = './data/k562_datasets.pkl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "import math \n",
    "import json\n",
    "import torch\n",
    "import torch.utils.data as td\n",
    "import torch.nn as nn\n",
    "import torch.nn.init as init\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torch.utils.data import DataLoader, Dataset, Subset, BatchSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seqnames                         1\n",
      "start                      1002760\n",
      "end                        1002760\n",
      "strand                           +\n",
      "ensembl_gene_id    ENSG00000187608\n",
      "score                          0.0\n",
      "ctcf                      -0.07771\n",
      "h4k20me1                 -0.429997\n",
      "h3k79me2                   -0.2804\n",
      "h3k4me1                  -0.217665\n",
      "h3k9me3                  -0.333359\n",
      "h3k36me3                 -0.801406\n",
      "sj5                      -0.039619\n",
      "sj3                      -0.059131\n",
      "rpts                     -0.187111\n",
      "wgbs                           0.0\n",
      "lambda_alphaj             0.026377\n",
      "zeta                      1.133344\n",
      "A                                0\n",
      "T                                0\n",
      "G                                1\n",
      "C                                0\n",
      "dataset                      train\n",
      "Name: 0, dtype: object\n",
      "['ctcf' 'h4k20me1' 'h3k79me2' 'h3k4me1' 'h3k9me3' 'h3k36me3' 'sj5' 'sj3'\n",
      " 'rpts' 'wgbs']\n",
      "Number of Samples: 136927782\n",
      "Number of Features: 10\n",
      "CUDA (GPU support) is available: True\n",
      "Number of GPUs available: 1\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Load datasets \"\"\"\n",
    "with open(input_data_file, 'rb') as file:\n",
    "    combined_datasets = pickle.load(file)\n",
    "    \n",
    "nucleotides = ['A', 'T', 'G', 'C']\n",
    "\n",
    "train_data = combined_datasets['train']\n",
    "valid_data = combined_datasets['valid']\n",
    "test_data = combined_datasets['test']\n",
    "\n",
    "print(train_data.iloc[0])\n",
    "\n",
    "column_names = np.array(train_data.columns)\n",
    "feature_names = column_names[6:16]\n",
    "num_features = len(feature_names)\n",
    "print(feature_names)\n",
    "num_samples = train_data.shape[0]\n",
    "num_seq_features = len(nucleotides)\n",
    "\n",
    "print(\"Number of Samples: \" + str(num_samples))\n",
    "print(\"Number of Features: \" + str(num_features))\n",
    "\n",
    "cuda_available = torch.cuda.is_available()\n",
    "print(\"CUDA (GPU support) is available:\", cuda_available)\n",
    "num_gpus = torch.cuda.device_count()\n",
    "print(\"Number of GPUs available:\", num_gpus)\n",
    "\n",
    "torch.backends.cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "\"\"\" Process data using a sliding window approach \"\"\"\n",
    "class GeneDataset(Dataset):\n",
    "    def __init__(self, dataframe, use_sliding_window=False, window_size=100):\n",
    "        self.dataframe = dataframe\n",
    "        self.grouped_data = dataframe.groupby('ensembl_gene_id')\n",
    "        self.use_sliding_window = use_sliding_window\n",
    "        self.window_size = window_size\n",
    "        self.cache = {}\n",
    "        self.windows = []\n",
    "\n",
    "        # use subsequence windows from genes\n",
    "        if self.use_sliding_window and window_size is not None:\n",
    "            self._create_windows()\n",
    "        # use full-length genes\n",
    "        else:\n",
    "            self._prepare_full_genes()\n",
    "    \n",
    "    def _create_windows(self):\n",
    "        for gene_id, group in self.grouped_data:\n",
    "            gene_length = len(group)\n",
    "            for start_idx in range(0, gene_length - self.window_size + 1, self.window_size):\n",
    "                end_idx = start_idx + self.window_size\n",
    "                if end_idx > gene_length:\n",
    "                    break\n",
    "                window = group.iloc[start_idx:end_idx]\n",
    "                self.windows.append((gene_id, window))\n",
    "    \n",
    "    def _prepare_full_genes(self):\n",
    "        for gene_id, group in self.grouped_data:\n",
    "            self.windows.append((gene_id, group))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.windows)\n",
    "\n",
    "    # prepare single window or gene\n",
    "    def __getitem__(self, idx):\n",
    "        gene_id, window = self.windows[idx]\n",
    "        \n",
    "        if gene_id in self.cache:\n",
    "            return self.cache[gene_id]\n",
    "        \n",
    "        strand_encoded = window['strand'].map({'-': 0, '+': 1}).values\n",
    "        strand_tensor = torch.tensor(strand_encoded, dtype=torch.int64)\n",
    " \n",
    "        result = {\n",
    "            'GeneId': gene_id,\n",
    "            'Seq_Name': window['seqnames'].iloc[0],\n",
    "            'Start': torch.tensor(window['start'].values, dtype=torch.int64),\n",
    "            'End': torch.tensor(window['end'].values, dtype=torch.int64),\n",
    "            'Strand': strand_tensor,\n",
    "            \n",
    "            # epigenomic features per gene j, site i\n",
    "            'Y_ji':  torch.tensor(window[feature_names].values, dtype=torch.float64),\n",
    "            \n",
    "            # read counts per gene j, site i\n",
    "            'X_ji': torch.tensor(window['score'].values, dtype=torch.float64),\n",
    "            \n",
    "            # read depth * initiation rate values per gene j\n",
    "            'C_j': torch.tensor(window['lambda_alphaj'].iloc[0], dtype=torch.float64),\n",
    "            \n",
    "            # GLM elongation rate predictions per gene j, site i\n",
    "            'Z_ji': torch.tensor(window['zeta'].values, dtype=torch.float64),\n",
    "            \n",
    "            # one-hot encoded sequences\n",
    "            'N_ji': torch.tensor(window[nucleotides].values, dtype=torch.float64), \n",
    "            'Length': len(window)\n",
    "        }\n",
    "    \n",
    "        self.cache[gene_id] = result\n",
    "\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Batch subsequences with same gene id together \"\"\"\n",
    "class GeneIdBatchSampler(BatchSampler):\n",
    "    def __init__(self, dataset):\n",
    "        self.dataset = dataset\n",
    "        self.batches = self._create_batches()\n",
    "\n",
    "    def _create_batches(self):\n",
    "        # Group indices by GeneId\n",
    "        gene_id_to_indices = {}\n",
    "        for idx in range(len(self.dataset)):\n",
    "            gene_id = self.dataset[idx]['GeneId']\n",
    "            if gene_id not in gene_id_to_indices:\n",
    "                gene_id_to_indices[gene_id] = []\n",
    "            gene_id_to_indices[gene_id].append(idx)\n",
    "\n",
    "        return list(gene_id_to_indices.values())\n",
    "\n",
    "    def __iter__(self):\n",
    "        for batch in self.batches:\n",
    "            yield batch\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dataset(data, use_sliding_window, window_size=None):\n",
    "    dataset = GeneDataset(data, use_sliding_window, window_size)\n",
    "    batch_sampler = GeneIdBatchSampler(dataset)\n",
    "    loader = DataLoader(dataset, batch_sampler=batch_sampler, shuffle=False, num_workers=7)\n",
    "    return loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def build_model(config):\n",
    "\n",
    "    class EpLinearModel(nn.Module):\n",
    "        def __init__(self, input_size):\n",
    "            super(EpLinearModel, self).__init__()\n",
    "            self.name = \"ep_linear\"\n",
    "            self.linear = nn.Linear(input_size, 1)\n",
    "\n",
    "        def forward(self, Y_ji):\n",
    "            x = self.linear(Y_ji)\n",
    "            return x.squeeze(-1)   \n",
    "    \n",
    "    class EpSeqLinearModel(nn.Module):\n",
    "        def __init__(self, num_ep_features, num_seq_features):\n",
    "            super(EpSeqLinearModel, self).__init__()\n",
    "            self.name = \"ep_seq_linear\"\n",
    "            self.y_linear = nn.Linear(num_ep_features, 1)\n",
    "            self.n_linear = nn.Linear(num_seq_features, 1)\n",
    "            self.final_linear = nn.Linear(2, 1)\n",
    "\n",
    "        def forward(self, Y_ji, N_ji):\n",
    "            y = self.y_linear(Y_ji)\n",
    "            n = self.n_linear(N_ji)\n",
    "            x = torch.cat((y, n), axis=-1)\n",
    "            x = self.final_linear(x)\n",
    "            return x.squeeze(-1)\n",
    "    \n",
    "    class LSTMModel(nn.Module):\n",
    "        def __init__(self, input_size, layer_size, num_layers, bidirectional):\n",
    "            super(LSTMModel, self).__init__()\n",
    "            self.name = \"lstm\"\n",
    "            self.lstm = nn.LSTM(input_size, layer_size, num_layers, bidirectional=bidirectional, batch_first=True)\n",
    "            self.bidirectional_linear = nn.Linear(2 * layer_size, 1)\n",
    "            self.linear = nn.Linear(layer_size, 1)\n",
    "            self.bidirectional = bidirectional\n",
    "\n",
    "        def forward(self, Y_ji, N_ji):\n",
    "            x = torch.cat((Y_ji, N_ji), axis=-1)\n",
    "            x, _ = self.lstm(x)\n",
    "            if self.bidirectional:\n",
    "                x = self.bidirectional_linear(x)\n",
    "            else:\n",
    "                x = self.linear(x)\n",
    "            return x.squeeze(-1)\n",
    "        \n",
    "    class CNN(nn.Module):\n",
    "        def __init__(self, num_ep_features, num_seq_features, \n",
    "                     y_channels, y_kernel_sizes,\n",
    "                     n_channels, n_kernel_sizes, dropout, \n",
    "                     lstm_layer_size, num_lstm_layers=None, bidirectional=False):\n",
    "            \n",
    "            super(CNN, self).__init__()\n",
    "            self.name = \"cnn\"            \n",
    "\n",
    "            self.y_convs = nn.ModuleList()\n",
    "            y_in_channels = num_ep_features\n",
    "            \n",
    "            if len(y_kernel_sizes) != len(y_channels):\n",
    "                raise InvalidConfigurationError(\"Y_ji channels and Y_ji kernels lists must have matching lengths\")\n",
    "            \n",
    "            # Y_ji convolutional layers\n",
    "            for idx, out_channels in enumerate(y_channels):\n",
    "                self.y_convs.append(\n",
    "                    nn.Conv1d(y_in_channels, out_channels, y_kernel_sizes[idx], stride=1, padding='same')\n",
    "                )\n",
    "                y_in_channels = out_channels\n",
    "            \n",
    "            \n",
    "            self.n_convs = nn.ModuleList()\n",
    "            n_in_channels = num_seq_features\n",
    "            \n",
    "            if len(n_kernel_sizes) != len(n_channels):\n",
    "                raise InvalidConfigurationError(\"N_ji channels and N_ji kernels lists must have matching lengths\")\n",
    "\n",
    "            for idx, out_channels in enumerate(n_channels):\n",
    "                self.n_convs.append(\n",
    "                    nn.Conv1d(n_in_channels, out_channels, n_kernel_sizes[idx], stride=1, padding='same')\n",
    "                )\n",
    "                n_in_channels = out_channels\n",
    "\n",
    "            self.relu = nn.ReLU()\n",
    "            self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "            # Final convolutional layer to map to a single output channel\n",
    "            # Since the output needs to be (batch_size, seq_len), we map the final features to 1\n",
    "            self.final_conv = nn.Conv1d(y_channels[-1] + n_channels[-1], 1, 1)  # 1x1 convolution\n",
    "            \n",
    "            self.num_lstm_layers = num_lstm_layers\n",
    "            if num_lstm_layers != 0 and num_lstm_layers != None:\n",
    "                self.gru = nn.GRU(input_size=y_channels[-1] + n_channels[-1], hidden_size=lstm_layer_size, num_layers=num_lstm_layers, bidirectional=bidirectional, batch_first=True)\n",
    "            \n",
    "            self.final_linear = nn.Linear(lstm_layer_size, 1)\n",
    "            self.bidirectional = True\n",
    "            self.final_bidirectional_linear = nn.Linear(lstm_layer_size*2, 1)\n",
    "            #self.batch_norm = nn.BatchNorm1d(y_hidden_layer_sizes[-1] + n_hidden_layer_sizes[-1])\n",
    "            \n",
    "        def forward(self, Y_ji, N_ji):\n",
    "            Y_ji = Y_ji.permute(0, 2, 1)  \n",
    "            N_ji = N_ji.permute(0, 2, 1)\n",
    "            \n",
    "            for conv in self.y_convs:\n",
    "                Y_ji = conv(Y_ji)\n",
    "                Y_ji = self.relu(Y_ji)\n",
    "                Y_ji = self.dropout(Y_ji)\n",
    "            \n",
    "            for conv in self.n_convs:\n",
    "                N_ji = conv(N_ji)\n",
    "                N_ji = self.relu(N_ji)\n",
    "                N_ji = self.dropout(N_ji)\n",
    "\n",
    "            x = torch.cat((Y_ji, N_ji), 1)\n",
    "            \n",
    "            #x = self.batch_norm(x)\n",
    "            \n",
    "            if self.num_lstm_layers != 0 and self.num_lstm_layers != None:\n",
    "                x = x.permute(0,2,1)\n",
    "                x, (h_n, c_n) = self.gru(x)\n",
    "                if self.bidirectional:\n",
    "                    x = self.final_bidirectional_linear(x)\n",
    "                else:\n",
    "                    x = self.final_linear(x)\n",
    "                x = x.squeeze(-1)\n",
    "            \n",
    "            else:\n",
    "                x = self.final_conv(x)\n",
    "                x = x.squeeze(1)  \n",
    "                \n",
    "            return x\n",
    "    \n",
    "    if config[\"model_type\"] == 'lstm':\n",
    "        model = LSTMModel(num_features + num_seq_features, config[\"lstm_layer_size\"], config[\"num_lstm_layers\"], config[\"bidirectional\"])\n",
    "    elif config[\"model_type\"] == 'ep_seq_linear':\n",
    "        model = EpSeqLinearModel(num_features, num_seq_features)\n",
    "    elif config[\"model_type\"] == 'ep_linear':\n",
    "        model = EpLinearModel(num_features)\n",
    "    elif config[\"model_type\"] == 'cnn':\n",
    "        lstm_layer_size = None\n",
    "        bidirectional = None\n",
    "        if config[\"num_lstm_layers\"] != 0 and config[\"num_lstm_layers\"] != None:\n",
    "            lstm_layer_size = config[\"lstm_layer_size\"]\n",
    "            bidirectional = config[\"bidirectional\"]\n",
    "        model = CNN(num_features, num_seq_features, config[\"y_channels\"], config[\"y_kernel_sizes\"], \n",
    "                    config[\"n_channels\"], config[\"n_kernel_sizes\"], config[\"dropout\"], \n",
    "                    config[\"num_lstm_layers\"], lstm_layer_size, bidirectional)\n",
    "    \n",
    "    if cuda_available:\n",
    "        if num_gpus > 1:\n",
    "            print(\"Using\", num_gpus, \"GPUs\")\n",
    "            model = torch.nn.DataParallel(model)\n",
    "        model = model.to('cuda')\n",
    "\n",
    "    print(model)\n",
    "\n",
    "    \n",
    "    model.double()\n",
    "\n",
    "    return model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_optimizer(network, learning_rate, l2_lambda):\n",
    "    optimizer = optim.Adam(network.parameters(), lr=learning_rate, weight_decay=l2_lambda)\n",
    "    return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def valid_epoch(model, loader, loss_fn):\n",
    "    model.eval()\n",
    "    total_neural_net_loss = 0\n",
    "    total_glm_loss = 0\n",
    "    neural_net_zeta = []\n",
    "    glm_zeta = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for idx, batch in enumerate(loader):\n",
    "            Y_ji_batch = batch['Y_ji'].to(device)\n",
    "            X_ji_batch = batch['X_ji'].to(device)\n",
    "            N_ji_batch = batch['N_ji'].to(device) \n",
    "            C_j_batch = batch['C_j'].to(device).unsqueeze(1)\n",
    "            Z_ji_batch = batch['Z_ji'].to(device)\n",
    "            \n",
    "            if model.name == \"ep_linear\":\n",
    "                outputs = model(Y_ji_batch)\n",
    "            else:\n",
    "                outputs = model(Y_ji_batch, N_ji_batch)\n",
    "                \n",
    "            neural_net_loss = loss_fn(X_ji_batch, C_j_batch, outputs)\n",
    "            glm_loss = loss_fn(X_ji_batch, C_j_batch, torch.log(Z_ji_batch))\n",
    "\n",
    "            total_neural_net_loss +=  neural_net_loss.item()\n",
    "            total_glm_loss += glm_loss.item()\n",
    "            \n",
    "            # store all predictions in list\n",
    "            neural_net_zeta.append(torch.exp(outputs.cpu().flatten()))\n",
    "            glm_zeta.append(batch['Z_ji'].flatten())\n",
    "    \n",
    "    # calculate average loss across all batches\n",
    "    avg_neural_net_loss = total_neural_net_loss / len(loader)\n",
    "    avg_glm_loss = total_glm_loss / len(loader)\n",
    "    \n",
    "    neural_net_zeta = torch.cat(neural_net_zeta)\n",
    "    glm_zeta = torch.cat(glm_zeta)\n",
    "    \n",
    "    return avg_neural_net_loss, avg_glm_loss, neural_net_zeta, glm_zeta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, loader, optimizer, loss_fn, l1_lambda):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for idx, batch in enumerate(loader):\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        Y_ji_batch = batch['Y_ji'].to(device) \n",
    "        X_ji_batch = batch['X_ji'].to(device)\n",
    "        N_ji_batch = batch['N_ji'].to(device) \n",
    "        C_j_batch = batch['C_j'].to(device).unsqueeze(1)\n",
    "        \n",
    "        if model.name == \"ep_linear\":\n",
    "            outputs = model(Y_ji_batch)\n",
    "        else:\n",
    "            outputs = model(Y_ji_batch, N_ji_batch)\n",
    "        \n",
    "        loss = loss_fn(X_ji_batch, C_j_batch, outputs)\n",
    "        \n",
    "        if l1_lambda != 0:\n",
    "            l1_norm = sum(torch.abs(p).sum() for p in model.parameters())\n",
    "            loss = loss + l1_lambda * l1_norm\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # calculate average loss across all batches\n",
    "        total_loss += loss.item()\n",
    "    avg_train_loss = total_loss / len(loader)\n",
    "    \n",
    "    return avg_train_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CustomLoss, self).__init__()\n",
    "\n",
    "    def forward(self, X_ji, C_j, rho_ji):\n",
    "        loss = X_ji * rho_ji + C_j * torch.exp(-rho_ji) - X_ji * torch.log(C_j)\n",
    "        \n",
    "        # calculate average loss within each batch\n",
    "        return (loss).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(config_folder_path + config_file_name + \".json\", 'r') as file:\n",
    "    config = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "increase_cut=0.00001\n",
    "patience=5\n",
    "\n",
    "def train():\n",
    "    model = build_model(config)\n",
    "    \n",
    "    train_window_size = None\n",
    "    if config[\"train_use_sliding_window\"]:\n",
    "        train_window_size = config[\"train_window_size\"]\n",
    "    train_loader = build_dataset(train_data, config[\"train_use_sliding_window\"], train_window_size)\n",
    "    \n",
    "    valid_window_size = None\n",
    "    if config[\"valid_use_sliding_window\"]:\n",
    "        valid_window_size = config[\"valid_window_size\"]\n",
    "    valid_loader = build_dataset(valid_data, config[\"valid_use_sliding_window\"], valid_window_size)\n",
    "    \n",
    "    optimizer = build_optimizer(model, config['learning_rate'], config['l2_lambda'])\n",
    "    \n",
    "    loss_fn = torch.jit.script(CustomLoss())\n",
    "    # track loss curves\n",
    "    loss_neural_net_train = [0] * config[\"epochs\"]\n",
    "    loss_neural_net_valid = [0] * config[\"epochs\"]\n",
    "    loss_glm_valid = [0] * config[\"epochs\"]\n",
    "    \n",
    "    # scheduler to reduce learning rate by half when new validation loss > old validation loss\n",
    "    old_neural_net_valid_loss = float('inf')\n",
    "    epochs_no_improve = 0\n",
    "    scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=1, verbose=True)\n",
    "\n",
    "    for epoch in range(config[\"epochs\"]):\n",
    "        print(f'Epoch {epoch+1}')\n",
    "        \n",
    "        train_loss = train_epoch(model, train_loader, optimizer, loss_fn, config['l1_lambda'])\n",
    "        loss_neural_net_train[epoch] = train_loss\n",
    "        print(f\"train loss: {train_loss: .5f}\")\n",
    "        \n",
    "        valid_neural_net_loss, valid_glm_loss, neural_net_zeta, glm_zeta = valid_epoch(model, valid_loader, loss_fn)\n",
    "        loss_neural_net_valid[epoch] = valid_neural_net_loss\n",
    "        loss_glm_valid[epoch] = valid_glm_loss\n",
    "        print(f\"valid neural net loss: {valid_neural_net_loss: .5f}\")\n",
    "        print(f\"valid glm loss: {valid_glm_loss: .5f}\")\n",
    "        \n",
    "        # compute metrics\n",
    "        mae = F.l1_loss(neural_net_zeta.squeeze(), glm_zeta)\n",
    "        mse = F.mse_loss(neural_net_zeta.squeeze(), glm_zeta)\n",
    "        correlation_coefficient = np.corrcoef(glm_zeta, neural_net_zeta.squeeze())[0, 1]\n",
    "        print(\"Correlation Coefficient:\", correlation_coefficient)\n",
    "        print(f\"Mean Absolute Error: {mae.item():.4f}\")\n",
    "        print(f\"Mean Squared Error: {mse.item():.4f}\")\n",
    "        \n",
    "        if config[\"wandb_logging\"]:\n",
    "            wandb.log({\"epoch\": epoch, \"train_loss\": train_loss, \"valid_neural_net_loss\": valid_neural_net_loss,\n",
    "           \"valid_glm_loss\": valid_glm_loss, \"correlation_coefficient\": correlation_coefficient,\n",
    "           \"mae\": mae, \"mse\": mse})\n",
    "        \n",
    "        # early stopping\n",
    "        if valid_neural_net_loss < old_neural_net_valid_loss - increase_cut:\n",
    "            epochs_no_improve = 0\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "            \n",
    "        if epochs_no_improve == patience:\n",
    "            print(\"Early Stopping\")\n",
    "            break\n",
    "        \n",
    "        # reduce learning rate if new loss > old loss\n",
    "        if valid_neural_net_loss > old_neural_net_valid_loss:\n",
    "            optimizer.param_groups[0]['lr'] *= 0.5\n",
    "            print(f\"Reduced learning rate to {optimizer.param_groups[0]['lr']}\")\n",
    "            \n",
    "        old_neural_net_valid_loss = valid_neural_net_loss\n",
    "        scheduler.step(valid_neural_net_loss)\n",
    "        \n",
    "    return model, loss_neural_net_train, loss_neural_net_valid, loss_glm_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config[\"wandb_logging\"]:\n",
    "    metric = {\n",
    "        'name': 'valid_neural_net_loss',\n",
    "        'goal': 'minimize'   \n",
    "    }\n",
    "    sweep_config['metric'] = metric\n",
    "    sweep_config['parameters'] = config\n",
    "    sweep_id = wandb.sweep(sweep_config, project=config_file_name)\n",
    "    wandb.init(config=config)\n",
    "    wandb.agent(sweep_id, train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/grid/siepel/home_norepl/hassett/.local/lib/python3.7/site-packages/torch/nn/init.py:388: UserWarning: Initializing zero-element tensors is a no-op\n",
      "  warnings.warn(\"Initializing zero-element tensors is a no-op\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN(\n",
      "  (y_convs): ModuleList(\n",
      "    (0): Conv1d(10, 16, kernel_size=(9,), stride=(1,), padding=same)\n",
      "  )\n",
      "  (n_convs): ModuleList(\n",
      "    (0): Conv1d(4, 32, kernel_size=(3,), stride=(1,), padding=same)\n",
      "    (1): Conv1d(32, 64, kernel_size=(5,), stride=(1,), padding=same)\n",
      "    (2): Conv1d(64, 128, kernel_size=(9,), stride=(1,), padding=same)\n",
      "  )\n",
      "  (relu): ReLU()\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      "  (final_conv): Conv1d(144, 1, kernel_size=(1,), stride=(1,))\n",
      "  (final_linear): Linear(in_features=0, out_features=1, bias=True)\n",
      "  (final_bidirectional_linear): Linear(in_features=0, out_features=1, bias=True)\n",
      ")\n",
      "Epoch 1\n",
      "train loss:  0.13781\n",
      "valid neural net loss:  0.13904\n",
      "valid glm loss:  0.14891\n",
      "Correlation Coefficient: 0.1827971483559709\n",
      "Mean Absolute Error: 0.5740\n",
      "Mean Squared Error: 0.5264\n",
      "Epoch 2\n",
      "train loss:  0.13470\n",
      "valid neural net loss:  0.13846\n",
      "valid glm loss:  0.14891\n",
      "Correlation Coefficient: 0.21406603099570096\n",
      "Mean Absolute Error: 0.6116\n",
      "Mean Squared Error: 0.6382\n",
      "Epoch 3\n",
      "train loss:  0.13402\n",
      "valid neural net loss:  0.13810\n",
      "valid glm loss:  0.14891\n",
      "Correlation Coefficient: 0.23231950276033372\n",
      "Mean Absolute Error: 0.6530\n",
      "Mean Squared Error: 0.7847\n",
      "Epoch 4\n",
      "train loss:  0.13351\n",
      "valid neural net loss:  0.13783\n",
      "valid glm loss:  0.14891\n",
      "Correlation Coefficient: 0.24931480171142087\n",
      "Mean Absolute Error: 0.6972\n",
      "Mean Squared Error: 0.9435\n",
      "Epoch 5\n",
      "train loss:  0.13314\n",
      "valid neural net loss:  0.13764\n",
      "valid glm loss:  0.14891\n",
      "Correlation Coefficient: 0.26625687315760393\n",
      "Mean Absolute Error: 0.7365\n",
      "Mean Squared Error: 1.0845\n",
      "Epoch 6\n",
      "train loss:  0.13283\n",
      "valid neural net loss:  0.13749\n",
      "valid glm loss:  0.14891\n",
      "Correlation Coefficient: 0.2807971054535491\n",
      "Mean Absolute Error: 0.7792\n",
      "Mean Squared Error: 1.2462\n",
      "Epoch 7\n",
      "train loss:  0.13257\n",
      "valid neural net loss:  0.13741\n",
      "valid glm loss:  0.14891\n",
      "Correlation Coefficient: 0.2883212770336736\n",
      "Mean Absolute Error: 0.8111\n",
      "Mean Squared Error: 1.3712\n",
      "Epoch 8\n",
      "train loss:  0.13235\n",
      "valid neural net loss:  0.13732\n",
      "valid glm loss:  0.14891\n",
      "Correlation Coefficient: 0.291376870271226\n",
      "Mean Absolute Error: 0.8493\n",
      "Mean Squared Error: 1.5369\n",
      "Epoch 9\n",
      "train loss:  0.13213\n",
      "valid neural net loss:  0.13723\n",
      "valid glm loss:  0.14891\n",
      "Correlation Coefficient: 0.2945005294501784\n",
      "Mean Absolute Error: 0.8738\n",
      "Mean Squared Error: 1.6391\n",
      "Epoch 10\n",
      "train loss:  0.13199\n",
      "valid neural net loss:  0.13718\n",
      "valid glm loss:  0.14891\n",
      "Correlation Coefficient: 0.29761450170654125\n",
      "Mean Absolute Error: 0.8917\n",
      "Mean Squared Error: 1.7155\n",
      "Epoch 11\n",
      "train loss:  0.13180\n",
      "valid neural net loss:  0.13709\n",
      "valid glm loss:  0.14891\n",
      "Correlation Coefficient: 0.29293512973654723\n",
      "Mean Absolute Error: 0.9208\n",
      "Mean Squared Error: 1.8595\n",
      "Epoch 12\n",
      "train loss:  0.13166\n",
      "valid neural net loss:  0.13703\n",
      "valid glm loss:  0.14891\n",
      "Correlation Coefficient: 0.2917244919417107\n",
      "Mean Absolute Error: 0.9344\n",
      "Mean Squared Error: 1.9277\n",
      "Epoch 13\n",
      "train loss:  0.13152\n",
      "valid neural net loss:  0.13697\n",
      "valid glm loss:  0.14891\n",
      "Correlation Coefficient: 0.2902175528927001\n",
      "Mean Absolute Error: 0.9491\n",
      "Mean Squared Error: 2.0005\n",
      "Epoch 14\n",
      "train loss:  0.13141\n",
      "valid neural net loss:  0.13691\n",
      "valid glm loss:  0.14891\n",
      "Correlation Coefficient: 0.28773680470351415\n",
      "Mean Absolute Error: 0.9576\n",
      "Mean Squared Error: 2.0535\n",
      "Epoch 15\n",
      "train loss:  0.13126\n",
      "valid neural net loss:  0.13685\n",
      "valid glm loss:  0.14891\n",
      "Correlation Coefficient: 0.28380012789794073\n",
      "Mean Absolute Error: 0.9769\n",
      "Mean Squared Error: 2.1631\n",
      "Epoch 16\n",
      "train loss:  0.13115\n",
      "valid neural net loss:  0.13679\n",
      "valid glm loss:  0.14891\n",
      "Correlation Coefficient: 0.28159151922634124\n",
      "Mean Absolute Error: 0.9905\n",
      "Mean Squared Error: 2.2426\n",
      "Epoch 17\n",
      "train loss:  0.13102\n",
      "valid neural net loss:  0.13670\n",
      "valid glm loss:  0.14891\n",
      "Correlation Coefficient: 0.2795972580984296\n",
      "Mean Absolute Error: 1.0093\n",
      "Mean Squared Error: 2.3515\n",
      "Epoch 18\n",
      "train loss:  0.13089\n",
      "valid neural net loss:  0.13665\n",
      "valid glm loss:  0.14891\n",
      "Correlation Coefficient: 0.27769157360930585\n",
      "Mean Absolute Error: 1.0171\n",
      "Mean Squared Error: 2.4008\n",
      "Epoch 19\n",
      "train loss:  0.13078\n",
      "valid neural net loss:  0.13659\n",
      "valid glm loss:  0.14891\n",
      "Correlation Coefficient: 0.2743804912690732\n",
      "Mean Absolute Error: 1.0280\n",
      "Mean Squared Error: 2.4786\n",
      "Epoch 20\n",
      "train loss:  0.13070\n",
      "valid neural net loss:  0.13654\n",
      "valid glm loss:  0.14891\n",
      "Correlation Coefficient: 0.27383333475750987\n",
      "Mean Absolute Error: 1.0380\n",
      "Mean Squared Error: 2.5373\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "if config[\"wandb_logging\"] == False:\n",
    "    model, loss_neural_net_train, loss_neural_net_valid, loss_glm_valid = train()\n",
    "\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    filename = f\"models/{config_file_name}.pth\"\n",
    "    torch.save(model.state_dict(), filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'sn_channels'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/tmp/7081267.1.gpu.q/ipykernel_3075764/1604919813.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m model = build_model(config[\"model_type\"], num_features, num_seq_features, \n\u001b[1;32m      4\u001b[0m                     \u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"y_channels\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"y_kernel_sizes\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m                     \u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"sn_channels\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"n_kernel_sizes\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"dropout\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m                     config[\"lstm_layer_size\"], config[\"num_lstm_layers\"], config[\"bidirectional\"])\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'sn_channels'"
     ]
    }
   ],
   "source": [
    "# load model state\n",
    "\n",
    "model = build_model(config[\"model_type\"], num_features, num_seq_features, \n",
    "                    config[\"y_channels\"], config[\"y_kernel_sizes\"],\n",
    "                    config[\"n_channels\"], config[\"n_kernel_sizes\"], config[\"dropout\"],\n",
    "                    config[\"lstm_layer_size\"], config[\"num_lstm_layers\"], config[\"bidirectional\"])\n",
    "\n",
    "model.load_state_dict(torch.load(f\"./models/{config_file_name}.pth\", map_location=torch.device('cpu')))\n",
    "\n",
    "cuda_available = torch.cuda.is_available()\n",
    "print(\"CUDA (GPU support) is available:\", cuda_available)\n",
    "num_gpus = torch.cuda.device_count()\n",
    "print(\"Number of GPUs available:\", num_gpus)\n",
    "if cuda_available:\n",
    "    if num_gpus > 1:\n",
    "        print(\"Using\", num_gpus, \"GPUs\")\n",
    "        model = torch.nn.DataParallel(model)\n",
    "    model = model.to('cuda')\n",
    "\n",
    "first_param_device = next(model.parameters()).device\n",
    "print(\"Model is on device:\", first_param_device)\n",
    "\n",
    "model.double()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function matplotlib.pyplot.show(close=None, block=None)>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "epochs = range(1, len(loss_neural_net_train) + 1)\n",
    "plt.plot(epochs, loss_neural_net_train, label='train_neural_net_loss')\n",
    "plt.plot(epochs, loss_neural_net_valid, label='valid_neural_net_loss')\n",
    "plt.plot(epochs, loss_glm_valid, label='valid_glm_loss')\n",
    "\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.savefig(\"./results/cnn_1/loss_curves.png\")\n",
    "plt.show "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "\n",
    "tstdl = build_dataset(test_data, False, None)\n",
    "data_iter = iter(tstdl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_data(glm_zeta, net_zeta, file_name, plot_first_window, window_size):\n",
    "    if plot_first_window:\n",
    "        indices = range(window_size)\n",
    "    else:\n",
    "        indices = range(len(glm_zeta))\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(10, 5))\n",
    "    \n",
    "    if plot_first_window:\n",
    "        net_zeta = net_zeta[0:window_size]\n",
    "        glm_zeta = glm_zeta[0:window_size]\n",
    "    \n",
    "    ax.scatter(indices, net_zeta, color='blue', label='Neural Net Zeta', s=10, alpha=0.5)\n",
    "    ax.scatter(indices, glm_zeta, color='orange', label='GLM Zeta', s=10, alpha=0.5)\n",
    "    \n",
    "    ax.set_title('Neural Net vs GLM Elongation Rate')\n",
    "    ax.set_xlabel('Index')\n",
    "    ax.set_ylabel('Elongation Rate')\n",
    "    ax.legend()\n",
    "    \n",
    "    plt.ylim(0, 6.5)\n",
    "\n",
    "    plt.savefig(file_name)\n",
    "    plt.close()\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_dir = results_folder_path + config_file_name\n",
    "os.makedirs(results_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute metrics for test dataset\n",
    "\n",
    "loss_fn = CustomLoss()\n",
    "\n",
    "total_net_loss = 0\n",
    "total_glm_loss = 0\n",
    "net_zeta = []\n",
    "glm_zeta = []\n",
    "with torch.no_grad():\n",
    "    for batch in tstdl:\n",
    "        Y_ji = batch['Y_ji'].to(device)\n",
    "        N_ji = batch['N_ji'].to(device)\n",
    "        X_ji = batch['X_ji'].to(device)\n",
    "        C_j = batch['C_j'].to(device).unsqueeze(1)\n",
    "        Z_ji = batch['Z_ji'].to(device)\n",
    "        \n",
    "        if model.name == \"ep_linear\":\n",
    "            rho_ji = model(Y_ji)\n",
    "        else:\n",
    "            rho_ji = model(Y_ji, N_ji)\n",
    "        \n",
    "        net_loss = loss_fn(X_ji, C_j, rho_ji)\n",
    "        glm_loss = loss_fn(X_ji, C_j, torch.log(Z_ji))\n",
    "        \n",
    "        total_net_loss += net_loss.item()\n",
    "        total_glm_loss += glm_loss.item()\n",
    "        \n",
    "        net_zeta.append(torch.exp(rho_ji.cpu()).flatten())\n",
    "        glm_zeta.append(batch['Z_ji'].flatten())\n",
    "\n",
    "        \n",
    "net_zeta = torch.cat(net_zeta)\n",
    "glm_zeta = torch.cat(glm_zeta)\n",
    "\n",
    "mae = F.l1_loss(net_zeta, glm_zeta)\n",
    "mse = F.mse_loss(net_zeta, glm_zeta)\n",
    "\n",
    "correlation_coefficient = np.corrcoef(glm_zeta, net_zeta)[0, 1]\n",
    "\n",
    "file_path = f\"{results_dir}/metrics.txt\"\n",
    "\n",
    "with open(file_path, \"w\") as file:\n",
    "    file.write(f\"Neural Net Loss: {total_net_loss/len(tstdl):.4f}\\n\")\n",
    "    file.write(f\"GLM Loss: {total_glm_loss/len(tstdl):.4f}\\n\")\n",
    "    file.write(f\"Correlation Coefficient: {correlation_coefficient:.4f}\\n\")\n",
    "    file.write(f\"Mean Absolute Error: {mae.item():.4f}\\n\")\n",
    "    file.write(f\"Mean Squared Error: {mse.item():.4f}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of samples: 11\n",
      "number of samples: 11\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "\n",
    "# plot for subset of genes in test dataset\n",
    "for i in range(0, 3):\n",
    "    inputs = next(data_iter) \n",
    "    print(\"number of samples: \" + str(len(inputs)))\n",
    "\n",
    "    with torch.no_grad():\n",
    "        Y_ji = inputs['Y_ji'].to(device)\n",
    "        N_ji = inputs['N_ji'].to(device)\n",
    "        if model.name == \"ep_linear\":\n",
    "            rho_ji = model(Y_ji)\n",
    "        else:\n",
    "            rho_ji = model(Y_ji, N_ji)\n",
    "\n",
    "    glm_zeta = inputs['Z_ji'][0]\n",
    "    # convert log(Z) outputs to Z\n",
    "    net_zeta = torch.exp(rho_ji.cpu().squeeze())\n",
    "\n",
    "    plot_data(glm_zeta, net_zeta, f\"{results_dir}/plot_{i}\", False, 0)\n",
    "    plot_data(glm_zeta, net_zeta, f\"{results_dir}/plot_w100_{i}\", True, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Anaconda3 (Python 3.7.6)",
   "language": "python",
   "name": "anaconda3_2020.02"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
