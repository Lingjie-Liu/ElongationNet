{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Master File w/ hyperparameter sweeping across multiple architectures\"\"\"\n",
    "# Run CPU-only, GPU code needs further testing\n",
    "\"\"\"\n",
    "Restart kernel after running\n",
    "Only need to run once\n",
    "\"\"\"\n",
    "!pip install scikit-learn matplotlib seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/grid/siepel/home_norepl/hassett/.local/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3553: DtypeWarning: Columns (0) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as colors\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.utils.data as td\n",
    "import torch.nn as nn\n",
    "import torch.nn.init as init\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torch.utils.data import DataLoader, Dataset, Subset\n",
    "import wandb\n",
    "\n",
    "\n",
    "froot = './data/k562_samp_epft_norm_test_1.csv'\n",
    "\n",
    "df = pd.read_csv(froot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  seqnames     start       end strand  ensembl_gene_id  score      ctcf  \\\n",
      "0       15  88623545  88623545      +  ENSG00000181026    0.0 -0.079992   \n",
      "1       15  88623546  88623546      +  ENSG00000181026    0.0 -0.079942   \n",
      "2       15  88623547  88623547      +  ENSG00000181026    0.0 -0.079893   \n",
      "3       15  88623548  88623548      +  ENSG00000181026    0.0 -0.079844   \n",
      "4       15  88623549  88623549      +  ENSG00000181026    0.0 -0.079796   \n",
      "\n",
      "   h3k36me3   h3k4me1  h3k79me2   h3k9me1   h3k9me3  h4k20me1       sj5  \\\n",
      "0 -0.000099  0.348531  4.423451  0.446508 -0.168099  3.232475 -0.028916   \n",
      "1  0.001638  0.352677  4.460072  0.453024 -0.169218  3.259194 -0.028916   \n",
      "2  0.003360  0.356807  4.496664  0.459491 -0.170339  3.285849 -0.028916   \n",
      "3  0.005065  0.360919  4.533223  0.465908 -0.171461  3.312435 -0.028916   \n",
      "4  0.006754  0.365013  4.569743  0.472274 -0.172584  3.338952 -0.028916   \n",
      "\n",
      "        sj3       dms  wgbs      rpts  lambda_alphaj      zeta  \n",
      "0 -0.057178 -0.307549   0.0  0.249626       0.052255  0.993283  \n",
      "1 -0.057178 -0.307549   0.0  0.249626       0.052255  0.992642  \n",
      "2 -0.057178 -0.307549   0.0  0.249626       0.052255  0.992008  \n",
      "3 -0.057178 -0.307549   0.0  0.249626       0.052255  0.991381  \n",
      "4 -0.057178 -0.307549   0.0  0.249626       0.052255  0.990762  \n",
      "['ctcf' 'h3k36me3' 'h3k4me1' 'h3k79me2' 'h3k9me1' 'h3k9me3' 'h4k20me1'\n",
      " 'sj5' 'sj3' 'dms' 'wgbs' 'rpts']\n",
      "Number of Samples: 16182613\n",
      "Number of Features: 12\n",
      "(16182613, 12)\n",
      "CUDA (GPU support) is available: True\n",
      "Number of GPUs available: 1\n"
     ]
    }
   ],
   "source": [
    "print(df.head())\n",
    "\n",
    "column_names = np.array(df.columns)\n",
    "feature_names = column_names[6:-2]\n",
    "num_features = len(feature_names)\n",
    "print(feature_names)\n",
    "num_samples = df.shape[0]\n",
    "\n",
    "# process read counts\n",
    "X_ji = df['score'].values\n",
    "\n",
    "# process GLM simulated elongation rates\n",
    "Z_ji = df['zeta'].values\n",
    "\n",
    "print(\"Number of Samples: \" + str(num_samples))\n",
    "print(\"Number of Features: \" + str(num_features))\n",
    "\n",
    "#Y_ji is a list of samples containing lists of their feature values\n",
    "    # [   \n",
    "    #   sample_1: [feat_1, feat_2,...,feat_n],\n",
    "    #   sample_2: [feat_1, feat_2,...,feat_n],\n",
    "    # ]\n",
    "\n",
    "Y_ji = df.iloc[:, 6:-2].values\n",
    "Y_ji_shape = Y_ji.shape\n",
    "print(Y_ji.shape)\n",
    "\n",
    "C_j = df['lambda_alphaj'].values\n",
    "\n",
    "gene_ids = df['ensembl_gene_id'].values\n",
    "\n",
    "cuda_available = torch.cuda.is_available()\n",
    "print(\"CUDA (GPU support) is available:\", cuda_available)\n",
    "num_gpus = torch.cuda.device_count()\n",
    "print(\"Number of GPUs available:\", num_gpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GeneDataset(Dataset):\n",
    "    def __init__(self, grouped_data):\n",
    "        self.grouped_data = grouped_data\n",
    "        self.keys = list(grouped_data.groups.keys())\n",
    "        self.cache = {}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.keys)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if idx in self.cache:\n",
    "            return self.cache[idx]\n",
    "        \n",
    "        gene_id = self.keys[idx]\n",
    "        gene_data = self.grouped_data.get_group(gene_id)\n",
    "        \n",
    "        y_ji_array = np.array(gene_data['Y_ji'].tolist()).reshape(-1, 12)\n",
    "        y_ji_tensor = torch.tensor(y_ji_array, dtype=torch.float64)\n",
    "            \n",
    "        data = gene_data.drop(columns=['GeneId', 'dataset', 'Y_ji'])\n",
    "        tensor_data = torch.tensor(data.values, dtype=torch.float64)\n",
    "    \n",
    "        result = {\n",
    "            'GeneId': gene_id,\n",
    "            'Y_ji': y_ji_tensor,\n",
    "            'gene_length': len(gene_data)\n",
    "        }\n",
    "        for col in data.columns:\n",
    "            result[col] = tensor_data[:, data.columns.get_loc(col)]\n",
    "            \n",
    "        self.cache[idx] = result\n",
    "\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train data size: 12713808\n",
      "val data size: 1798949\n",
      "test data size: 1669856\n",
      "train data size: 415\n",
      "val data size: 52\n",
      "test data size: 52\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data = pd.DataFrame({\n",
    "    'GeneId': gene_ids,\n",
    "    'Y_ji': [row for row in Y_ji],\n",
    "    'X_ji': X_ji,\n",
    "    'C_j': C_j,\n",
    "    'Z_ji': Z_ji\n",
    "})\n",
    "\n",
    "grouped = data.groupby('GeneId')\n",
    "\n",
    "train_idx, temp_idx = train_test_split(list(grouped.groups.keys()), test_size=0.2, random_state=42)\n",
    "\n",
    "val_idx, test_idx = train_test_split(temp_idx, test_size=0.5, random_state=42)\n",
    "\n",
    "dataset_mapping = {gene_id: 'train' for gene_id in train_idx}\n",
    "dataset_mapping.update({gene_id: 'val' for gene_id in val_idx})\n",
    "dataset_mapping.update({gene_id: 'test' for gene_id in test_idx})\n",
    "\n",
    "data['dataset'] = data['GeneId'].map(dataset_mapping)\n",
    "\n",
    "train_data = data[data['dataset'] == 'train']\n",
    "valid_data = data[data['dataset'] == 'val']\n",
    "test_data = data[data['dataset'] == 'test']\n",
    "\n",
    "print(\"train data size: \" + str(len(train_data)))\n",
    "print(\"val data size: \" + str(len(valid_data)))\n",
    "print(\"test data size: \" + str(len(test_data)))\n",
    "\n",
    "train_data = train_data.groupby('GeneId')\n",
    "valid_data = valid_data.groupby('GeneId')\n",
    "test_data = test_data.groupby('GeneId')\n",
    "\n",
    "print(\"train data size: \" + str(len(train_data)))\n",
    "print(\"val data size: \" + str(len(valid_data)))\n",
    "print(\"test data size: \" + str(len(test_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dataset(train_data, batch_size):#, custom_collate_fn):\n",
    "    dataset = GeneDataset(train_data)\n",
    "    loader = DataLoader(dataset, batch_size=batch_size,#, collate_fn=custom_collate_fn, \n",
    "                        num_workers=7, shuffle=False, pin_memory=True)\n",
    "    return loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def build_model(model_type, num_layers, hidden_layer_size, bidirectional):\n",
    "    \n",
    "    class LSTMModel(nn.Module):\n",
    "        def __init__(self, input_size, hidden_layer_size, output_size, num_layers, bidirectional):\n",
    "            super(LSTMModel, self).__init__()\n",
    "            self.lstm = nn.LSTM(input_size, hidden_layer_size, num_layers, bidirectional=bidirectional, batch_first=True)\n",
    "            #self.bidirectional_linear = nn.Linear(2 * hidden_layer_size, output_size)\n",
    "            self.linear = nn.Linear(hidden_layer_size, output_size)\n",
    "\n",
    "        def forward(self, x):\n",
    "            x, _ = self.lstm(x)\n",
    "            x = self.linear(x)\n",
    "            return x\n",
    "    \n",
    "    if model_type == 'lstm':\n",
    "        model = LSTMModel(num_features, hidden_layer_size, 1, num_layers, bidirectional)\n",
    "    elif model_type == 'linear':\n",
    "        model = nn.Linear(num_features, 1, bias=False)\n",
    "    \n",
    "    if cuda_available:\n",
    "        if num_gpus > 1:\n",
    "            print(\"Using\", num_gpus, \"GPUs\")\n",
    "            model = torch.nn.DataParallel(model)\n",
    "        model = model.to('cuda')\n",
    "\n",
    "    print(model)\n",
    "    \"\"\"\n",
    "    arr = torch.randn((1,12,2000)).to(device)\n",
    "    print(model(arr).shape)\n",
    "    nparm = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    print(\"Number of parameters: \" + str(nparm))\n",
    "    \"\"\"\n",
    "\n",
    "    first_param_device = next(model.parameters()).device\n",
    "    print(\"Model is on device:\", first_param_device)\n",
    "    \n",
    "    # expected weights are close to 0 which is why 0 initializing weights converges much quicker\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        for param in model.parameters():\n",
    "            param.zero_()\n",
    "    \"\"\"\n",
    "    \n",
    "    model.double()\n",
    "\n",
    "    return model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CustomLoss, self).__init__()\n",
    "\n",
    "    def forward(self, X_ji, C_j, rho_ji):\n",
    "        C_j_value = C_j[0]\n",
    "        X_ji = X_ji.squeeze(0)\n",
    "        loss = X_ji * rho_ji + C_j_value * torch.exp(-rho_ji) - X_ji * torch.log(C_j_value)\n",
    "        return (loss).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_optimizer(network, optimizer, learning_rate, momentum):\n",
    "    if optimizer == \"sgd\":\n",
    "        optimizer = optim.SGD(network.parameters(),\n",
    "                              lr=learning_rate, momentum=momentum)\n",
    "    # Adam optimizer adapts the learning rate for each parameter individually\n",
    "    elif optimizer == \"adam\":\n",
    "        optimizer = optim.Adam(network.parameters(),\n",
    "                               lr=learning_rate)\n",
    "    return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def valid_epoch(model, loader, loss_fn):\n",
    "    model.eval()\n",
    "    total_neural_net_loss = 0\n",
    "    total_glm_loss = 0\n",
    "    neural_net_zeta = []\n",
    "    glm_zeta = []\n",
    "    with torch.no_grad():\n",
    "        for idx, batch in enumerate(loader):\n",
    "            Y_ji_batch = batch['Y_ji'].to(device)\n",
    "            X_ji_batch = batch['X_ji'].to(device)\n",
    "            C_j_batch = batch['C_j'].to(device)\n",
    "            Z_ji_batch = batch['Z_ji'].to(device)\n",
    "            outputs = model(Y_ji_batch)\n",
    "            neural_net_loss = loss_fn(X_ji_batch, C_j_batch, outputs.squeeze(0).squeeze(1))\n",
    "            glm_loss = loss_fn(X_ji_batch, C_j_batch, Z_ji_batch)\n",
    "            #normalized_neural_net_loss =  neural_net_loss.item() / Y_ji_batch.shape[1] # normalize by seq length if using different lengths across different training runs\n",
    "            total_neural_net_loss +=  neural_net_loss.item()#normalized_neural_net_loss\n",
    "            #normalized_glm_loss = glm_loss.item() / Y_ji_batch.shape[1]\n",
    "            total_glm_loss += glm_loss.item()#normalized_glm_loss\n",
    "            neural_net_zeta.append(torch.exp(outputs.cpu()[0]))\n",
    "            glm_zeta.append(batch['Z_ji'][0])\n",
    "    avg_neural_net_loss = total_neural_net_loss / len(loader)\n",
    "    avg_glm_loss = total_glm_loss / len(loader)\n",
    "    neural_net_zeta = torch.cat(neural_net_zeta, dim=0)\n",
    "    glm_zeta = torch.cat(glm_zeta, dim=0)\n",
    "    return avg_neural_net_loss, avg_glm_loss, neural_net_zeta, glm_zeta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, loader, optimizer, loss_fn):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for idx, batch in enumerate(loader):\n",
    "        optimizer.zero_grad()\n",
    "        Y_ji_batch = batch['Y_ji'].to(device)\n",
    "        X_ji_batch = batch['X_ji'].to(device)\n",
    "        C_j_batch = batch['C_j'].to(device)\n",
    "        outputs = model(Y_ji_batch)\n",
    "        loss = loss_fn(X_ji_batch, C_j_batch, outputs.squeeze(0).squeeze(1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        #normalized_loss = loss.item() / Y_ji_batch.shape[1]\n",
    "        total_loss += loss.item()#normalized_loss\n",
    "    avg_train_loss = total_loss / len(loader)\n",
    "    return avg_train_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_profiler_results(profiler):\n",
    "    # Sort by CUDA memory usage if using CUDA, else sort by CPU memory usage\n",
    "    sort_by_key = \"cuda_memory_usage\" if torch.cuda.is_available() else \"cpu_memory_usage\"\n",
    "    print(profiler.key_averages().table(sort_by=sort_by_key, row_limit=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "epochs = 3\n",
    "\n",
    "def train():\n",
    "    model = build_model('linear', 1, 100, False)\n",
    "    train_loader = build_dataset(train_data, 1)\n",
    "    valid_loader = build_dataset(valid_data, 1)\n",
    "    optimizer = build_optimizer(model, 'adam', 1e-3, 0)\n",
    "    loss_fn = CustomLoss()\n",
    "    loss_neural_net_train = [0] * epochs\n",
    "    loss_neural_net_valid = [0] * epochs\n",
    "    loss_glm_valid = [0] * epochs\n",
    "    old_train_loss = float('inf')\n",
    "    scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=1, verbose=True)\n",
    "\n",
    "    with torch.profiler.profile(\n",
    "    activities=[torch.profiler.ProfilerActivity.CPU, torch.profiler.ProfilerActivity.CUDA],\n",
    "    schedule=torch.profiler.schedule(wait=1, warmup=1, active=3),\n",
    "    on_trace_ready=print_profiler_results,\n",
    "    record_shapes=True,\n",
    "    profile_memory=True  # Ensure memory profiling is enabled\n",
    "    ) as profiler:\n",
    "        for epoch in range(epochs):\n",
    "            print(f'Epoch {epoch+1}')\n",
    "            train_loss = train_epoch(model, train_loader, optimizer, loss_fn)\n",
    "            loss_neural_net_train[epoch] = train_loss\n",
    "            print(\"train loss: \"+ str(train_loss))\n",
    "            valid_neural_net_loss, valid_glm_loss, neural_net_zeta, glm_zeta = valid_epoch(model, valid_loader, loss_fn)\n",
    "            loss_neural_net_valid[epoch] = valid_neural_net_loss\n",
    "            loss_glm_valid[epoch] = valid_glm_loss\n",
    "            print(\"valid neural net loss: \"+ str(valid_neural_net_loss))\n",
    "            print(\"valid glm loss: \"+ str(valid_glm_loss))\n",
    "            mae = F.l1_loss(neural_net_zeta.squeeze(), glm_zeta)\n",
    "            mse = F.mse_loss(neural_net_zeta.squeeze(), glm_zeta)\n",
    "            correlation_coefficient = np.corrcoef(glm_zeta, neural_net_zeta.squeeze())[0, 1]\n",
    "            print(\"Correlation Coefficient:\", correlation_coefficient)\n",
    "            print(f\"Mean Absolute Error: {mae.item():.4f}\")\n",
    "            print(f\"Mean Squared Error: {mse.item():.4f}\")\n",
    "            if train_loss > old_train_loss:\n",
    "                optimizer.param_groups[0]['lr'] *= 0.5\n",
    "                print(f\"Reduced learning rate to {optimizer.param_groups[0]['lr']}\")\n",
    "            old_train_loss = train_loss\n",
    "            scheduler.step(train_loss)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear(in_features=12, out_features=1, bias=False)\n",
      "Model is on device: cuda:0\n",
      "Epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 40000, 12])\n",
      "torch.Size([1, 22000, 12])\n",
      "torch.Size([1, 10000, 12])\n",
      "torch.Size([1, 6000, 12])\n",
      "torch.Size([1, 52000, 12])\n",
      "torch.Size([1, 10000, 12])\n",
      "torch.Size([1, 2000, 12])\n",
      "torch.Size([1, 2000, 12])\n",
      "torch.Size([1, 10000, 12])\n",
      "torch.Size([1, 15802, 12])\n",
      "torch.Size([1, 82000, 12])\n",
      "torch.Size([1, 48000, 12])\n",
      "torch.Size([1, 81908, 12])\n",
      "torch.Size([1, 30000, 12])\n",
      "torch.Size([1, 46000, 12])\n",
      "torch.Size([1, 29993, 12])\n",
      "torch.Size([1, 161805, 12])\n",
      "torch.Size([1, 26000, 12])\n",
      "torch.Size([1, 59890, 12])\n",
      "torch.Size([1, 2000, 12])\n",
      "torch.Size([1, 34000, 12])\n",
      "torch.Size([1, 2000, 12])\n",
      "torch.Size([1, 1724, 12])\n",
      "torch.Size([1, 14000, 12])\n",
      "torch.Size([1, 80000, 12])\n",
      "torch.Size([1, 4000, 12])\n",
      "torch.Size([1, 7865, 12])\n",
      "torch.Size([1, 8000, 12])\n",
      "torch.Size([1, 18000, 12])\n",
      "torch.Size([1, 2000, 12])\n",
      "torch.Size([1, 4000, 12])\n",
      "torch.Size([1, 144582, 12])\n",
      "torch.Size([1, 40000, 12])\n",
      "torch.Size([1, 7929, 12])\n",
      "torch.Size([1, 28000, 12])\n",
      "torch.Size([1, 8000, 12])\n",
      "torch.Size([1, 8000, 12])\n",
      "torch.Size([1, 2000, 12])\n",
      "torch.Size([1, 8000, 12])\n",
      "torch.Size([1, 26000, 12])\n",
      "torch.Size([1, 18000, 12])\n",
      "torch.Size([1, 8000, 12])\n",
      "torch.Size([1, 33777, 12])\n",
      "torch.Size([1, 125909, 12])\n",
      "torch.Size([1, 18000, 12])\n",
      "torch.Size([1, 26000, 12])\n",
      "torch.Size([1, 269633, 12])\n",
      "torch.Size([1, 2000, 12])\n",
      "torch.Size([1, 20000, 12])\n",
      "torch.Size([1, 2000, 12])\n",
      "torch.Size([1, 21847, 12])\n",
      "torch.Size([1, 12000, 12])\n",
      "torch.Size([1, 24000, 12])\n",
      "torch.Size([1, 12000, 12])\n",
      "torch.Size([1, 2000, 12])\n",
      "torch.Size([1, 25849, 12])\n",
      "torch.Size([1, 4000, 12])\n",
      "torch.Size([1, 4000, 12])\n",
      "torch.Size([1, 2000, 12])\n",
      "torch.Size([1, 2000, 12])\n",
      "torch.Size([1, 10000, 12])\n",
      "torch.Size([1, 28000, 12])\n",
      "torch.Size([1, 151273, 12])\n",
      "torch.Size([1, 19929, 12])\n",
      "torch.Size([1, 6000, 12])\n",
      "torch.Size([1, 30000, 12])\n",
      "torch.Size([1, 359531, 12])\n",
      "torch.Size([1, 16000, 12])\n",
      "torch.Size([1, 10000, 12])\n",
      "torch.Size([1, 12000, 12])\n",
      "torch.Size([1, 4000, 12])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 10000, 12])\n",
      "torch.Size([1, 103958, 12])\n",
      "torch.Size([1, 10000, 12])\n",
      "torch.Size([1, 4000, 12])\n",
      "torch.Size([1, 12000, 12])\n",
      "torch.Size([1, 6000, 12])\n",
      "torch.Size([1, 6000, 12])\n",
      "torch.Size([1, 73876, 12])\n",
      "torch.Size([1, 52968, 12])\n",
      "torch.Size([1, 22000, 12])\n",
      "torch.Size([1, 8000, 12])\n",
      "torch.Size([1, 6000, 12])\n",
      "torch.Size([1, 14000, 12])\n",
      "torch.Size([1, 8000, 12])\n",
      "torch.Size([1, 19900, 12])\n",
      "torch.Size([1, 2000, 12])\n",
      "torch.Size([1, 12000, 12])\n",
      "torch.Size([1, 24000, 12])\n",
      "torch.Size([1, 18000, 12])\n",
      "torch.Size([1, 10000, 12])\n",
      "torch.Size([1, 6000, 12])\n",
      "torch.Size([1, 82000, 12])\n",
      "torch.Size([1, 2000, 12])\n",
      "torch.Size([1, 22000, 12])\n",
      "torch.Size([1, 32000, 12])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-11:\n",
      "Traceback (most recent call last):\n",
      "  File \"/grid/it/data/elzar/easybuild/software/Anaconda3/2020.02/lib/python3.7/threading.py\", line 926, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/grid/it/data/elzar/easybuild/software/Anaconda3/2020.02/lib/python3.7/threading.py\", line 870, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/grid/siepel/home_norepl/hassett/.local/lib/python3.7/site-packages/torch/utils/data/_utils/pin_memory.py\", line 28, in _pin_memory_loop\n",
      "    r = in_queue.get(timeout=MP_STATUS_CHECK_INTERVAL)\n",
      "  File \"/grid/it/data/elzar/easybuild/software/Anaconda3/2020.02/lib/python3.7/multiprocessing/queues.py\", line 113, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "  File \"/grid/siepel/home_norepl/hassett/.local/lib/python3.7/site-packages/torch/multiprocessing/reductions.py\", line 289, in rebuild_storage_fd\n",
      "    fd = df.detach()\n",
      "  File \"/grid/it/data/elzar/easybuild/software/Anaconda3/2020.02/lib/python3.7/multiprocessing/resource_sharer.py\", line 57, in detach\n",
      "    with _resource_sharer.get_connection(self._id) as conn:\n",
      "  File \"/grid/it/data/elzar/easybuild/software/Anaconda3/2020.02/lib/python3.7/multiprocessing/resource_sharer.py\", line 87, in get_connection\n",
      "    c = Client(address, authkey=process.current_process().authkey)\n",
      "  File \"/grid/it/data/elzar/easybuild/software/Anaconda3/2020.02/lib/python3.7/multiprocessing/connection.py\", line 492, in Client\n",
      "    c = SocketClient(address)\n",
      "  File \"/grid/it/data/elzar/easybuild/software/Anaconda3/2020.02/lib/python3.7/multiprocessing/connection.py\", line 619, in SocketClient\n",
      "    s.connect(address)\n",
      "FileNotFoundError: [Errno 2] No such file or directory\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/6950373.1.gpu_ded.q/ipykernel_388467/2115540189.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/6950373.1.gpu_ded.q/ipykernel_388467/3794033013.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Epoch {epoch+1}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m             \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m             \u001b[0mloss_neural_net_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"train loss: \"\u001b[0m\u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/6950373.1.gpu_ded.q/ipykernel_388467/1721798876.py\u001b[0m in \u001b[0;36mtrain_epoch\u001b[0;34m(model, loader, optimizer, loss_fn)\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m         \u001b[0mY_ji_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Y_ji'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY_ji_batch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mX_ji_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'X_ji'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\"\"\"\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "filename = f\"models/Best_Linear_Elongation_Model.pth\"\n",
    "torch.save(model.state_dict(), filename)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Linear(num_features, 1, bias=False)\n",
    "model.load_state_dict(torch.load(\"models/Elongation_Model.pth\"))\n",
    "cuda_available = torch.cuda.is_available()\n",
    "print(\"CUDA (GPU support) is available:\", cuda_available)\n",
    "num_gpus = torch.cuda.device_count()\n",
    "print(\"Number of GPUs available:\", num_gpus)\n",
    "if cuda_available:\n",
    "    if num_gpus > 1:\n",
    "        print(\"Using\", num_gpus, \"GPUs\")\n",
    "        model = torch.nn.DataParallel(model)\n",
    "    model = model.to('cuda')\n",
    "\n",
    "first_param_device = next(model.parameters()).device\n",
    "print(\"Model is on device:\", first_param_device)\n",
    "model.double()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = model.weight.data.cpu().numpy()\n",
    "#bias = model.bias.data.cpu().numpy()\n",
    "\n",
    "combined = ', '.join([f'\"{s}\": {f}' for s, f in zip(feature_names, weights[0])])\n",
    "print(combined)\n",
    "\n",
    "#print(\"bias: \" + str(model.bias.data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glm_kappa = [-0.0224536145637661, -0.094592589, -0.023815382, 0.030402922, -0.067234092, -0.032196914, -0.040911478, -0.018557168, -0.033545905, -0.051103287, -0.204434712, 0.015831043]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "GLM K\n",
    "\n",
    "* ctcf: -0.02\n",
    "* h3k36me3: -0.09\n",
    "* h3k4me1: -0.02\n",
    "* h3k79me2: +0.03\n",
    "* h3k9me1: -0.06\n",
    "* h3k9me3: -0.03\n",
    "* h4k20me1: -0.04\n",
    "* sj5: -0.02\n",
    "* sj3: -0.03\n",
    "* dms->stem-loop: -0.05\n",
    "* rpts->low-complex: +0.01\n",
    "* wgbs->DNAm: -0.2\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = range(1, len(loss_neural_net_train) + 1)\n",
    "plt.plot(epochs, loss_neural_net_train, label='train_neural_net_loss')\n",
    "plt.plot(epochs, loss_neural_net_valid, label='valid_neural_net_loss')\n",
    "plt.plot(epochs, loss_glm_valid, label='valid_glm_loss')\n",
    "\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_data(glm_zeta, net_zeta):\n",
    "    indices = range(len(glm_zeta))\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(10, 5))\n",
    "    \n",
    "    ax.scatter(indices, net_zeta, color='blue', label='Neural Net Zeta', s=10, alpha=0.5)\n",
    "    ax.scatter(indices, glm_zeta, color='orange', label='GLM Zeta', s=10, alpha=0.5)\n",
    "    \n",
    "    ax.set_title('Neural Net vs GLM Elongation Rate')\n",
    "    ax.set_xlabel('Index')\n",
    "    ax.set_ylabel('Elongation Rate')\n",
    "    ax.legend()\n",
    "    \n",
    "    plt.ylim(0.5, 1.3)\n",
    "\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_batches = create_batches(test_data.groupby('GeneId'), 2000)\n",
    "\n",
    "test_dataset = GeneDataset(test_batches)\n",
    "tstdl = DataLoader(test_dataset, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "\n",
    "data_iter = iter(tstdl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0, 4):\n",
    "    inputs = next(data_iter) \n",
    "    print(\"number of samples: \" + str(len(inputs)))\n",
    "\n",
    "    with torch.no_grad():\n",
    "        y_inputs = inputs['Y_ji'].to(device)\n",
    "        rho_ji = model(y_inputs)\n",
    "        print(rho_ji)\n",
    "\n",
    "    glm_zeta = inputs['Z_ji'][0]\n",
    "    # convert log(Z) outputs to Z\n",
    "    net_zeta = torch.exp(rho_ji.cpu().squeeze())\n",
    "    \n",
    "    plot_data(glm_zeta, net_zeta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net_zeta = []\n",
    "glm_zeta = []\n",
    "with torch.no_grad():\n",
    "    for batch in tstdl:\n",
    "        y_inputs = batch['Y_ji'].to(device)\n",
    "        rho_ji = model(y_inputs)\n",
    "        # convert log(Z) outputs to Z\n",
    "        net_zeta.append(torch.exp(rho_ji.cpu()[0]))\n",
    "        glm_zeta.append(batch['Z_ji'][0])\n",
    "\n",
    "net_zeta = torch.cat(net_zeta, dim=0)\n",
    "glm_zeta = torch.cat(glm_zeta, dim=0)\n",
    "mae = F.l1_loss(net_zeta.squeeze(), glm_zeta)\n",
    "mse = F.mse_loss(net_zeta.squeeze(), glm_zeta)\n",
    "\n",
    "correlation_coefficient = np.corrcoef(glm_zeta, net_zeta.squeeze())[0, 1]\n",
    "print(\"Correlation Coefficient:\", correlation_coefficient)\n",
    "\n",
    "print(f\"Mean Absolute Error: {mae.item():.4f}\")\n",
    "print(f\"Mean Squared Error: {mse.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def density_plot(glm_zeta, net_zeta, gene_id):\n",
    "    sns.kdeplot(x=glm_zeta, y=net_zeta, fill=True, cmap=\"Blues\")\n",
    "            \n",
    "    plt.xlim([min(glm_zeta), max(glm_zeta)])\n",
    "    plt.ylim([min(net_zeta), max(net_zeta)])\n",
    "\n",
    "\n",
    "    plt.xlabel('GLM Elongation Rate')\n",
    "    plt.ylabel('Neural Net Elongation Rate')\n",
    "    plt.title(gene_id)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scatterplot(net_zeta, glm_zeta, gene_id):\n",
    "    indices = range(len(glm_zeta))\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(10, 5))\n",
    "    \n",
    "    min_val = min(min(net_zeta), min(glm_zeta))\n",
    "    max_val = max(max(net_zeta), max(glm_zeta))\n",
    "\n",
    "    plt.xlim(min_val, max_val)\n",
    "    plt.ylim(min_val, max_val)\n",
    "    \n",
    "    ax.scatter(net_zeta, glm_zeta, s=5)\n",
    "    \n",
    "    ax.set_title(gene_id)\n",
    "    ax.set_xlabel('Neural Net Zeta')\n",
    "    ax.set_ylabel('GLM Zeta')\n",
    "    ax.legend()\n",
    "\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_batches2 = create_batches(test_data.groupby('GeneId'), batch_size=2000)\n",
    "\n",
    "test_dataset2 = GeneDataset(test_batches2)\n",
    "tstdl2 = DataLoader(test_dataset2, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_loss = 0\n",
    "loss_fn = CustomLoss()\n",
    "for batch in tstdl2:\n",
    "    gene_id = batch['GeneId'][0]\n",
    "    model.eval()\n",
    "    #print(\"number of samples: \" + str(len(batch)))\n",
    "\n",
    "    with torch.no_grad():\n",
    "        y_inputs = batch['Y_ji'].to(device)\n",
    "        rho_ji = model(y_inputs)\n",
    "        loss = loss_fn(batch['X_ji'], batch['C_j'], rho_ji)\n",
    "    \n",
    "    total_loss += loss.item()\n",
    "\n",
    "    glm_zeta = batch['Z_ji'][0]\n",
    "    # convert log(Z) outputs to Z\n",
    "    net_zeta = torch.exp(rho_ji.cpu().squeeze())\n",
    "        \n",
    "    #density_plot(glm_zeta, net_zeta, gene_id)\n",
    "        \n",
    "    #scatterplot(predicted_zeta, simulated_zeta, gene_id)\n",
    "        \n",
    "    #plot_data(glm_zeta, net_zeta)\n",
    "        \n",
    "print(total_loss / len(tstdl2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 10))\n",
    "\n",
    "sns.scatterplot(x=glm_kappa, y=weights[0])\n",
    "\n",
    "for i in range(len(glm_kappa)):\n",
    "    plt.text(glm_kappa[i], weights[0][i], feature_names[i], fontsize=13, ha='right', va='top')\n",
    "plt.xlabel('GLM Weights')\n",
    "plt.ylabel('Neural Net Weights')\n",
    "\n",
    "max_val = max(np.max(glm_kappa), np.max(weights[0])) + 0.04\n",
    "min_val = min(np.min(glm_kappa), np.min(weights[0])) - 0.04\n",
    "\n",
    "plt.xlim(max_val, min_val)\n",
    "plt.ylim(max_val, min_val)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# profiling code\n",
    "\"\"\"\n",
    "def print_profiler_results(profiler):\n",
    "    print(profiler.key_averages().table(sort_by=\"self_cpu_time_total\", row_limit=10))\n",
    "\n",
    "with torch.profiler.profile(\n",
    "    activities=[torch.profiler.ProfilerActivity.CPU, torch.profiler.ProfilerActivity.CUDA],\n",
    "    schedule=torch.profiler.schedule(wait=1, warmup=1, active=3),\n",
    "    on_trace_ready=print_profiler_results,\n",
    "    record_shapes=True,\n",
    "    profile_memory=True\n",
    ") as profiler:\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f'Epoch {epoch+1}')\n",
    "        model.train()\n",
    "        trndl = DataLoader(train_set, batch_size=batch_size, num_workers=7, shuffle=False, pin_memory=True)\n",
    "        for i, batch in enumerate(trndl):\n",
    "            optimizer.zero_grad()\n",
    "            Y_ji_batch = batch['Y_ji'].to(device)\n",
    "            X_ji_batch = batch['X_ji'].to(device)\n",
    "            C_j_batch = batch['C_j'].to(device)\n",
    "            outputs = model(Y_ji_batch)\n",
    "            loss = loss_fn(X_ji_batch, C_j_batch, outputs)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            loss_hist_train[epoch] += loss.item()\n",
    "            profiler.step()\n",
    "        loss_hist_train[epoch] /= len(trndl)\n",
    "        del trndl\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Anaconda3 (Python 3.7.6)",
   "language": "python",
   "name": "anaconda3_2020.02"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
