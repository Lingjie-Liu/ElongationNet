{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run CPU-only, GPU code needs further testing\n",
    "\"\"\"\n",
    "Restart kernel after running\n",
    "Only need to run once\n",
    "\"\"\"\n",
    "!pip install scikit-learn matplotlib seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/grid/it/data/elzar/easybuild/software/Anaconda3/2020.02/lib/python3.7/site-packages/requests/__init__.py:91: RequestsDependencyWarning: urllib3 (2.0.7) or chardet (3.0.4) doesn't match a supported version!\n",
      "  RequestsDependencyWarning)\n",
      "/grid/siepel/home_norepl/hassett/.local/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3553: DtypeWarning: Columns (0) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as colors\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.utils.data as td\n",
    "import torch.nn as nn\n",
    "import torch.nn.init as init\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torch.utils.data import DataLoader, Dataset, Subset\n",
    "import wandb\n",
    "\n",
    "\n",
    "froot = './data/k562_samp_epft_norm_test_1.csv'\n",
    "\n",
    "df = pd.read_csv(froot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  seqnames     start       end strand  ensembl_gene_id  score      ctcf  \\\n",
      "0       15  88623545  88623545      +  ENSG00000181026    0.0 -0.079992   \n",
      "1       15  88623546  88623546      +  ENSG00000181026    0.0 -0.079942   \n",
      "2       15  88623547  88623547      +  ENSG00000181026    0.0 -0.079893   \n",
      "3       15  88623548  88623548      +  ENSG00000181026    0.0 -0.079844   \n",
      "4       15  88623549  88623549      +  ENSG00000181026    0.0 -0.079796   \n",
      "\n",
      "   h3k36me3   h3k4me1  h3k79me2   h3k9me1   h3k9me3  h4k20me1       sj5  \\\n",
      "0 -0.000099  0.348531  4.423451  0.446508 -0.168099  3.232475 -0.028916   \n",
      "1  0.001638  0.352677  4.460072  0.453024 -0.169218  3.259194 -0.028916   \n",
      "2  0.003360  0.356807  4.496664  0.459491 -0.170339  3.285849 -0.028916   \n",
      "3  0.005065  0.360919  4.533223  0.465908 -0.171461  3.312435 -0.028916   \n",
      "4  0.006754  0.365013  4.569743  0.472274 -0.172584  3.338952 -0.028916   \n",
      "\n",
      "        sj3       dms  wgbs      rpts  lambda_alphaj      zeta  \n",
      "0 -0.057178 -0.307549   0.0  0.249626       0.052255  0.993283  \n",
      "1 -0.057178 -0.307549   0.0  0.249626       0.052255  0.992642  \n",
      "2 -0.057178 -0.307549   0.0  0.249626       0.052255  0.992008  \n",
      "3 -0.057178 -0.307549   0.0  0.249626       0.052255  0.991381  \n",
      "4 -0.057178 -0.307549   0.0  0.249626       0.052255  0.990762  \n"
     ]
    }
   ],
   "source": [
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['h3k36me3' 'h3k4me1' 'h3k79me2' 'h3k9me1' 'h3k9me3']\n"
     ]
    }
   ],
   "source": [
    "column_names = np.array(df.columns)\n",
    "feature_names = column_names[7:12]#column_names[6:-2]\n",
    "num_features = len(feature_names)\n",
    "#nucleotides = column_names[-6:-2]\n",
    "print(feature_names)\n",
    "#print(nucleotides)\n",
    "num_samples = df.shape[0]\n",
    "\n",
    "# process read counts\n",
    "X_ji = df['score'].values\n",
    "\n",
    "# process GLM simulated elongation rates\n",
    "Z_ji = df['zeta'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Samples: 16182613\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of Samples: \" + str(num_samples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Features: 5\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of Features: \" + str(num_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16182613, 12)\n"
     ]
    }
   ],
   "source": [
    "#Y_ji is a list of samples containing lists of their feature values\n",
    "    # [   \n",
    "    #   sample_1: [feat_1, feat_2,...,feat_n],\n",
    "    #   sample_2: [feat_1, feat_2,...,feat_n],\n",
    "    # ]\n",
    "\n",
    "Y_ji = df.iloc[:, 6:-2].values\n",
    "Y_ji_shape = Y_ji.shape\n",
    "print(Y_ji.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "C_j = df['lambda_alphaj'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "gene_ids = df['ensembl_gene_id'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of genes: 519\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of genes: \" + str(len(np.unique(gene_ids))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA (GPU support) is available: False\n",
      "Number of GPUs available: 0\n"
     ]
    }
   ],
   "source": [
    "cuda_available = torch.cuda.is_available()\n",
    "print(\"CUDA (GPU support) is available:\", cuda_available)\n",
    "num_gpus = torch.cuda.device_count()\n",
    "print(\"Number of GPUs available:\", num_gpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/grid/siepel/home_norepl/hassett/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py:65: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n",
      "  \"num_layers={}\".format(dropout, num_layers))\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from pdb import set_trace as stop\n",
    "\n",
    "# AttentiveChrome Model\n",
    "def batch_product(iput, mat2):\n",
    "    result = None\n",
    "    for i in range(iput.size()[0]):\n",
    "        op = torch.mm(iput[i], mat2)\n",
    "        op = op.unsqueeze(0)\n",
    "        if(result is None):\n",
    "            result = op\n",
    "        else:\n",
    "            result = torch.cat((result,op),0)\n",
    "    return result.squeeze(2)\n",
    "\n",
    "\n",
    "class rec_attention(nn.Module):\n",
    "    # attention with bin context vector per HM and HM context vector\n",
    "    def __init__(self,hm,args):\n",
    "        super(rec_attention,self).__init__()\n",
    "        self.num_directions=2 if args.bidirectional else 1\n",
    "        if (hm==False):\n",
    "            self.bin_rep_size=args.bin_rnn_size*self.num_directions\n",
    "        else:\n",
    "            self.bin_rep_size=args.bin_rnn_size\n",
    "\n",
    "        self.bin_context_vector=nn.Parameter(torch.Tensor(self.bin_rep_size,1),requires_grad=True)\n",
    "\n",
    "\n",
    "        self.softmax=nn.Softmax(dim=1)\n",
    "\n",
    "        self.bin_context_vector.data.uniform_(-0.1, 0.1)\n",
    "\n",
    "    def forward(self,iput):\n",
    "        alpha=self.softmax(batch_product(iput,self.bin_context_vector))\n",
    "        [batch_size,source_length,bin_rep_size2]=iput.size()\n",
    "        repres=torch.bmm(alpha.unsqueeze(2).view(batch_size,-1,source_length),iput)\n",
    "        return repres,alpha\n",
    "\n",
    "\n",
    "\n",
    "class recurrent_encoder(nn.Module):\n",
    "    # modular LSTM encoder\n",
    "    def __init__(self,n_bins,ip_bin_size,hm,args):\n",
    "        super(recurrent_encoder,self).__init__()\n",
    "        self.bin_rnn_size=args.bin_rnn_size\n",
    "        self.ipsize=ip_bin_size\n",
    "        self.seq_length=n_bins\n",
    "\n",
    "        self.num_directions=2 if args.bidirectional else 1\n",
    "        if (hm==False):\n",
    "            self.bin_rnn_size=args.bin_rnn_size\n",
    "        else:\n",
    "            self.bin_rnn_size=args.bin_rnn_size // 2\n",
    "        self.bin_rep_size=self.bin_rnn_size*self.num_directions\n",
    "\n",
    "\n",
    "        self.rnn=nn.LSTM(self.ipsize,self.bin_rnn_size,num_layers=args.num_layers,dropout=args.dropout,bidirectional=args.bidirectional)\n",
    "\n",
    "        self.bin_attention=rec_attention(hm,args)\n",
    "    def outputlength(self):\n",
    "        return self.bin_rep_size\n",
    "    def forward(self,single_hm,hidden=None):\n",
    "        bin_output, hidden = self.rnn(single_hm,hidden)\n",
    "        bin_output = bin_output.permute(1,0,2)\n",
    "        hm_rep,bin_alpha = self.bin_attention(bin_output)\n",
    "        return hm_rep,bin_alpha\n",
    "\n",
    "\n",
    "class AttrDict(dict):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super(AttrDict, self).__init__(*args, **kwargs)\n",
    "        self.__dict__ = self\n",
    "\n",
    "\n",
    "class att_chrome(nn.Module):\n",
    "    def __init__(self,args):\n",
    "        super(att_chrome,self).__init__()\n",
    "        self.n_hms=args.n_hms\n",
    "        self.n_bins=args.n_bins\n",
    "        self.ip_bin_size=1\n",
    "\n",
    "        self.rnn_hms=nn.ModuleList()\n",
    "        for i in range(self.n_hms):\n",
    "            self.rnn_hms.append(recurrent_encoder(self.n_bins,self.ip_bin_size,False,args))\n",
    "        self.opsize = self.rnn_hms[0].outputlength()\n",
    "        self.hm_level_rnn_1=recurrent_encoder(self.n_hms,self.opsize,True,args)\n",
    "        self.opsize2=self.hm_level_rnn_1.outputlength()\n",
    "        self.diffopsize=2*(self.opsize2)\n",
    "        self.fdiff1_1=nn.Linear(self.opsize2,100)\n",
    "\n",
    "    def forward(self,iput):\n",
    "\n",
    "        bin_a=None\n",
    "        level1_rep=None\n",
    "        [batch_size,_,_]=iput.size()\n",
    "\n",
    "        for hm,hm_encdr in enumerate(self.rnn_hms):\n",
    "            hmod=iput[:,:,hm].contiguous()\n",
    "            hmod=torch.t(hmod).unsqueeze(2)\n",
    "\n",
    "            op,a= hm_encdr(hmod)\n",
    "            if level1_rep is None:\n",
    "                level1_rep=op\n",
    "                bin_a=a\n",
    "            else:\n",
    "                level1_rep=torch.cat((level1_rep,op),1)\n",
    "                bin_a=torch.cat((bin_a,a),1)\n",
    "        level1_rep=level1_rep.permute(1,0,2)\n",
    "        final_rep_1,hm_level_attention_1=self.hm_level_rnn_1(level1_rep)\n",
    "        final_rep_1=final_rep_1.squeeze(1)\n",
    "        prediction_m=((self.fdiff1_1(final_rep_1)))\n",
    "\n",
    "        return prediction_m\n",
    "\n",
    "# dropout won't be applied to single-layer RNN\n",
    "args_dict = {'lr': 0.0001, 'model_name': 'attchrome', 'clip': 1, 'epochs': 2, 'batch_size': 10, 'dropout': 0.5, 'cell_1': 'Cell1', 'save_root': 'Results/Cell1', 'data_root': 'data/', 'gpuid': 0, 'gpu': 0, 'n_hms': 5, 'n_bins': 200, 'bin_rnn_size': 32, 'num_layers': 1, 'unidirectional': False, 'save_attention_maps': False, 'attentionfilename': 'beta_attention.txt', 'test_on_saved_model': False, 'bidirectional': True, 'dataset': 'Cell1'}\n",
    "att_chrome_args = AttrDict(args_dict)\n",
    "att_chrome_model = att_chrome(att_chrome_args)\n",
    "\n",
    "dtype = torch.FloatTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==>initializing a new model\n"
     ]
    }
   ],
   "source": [
    "print(\"==>initializing a new model\")\n",
    "for p in att_chrome_model.parameters():\n",
    "    p.data.uniform_(-0.1,0.1)\n",
    "\n",
    "optimizer = optim.Adam(att_chrome_model.parameters(), lr = 0.0002)\n",
    "#optimizer = optim.SGD(model.parameters(), lr = args.lr, momentum=args.momentum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n# original code should also work for grouping data\\nimport collections\\n\\ncount = 0\\nattr=collections.OrderedDict()\\nwindows = 100\\n\\n# need to split by gene later\\nfor i in range(0,num_samples,windows):\\n        hm1=torch.zeros(windows,1)\\n        hm2=torch.zeros(windows,1)\\n        hm3=torch.zeros(windows,1)\\n        hm4=torch.zeros(windows,1)\\n        hm5=torch.zeros(windows,1)\\n        x_ji=torch.zeros(windows,1)\\n        c_j=torch.zeros(windows,1)\\n        z_ji=torch.zeros(windows,1)\\n        for w in range(0,windows):\\n            hm1[w][0]=int(Y_ji[i+w][0])\\n            hm2[w][0]=int(Y_ji[i+w][1])\\n            hm3[w][0]=int(Y_ji[i+w][2])\\n            hm4[w][0]=int(Y_ji[i+w][3])\\n            hm5[w][0]=int(Y_ji[i+w][4])\\n            x_ji[w][0]=int(X_ji[i+w])\\n            c_j[w][0]=int(C_j[i+w])\\n            z_ji[w][0]=int(Z_ji[i+w])\\n        geneID=gene_ids[i]\\n\\n        attr[count]={\\n            'geneID':geneID,\\n            'hm1':hm1,\\n            'hm2':hm2,\\n            'hm3':hm3,\\n            'hm4':hm4,\\n            'hm5':hm5,\\n            'x_ji':x_ji,\\n            'c_j':c_j,\\n            'z_ji':z_ji\\n        }\\n        count+=1\\n\""
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "# original code should also work for grouping data\n",
    "import collections\n",
    "\n",
    "count = 0\n",
    "attr=collections.OrderedDict()\n",
    "windows = 100\n",
    "\n",
    "# need to split by gene later\n",
    "for i in range(0,num_samples,windows):\n",
    "        hm1=torch.zeros(windows,1)\n",
    "        hm2=torch.zeros(windows,1)\n",
    "        hm3=torch.zeros(windows,1)\n",
    "        hm4=torch.zeros(windows,1)\n",
    "        hm5=torch.zeros(windows,1)\n",
    "        x_ji=torch.zeros(windows,1)\n",
    "        c_j=torch.zeros(windows,1)\n",
    "        z_ji=torch.zeros(windows,1)\n",
    "        for w in range(0,windows):\n",
    "            hm1[w][0]=int(Y_ji[i+w][0])\n",
    "            hm2[w][0]=int(Y_ji[i+w][1])\n",
    "            hm3[w][0]=int(Y_ji[i+w][2])\n",
    "            hm4[w][0]=int(Y_ji[i+w][3])\n",
    "            hm5[w][0]=int(Y_ji[i+w][4])\n",
    "            x_ji[w][0]=int(X_ji[i+w])\n",
    "            c_j[w][0]=int(C_j[i+w])\n",
    "            z_ji[w][0]=int(Z_ji[i+w])\n",
    "        geneID=gene_ids[i]\n",
    "\n",
    "        attr[count]={\n",
    "            'geneID':geneID,\n",
    "            'hm1':hm1,\n",
    "            'hm2':hm2,\n",
    "            'hm3':hm3,\n",
    "            'hm4':hm4,\n",
    "            'hm5':hm5,\n",
    "            'x_ji':x_ji,\n",
    "            'c_j':c_j,\n",
    "            'z_ji':z_ji\n",
    "        }\n",
    "        count+=1\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GeneDataset(Dataset):\n",
    "    def __init__(self, GeneId, Y_ji, X_ji, C_j, Z_ji):\n",
    "        self.GeneId = GeneId\n",
    "        self.Y_ji = Y_ji\n",
    "        self.X_ji = X_ji\n",
    "        self.C_j = C_j\n",
    "        self.Z_ji = Z_ji\n",
    "\n",
    "    def __len__(self):\n",
    "        # Only count full batches\n",
    "        return len(self.Y_ji) // 100\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        batch_start = idx * 100\n",
    "        batch_end = batch_start + 100\n",
    "\n",
    "        data = {\n",
    "            #'GeneId':  self.GeneId[batch_start:batch_end], # Uncomment if needed\n",
    "            'Y_ji':  torch.tensor(self.Y_ji[batch_start:batch_end], dtype=torch.float32),\n",
    "            'X_ji': torch.tensor(self.X_ji[batch_start:batch_end], dtype=torch.float32),\n",
    "            'C_j': torch.tensor(self.C_j[batch_start:batch_end], dtype=torch.float32),\n",
    "            'Z_ji': torch.tensor(self.Z_ji[batch_start:batch_end], dtype=torch.float32)\n",
    "        }\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_gene_ids, test_gene_ids, train_Y_ji, test_Y_ji, train_X_ji, test_X_ji, train_C_j, test_C_j, train_Z_ji, test_Z_ji = train_test_split(gene_ids, Y_ji, X_ji, C_j, Z_ji, test_size=0.2, shuffle=False)\n",
    "\n",
    "# Create dataset instances for train and test\n",
    "train_dataset = GeneDataset(train_gene_ids, train_Y_ji, train_X_ji, train_C_j, train_Z_ji)\n",
    "test_dataset = GeneDataset(test_gene_ids, test_Y_ji, test_X_ji, test_C_j, test_Z_ji)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nclass HMData(Dataset):\\n    # Dataset class for loading data\\n    def __init__(self,data_cell1,transform=None):\\n        self.c1=data_cell1\\n    def __len__(self):\\n        return len(self.c1)\\n    def __getitem__(self,i):\\n        x_ji=self.c1[i]['x_ji']\\n        c_j=self.c1[i]['c_j']\\n        z_ji=self.c1[i]['z_ji']\\n        geneID=self.c1[i]['geneID']\\n        sample={'geneID':geneID,\\n               'input':final_data_c1,\\n               'x_ji':x_ji,\\n                'c_j':c_j,\\n                'z_ji':z_ji\\n               }\\n        return sample\\n\""
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "class HMData(Dataset):\n",
    "    # Dataset class for loading data\n",
    "    def __init__(self,data_cell1,transform=None):\n",
    "        self.c1=data_cell1\n",
    "    def __len__(self):\n",
    "        return len(self.c1)\n",
    "    def __getitem__(self,i):\n",
    "        x_ji=self.c1[i]['x_ji']\n",
    "        c_j=self.c1[i]['c_j']\n",
    "        z_ji=self.c1[i]['z_ji']\n",
    "        geneID=self.c1[i]['geneID']\n",
    "        sample={'geneID':geneID,\n",
    "               'input':final_data_c1,\n",
    "               'x_ji':x_ji,\n",
    "                'c_j':c_j,\n",
    "                'z_ji':z_ji\n",
    "               }\n",
    "        return sample\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "from torch.utils.data import random_split\n",
    "\n",
    "hm_dataset=HMData(attr)\n",
    "train_size = int(0.8 * len(hm_dataset))\n",
    "test_size = len(hm_dataset) - train_size\n",
    "train_dataset, test_dataset = random_split(hm_dataset, [train_size, test_size])\n",
    "\"\"\"\n",
    "\n",
    "Train = torch.utils.data.DataLoader(train_dataset, batch_size=500, shuffle=True)\n",
    "#Valid = torch.utils.data.DataLoader(valid_inputs, batch_size=args.batch_size, shuffle=False)\n",
    "Test = torch.utils.data.DataLoader(test_dataset, batch_size=16, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "259\n"
     ]
    }
   ],
   "source": [
    "print(len(Train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\ndef build_dataset(train_data, batch_size):\\n    batches = create_batches(train_data.groupby('GeneId'), batch_size)\\n    dataset = GeneDataset(batches)\\n    loader = DataLoader(dataset, batch_size=1, num_workers=7, shuffle=False, pin_memory=True)\\n    return loader\\n\""
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "def build_dataset(train_data, batch_size):\n",
    "    batches = create_batches(train_data.groupby('GeneId'), batch_size)\n",
    "    dataset = GeneDataset(batches)\n",
    "    loader = DataLoader(dataset, batch_size=1, num_workers=7, shuffle=False, pin_memory=True)\n",
    "    return loader\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CustomLoss, self).__init__()\n",
    "\n",
    "    def forward(self, X_ji, C_j, rho_ji):\n",
    "        C_j_value = C_j[0]\n",
    "        #print(X_ji.shape)\n",
    "        #print(rho_ji.shape)\n",
    "        X_ji = X_ji.squeeze(0)\n",
    "        rho_ji = rho_ji.squeeze(0).squeeze(1)\n",
    "        loss = X_ji * rho_ji + C_j_value * torch.exp(-rho_ji) - X_ji * torch.log(C_j_value)\n",
    "        return (loss).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndef build_optimizer(network, optimizer, learning_rate, momentum):\\n    if optimizer == \"sgd\":\\n        optimizer = optim.SGD(network.parameters(),\\n                              lr=learning_rate, momentum=momentum)\\n    elif optimizer == \"adam\":\\n        optimizer = optim.Adam(network.parameters(),\\n                               lr=learning_rate)\\n    return optimizer\\n'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "def build_optimizer(network, optimizer, learning_rate, momentum):\n",
    "    if optimizer == \"sgd\":\n",
    "        optimizer = optim.SGD(network.parameters(),\n",
    "                              lr=learning_rate, momentum=momentum)\n",
    "    elif optimizer == \"adam\":\n",
    "        optimizer = optim.Adam(network.parameters(),\n",
    "                               lr=learning_rate)\n",
    "    return optimizer\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, loader, optimizer, loss_fn):\n",
    "    model.train()\n",
    "    print(len(loader))\n",
    "    total_loss = 0\n",
    "    for idx, batch in enumerate(loader):\n",
    "        optimizer.zero_grad()\n",
    "        Y_ji_batch = batch['Y_ji'].to(device)\n",
    "        X_ji_batch = batch['X_ji'].to(device)\n",
    "        C_j_batch = batch['C_j'].to(device)\n",
    "        outputs = model(Y_ji_batch)\n",
    "        loss = loss_fn(X_ji_batch, C_j_batch, outputs)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    avg_loss = total_loss / len(loader)\n",
    "    return avg_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "259\n",
      "total\n",
      "0.11065784364248335\n",
      "Epoch 2\n",
      "259\n",
      "total\n",
      "0.10809639044479974\n",
      "Epoch 3\n",
      "259\n"
     ]
    }
   ],
   "source": [
    "def print_profiler_results(profiler):\n",
    "    print(profiler.key_averages().table(sort_by=\"self_cpu_time_total\", row_limit=10))\n",
    "\n",
    "loader = Train#build_dataset(train_data, 16)#16000000)\n",
    "optimizer = optimizer#build_optimizer(model, \"adam\", 1e-4, 0)\n",
    "loss_fn = CustomLoss()\n",
    "\n",
    "with torch.profiler.profile(\n",
    "    activities=[torch.profiler.ProfilerActivity.CPU, torch.profiler.ProfilerActivity.CUDA],\n",
    "    schedule=torch.profiler.schedule(wait=1, warmup=1, active=3),\n",
    "    on_trace_ready=print_profiler_results,\n",
    "    record_shapes=True,\n",
    "    profile_memory=True\n",
    ") as profiler:\n",
    "    for epoch in range(3):\n",
    "        print(f'Epoch {epoch+1}')\n",
    "        att_chrome_model.train()\n",
    "        print(len(loader))\n",
    "        total_loss = 0\n",
    "        for idx, batch in enumerate(loader):\n",
    "            optimizer.zero_grad()\n",
    "            Y_ji_batch = batch['Y_ji']#.to(device)\n",
    "            X_ji_batch = batch['X_ji']#.to(device)\n",
    "            C_j_batch = batch['C_j']#.to(device)\n",
    "            outputs = att_chrome_model(Y_ji_batch)\n",
    "            loss = loss_fn(X_ji_batch, C_j_batch, outputs)\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(att_chrome_model.parameters(), 1)\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "            #profiler.step()\n",
    "        print(\"total\")\n",
    "        avg_loss = total_loss / len(loader)\n",
    "        print(avg_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\"\"\"\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "filename = f\"models/Elongation_Linear_Model.pth\"\n",
    "torch.save(model.state_dict(), filename)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Linear(num_features, 1, bias=False)\n",
    "model.load_state_dict(torch.load(\"models/Elongation_Linear_Model.pth\"))\n",
    "cuda_available = torch.cuda.is_available()\n",
    "print(\"CUDA (GPU support) is available:\", cuda_available)\n",
    "num_gpus = torch.cuda.device_count()\n",
    "print(\"Number of GPUs available:\", num_gpus)\n",
    "if cuda_available:\n",
    "    \"\"\"\n",
    "    if num_gpus > 1:\n",
    "        print(\"Using\", num_gpus, \"GPUs\")\n",
    "        model = torch.nn.DataParallel(model)\n",
    "    \"\"\"\n",
    "    model = model.to('cuda')\n",
    "\n",
    "first_param_device = next(model.parameters()).device\n",
    "print(\"Model is on device:\", first_param_device)\n",
    "model.double()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = model.weight.data.cpu().numpy()\n",
    "#bias = model.bias.data.cpu().numpy()\n",
    "\n",
    "combined = ', '.join([f'\"{s}\": {f}' for s, f in zip(feature_names, weights[0])])\n",
    "print(combined)\n",
    "\n",
    "#print(\"bias: \" + str(model.bias.data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glm_kappa = [-0.0224536145637661, -0.094592589, -0.023815382, 0.030402922, -0.067234092, -0.032196914, -0.040911478, -0.018557168, -0.033545905, -0.051103287, -0.204434712, 0.015831043]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "GLM K\n",
    "\n",
    "* ctcf: -0.02\n",
    "* h3k36me3: -0.09\n",
    "* h3k4me1: -0.02\n",
    "* h3k79me2: +0.03\n",
    "* h3k9me1: -0.06\n",
    "* h3k9me3: -0.03\n",
    "* h4k20me1: -0.04\n",
    "* sj5: -0.02\n",
    "* sj3: -0.03\n",
    "* dms->stem-loop: -0.05\n",
    "* rpts->low-complex: +0.01\n",
    "* wgbs->DNAm: -0.2\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = range(1, len(loss_hist_train) + 1)\n",
    "plt.plot(epochs, loss_hist_train, label='train_loss')\n",
    "plt.plot(epochs, loss_hist_valid, label='valid_loss')\n",
    "\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_data(glm_zeta, net_zeta):\n",
    "    indices = range(len(glm_zeta))\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(10, 5))\n",
    "    \n",
    "    ax.scatter(indices, net_zeta, color='blue', label='Neural Net Zeta', s=10, alpha=0.5)\n",
    "    ax.scatter(indices, glm_zeta, color='orange', label='GLM Zeta', s=10, alpha=0.5)\n",
    "    \n",
    "    ax.set_title('Neural Net vs GLM Elongation Rate')\n",
    "    ax.set_xlabel('Index')\n",
    "    ax.set_ylabel('Elongation Rate')\n",
    "    ax.legend()\n",
    "    \n",
    "    plt.ylim(0.5, 1.3)\n",
    "\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_batches = create_batches(test_data.groupby('GeneId'), max_batch_size=64)\n",
    "\n",
    "test_dataset = GeneDataset(test_batches)\n",
    "tstdl = DataLoader(test_dataset, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net_zeta = []\n",
    "glm_zeta = []\n",
    "with torch.no_grad():\n",
    "    for batch in tstdl:\n",
    "        y_inputs = batch['Y_ji'].to(device)\n",
    "        rho_ji = model(y_inputs)\n",
    "        # convert log(Z) outputs to Z\n",
    "        net_zeta.append(torch.exp(rho_ji.cpu()[0]))\n",
    "        glm_zeta.append(batch['Z_ji'][0])\n",
    "\n",
    "net_zeta = torch.cat(net_zeta, dim=0)\n",
    "glm_zeta = torch.cat(glm_zeta, dim=0)\n",
    "mae = F.l1_loss(net_zeta.squeeze(), glm_zeta)\n",
    "mse = F.mse_loss(net_zeta.squeeze(), glm_zeta)\n",
    "\n",
    "correlation_coefficient = np.corrcoef(glm_zeta, net_zeta.squeeze())[0, 1]\n",
    "print(\"Correlation Coefficient:\", correlation_coefficient)\n",
    "\n",
    "print(f\"Mean Absolute Error: {mae.item():.4f}\")\n",
    "print(f\"Mean Squared Error: {mse.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def density_plot(glm_zeta, net_zeta, gene_id):\n",
    "    sns.kdeplot(x=glm_zeta, y=net_zeta, fill=True, cmap=\"Blues\")\n",
    "            \n",
    "    plt.xlim([min(glm_zeta), max(glm_zeta)])\n",
    "    plt.ylim([min(net_zeta), max(net_zeta)])\n",
    "\n",
    "\n",
    "    plt.xlabel('GLM Elongation Rate')\n",
    "    plt.ylabel('Neural Net Elongation Rate')\n",
    "    plt.title(gene_id)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scatterplot(net_zeta, glm_zeta, gene_id):\n",
    "    indices = range(len(glm_zeta))\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(10, 5))\n",
    "    \n",
    "    min_val = min(min(net_zeta), min(glm_zeta))\n",
    "    max_val = max(max(net_zeta), max(glm_zeta))\n",
    "\n",
    "    plt.xlim(min_val, max_val)\n",
    "    plt.ylim(min_val, max_val)\n",
    "    \n",
    "    ax.scatter(net_zeta, glm_zeta, s=5)\n",
    "    \n",
    "    ax.set_title(gene_id)\n",
    "    ax.set_xlabel('Neural Net Zeta')\n",
    "    ax.set_ylabel('GLM Zeta')\n",
    "    ax.legend()\n",
    "\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_batches2 = create_batches(test_data.groupby('GeneId'), max_batch_size=2000)\n",
    "\n",
    "test_dataset2 = GeneDataset(test_batches2)\n",
    "tstdl2 = DataLoader(test_dataset2, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_loss = 0\n",
    "loss_fn = CustomLoss()\n",
    "for batch in tstdl2:\n",
    "    gene_id = batch['GeneId'][0]\n",
    "    model.eval()\n",
    "    #print(\"number of samples: \" + str(len(batch)))\n",
    "\n",
    "    with torch.no_grad():\n",
    "        y_inputs = batch['Y_ji'].to(device)\n",
    "        rho_ji = model(y_inputs)\n",
    "    \n",
    "    glm_zeta = batch['Z_ji'][0]\n",
    "    # convert log(Z) outputs to Z\n",
    "    net_zeta = torch.exp(rho_ji.cpu().squeeze())\n",
    "        \n",
    "    density_plot(glm_zeta, net_zeta, gene_id)\n",
    "                \n",
    "    plot_data(glm_zeta, net_zeta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 10))\n",
    "\n",
    "sns.scatterplot(x=glm_kappa, y=weights[0])\n",
    "\n",
    "for i in range(len(glm_kappa)):\n",
    "    plt.text(glm_kappa[i], weights[0][i], feature_names[i], fontsize=13, ha='right', va='top')\n",
    "plt.xlabel('GLM Weights')\n",
    "plt.ylabel('Neural Net Weights')\n",
    "\n",
    "max_val = max(np.max(glm_kappa), np.max(weights[0])) + 0.04\n",
    "min_val = min(np.min(glm_kappa), np.min(weights[0])) - 0.04\n",
    "\n",
    "plt.xlim(max_val, min_val)\n",
    "plt.ylim(max_val, min_val)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# profiling code\n",
    "\"\"\"\n",
    "def print_profiler_results(profiler):\n",
    "    print(profiler.key_averages().table(sort_by=\"self_cpu_time_total\", row_limit=10))\n",
    "\n",
    "with torch.profiler.profile(\n",
    "    activities=[torch.profiler.ProfilerActivity.CPU, torch.profiler.ProfilerActivity.CUDA],\n",
    "    schedule=torch.profiler.schedule(wait=1, warmup=1, active=3),\n",
    "    on_trace_ready=print_profiler_results,\n",
    "    record_shapes=True,\n",
    "    profile_memory=True\n",
    ") as profiler:\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f'Epoch {epoch+1}')\n",
    "        model.train()\n",
    "        trndl = DataLoader(train_set, batch_size=batch_size, num_workers=7, shuffle=False, pin_memory=True)\n",
    "        for i, batch in enumerate(trndl):\n",
    "            optimizer.zero_grad()\n",
    "            Y_ji_batch = batch['Y_ji'].to(device)\n",
    "            X_ji_batch = batch['X_ji'].to(device)\n",
    "            C_j_batch = batch['C_j'].to(device)\n",
    "            outputs = model(Y_ji_batch)\n",
    "            loss = loss_fn(X_ji_batch, C_j_batch, outputs)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            loss_hist_train[epoch] += loss.item()\n",
    "            profiler.step()\n",
    "        loss_hist_train[epoch] /= len(trndl)\n",
    "        del trndl\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sparse loss function\n",
    "\"\"\"\n",
    "def sparse_dense_mul(s, d):\n",
    "    i = s._indices()\n",
    "    v = s._values()\n",
    "    dv = d[i.squeeze()]\n",
    "    return torch.sparse.FloatTensor(i, v * dv, s.size())\n",
    "\n",
    "def sparse_dense_add(s, d):\n",
    "    s = s.to(d.device)\n",
    "\n",
    "    indices = s._indices()\n",
    "    if indices.dim() == 1:\n",
    "        indices = indices.squeeze()\n",
    "    values = s._values()\n",
    "\n",
    "    d[indices] += values\n",
    "\n",
    "    return d\n",
    "\n",
    "class SparseCustomLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SparseCustomLoss, self).__init__()\n",
    "\n",
    "    def forward(self, X_ji, C_j, rho_ji):\n",
    "        C_j_value = C_j[0]\n",
    "        X_ji = X_ji.squeeze(0)\n",
    "        rho_ji = rho_ji.squeeze(0).squeeze(1)\n",
    "        #X_ji_sparse = X_ji.to_sparse()\n",
    "        #term1 = sparse_dense_mul(X_ji_sparse, rho_ji)\n",
    "        #term2 = C_j_value * torch.exp(-rho_ji)\n",
    "        #loss = sparse_dense_add(term1, term2)\n",
    "        loss = X_ji * rho_ji + C_j * torch.exp(-rho_ji)\n",
    "        return (loss).mean()\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Anaconda3 (Python 3.7.6)",
   "language": "python",
   "name": "anaconda3_2020.02"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
