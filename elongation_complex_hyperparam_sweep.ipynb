{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run CPU-only, GPU code needs further testing\n",
    "\"\"\"\n",
    "Restart kernel after running\n",
    "Only need to run once\n",
    "\"\"\"\n",
    "!pip install scikit-learn matplotlib seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/grid/it/data/elzar/easybuild/software/Anaconda3/2020.02/lib/python3.7/site-packages/requests/__init__.py:91: RequestsDependencyWarning: urllib3 (2.0.7) or chardet (3.0.4) doesn't match a supported version!\n",
      "  RequestsDependencyWarning)\n",
      "/grid/siepel/home_norepl/hassett/.local/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3553: DtypeWarning: Columns (0) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as colors\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.utils.data as td\n",
    "import torch.nn as nn\n",
    "import torch.nn.init as init\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torch.utils.data import DataLoader, Dataset, Subset\n",
    "import wandb\n",
    "\n",
    "\n",
    "froot = './data/k562_samp_epft_norm_test_1.csv'\n",
    "\n",
    "df = pd.read_csv(froot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mhassett\u001b[0m (\u001b[33melongation-net\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  seqnames     start       end strand  ensembl_gene_id  score      ctcf  \\\n",
      "0       15  88623545  88623545      +  ENSG00000181026    0.0 -0.079992   \n",
      "1       15  88623546  88623546      +  ENSG00000181026    0.0 -0.079942   \n",
      "2       15  88623547  88623547      +  ENSG00000181026    0.0 -0.079893   \n",
      "3       15  88623548  88623548      +  ENSG00000181026    0.0 -0.079844   \n",
      "4       15  88623549  88623549      +  ENSG00000181026    0.0 -0.079796   \n",
      "\n",
      "   h3k36me3   h3k4me1  h3k79me2   h3k9me1   h3k9me3  h4k20me1       sj5  \\\n",
      "0 -0.000099  0.348531  4.423451  0.446508 -0.168099  3.232475 -0.028916   \n",
      "1  0.001638  0.352677  4.460072  0.453024 -0.169218  3.259194 -0.028916   \n",
      "2  0.003360  0.356807  4.496664  0.459491 -0.170339  3.285849 -0.028916   \n",
      "3  0.005065  0.360919  4.533223  0.465908 -0.171461  3.312435 -0.028916   \n",
      "4  0.006754  0.365013  4.569743  0.472274 -0.172584  3.338952 -0.028916   \n",
      "\n",
      "        sj3       dms  wgbs      rpts  lambda_alphaj      zeta  \n",
      "0 -0.057178 -0.307549   0.0  0.249626       0.052255  0.993283  \n",
      "1 -0.057178 -0.307549   0.0  0.249626       0.052255  0.992642  \n",
      "2 -0.057178 -0.307549   0.0  0.249626       0.052255  0.992008  \n",
      "3 -0.057178 -0.307549   0.0  0.249626       0.052255  0.991381  \n",
      "4 -0.057178 -0.307549   0.0  0.249626       0.052255  0.990762  \n"
     ]
    }
   ],
   "source": [
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ctcf' 'h3k36me3' 'h3k4me1' 'h3k79me2' 'h3k9me1' 'h3k9me3' 'h4k20me1'\n",
      " 'sj5' 'sj3' 'dms' 'wgbs' 'rpts']\n"
     ]
    }
   ],
   "source": [
    "column_names = np.array(df.columns)\n",
    "feature_names = column_names[6:-2]\n",
    "num_features = len(feature_names)\n",
    "#nucleotides = column_names[-6:-2]\n",
    "print(feature_names)\n",
    "#print(nucleotides)\n",
    "num_samples = df.shape[0]\n",
    "\n",
    "# process read counts\n",
    "X_ji = df['score'].values\n",
    "\n",
    "# process GLM simulated elongation rates\n",
    "Z_ji = df['zeta'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Samples: 16182613\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of Samples: \" + str(num_samples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Features: 12\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of Features: \" + str(num_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16182613, 12)\n"
     ]
    }
   ],
   "source": [
    "#Y_ji is a list of samples containing lists of their feature values\n",
    "    # [   \n",
    "    #   sample_1: [feat_1, feat_2,...,feat_n],\n",
    "    #   sample_2: [feat_1, feat_2,...,feat_n],\n",
    "    # ]\n",
    "\n",
    "Y_ji = df.iloc[:, 6:-2].values\n",
    "Y_ji_shape = Y_ji.shape\n",
    "print(Y_ji.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "C_j = df['lambda_alphaj'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "gene_ids = df['ensembl_gene_id'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "sweep_config = {\n",
    "    'method': 'grid'\n",
    "}\n",
    "metric = {\n",
    "    'name': 'loss',\n",
    "    'goal': 'minimize'   \n",
    "    }\n",
    "\n",
    "sweep_config['metric'] = metric\n",
    "\n",
    "parameters_dict = {\n",
    "    'optimizer': {\n",
    "        'values': ['adam', 'sgd']\n",
    "        },\n",
    "    'momentum': {\n",
    "        'values': [0, 0.5, 0.8, 0.9]\n",
    "        },\n",
    "    'learn_rate': {\n",
    "          'values': [0.001, 0.0001, 0.00001, 0.000005, 1e-5, 1e-6, 1e-7, 1e-8]\n",
    "        },\n",
    "    'batch_size': {\n",
    "        'values': [500, 1000, 2000, 5000]\n",
    "    },\n",
    "    }\n",
    "\n",
    "parameters_dict.update({\n",
    "    'epochs': {\n",
    "        'value': 3}\n",
    "    })\n",
    "\n",
    "sweep_config['parameters'] = parameters_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create sweep with ID: gdmgh1ne\n",
      "Sweep URL: https://wandb.ai/elongation-net/elongation-net/sweeps/gdmgh1ne\n"
     ]
    }
   ],
   "source": [
    "sweep_id = wandb.sweep(sweep_config, project=\"elongation-net\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_batch_size = 2000#2000#64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA (GPU support) is available: False\n",
      "Number of GPUs available: 0\n"
     ]
    }
   ],
   "source": [
    "cuda_available = torch.cuda.is_available()\n",
    "print(\"CUDA (GPU support) is available:\", cuda_available)\n",
    "num_gpus = torch.cuda.device_count()\n",
    "print(\"Number of GPUs available:\", num_gpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GeneDataset(Dataset):\n",
    "    def __init__(self, batches):\n",
    "        self.batches = batches\n",
    "        self.cache = {}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.batches)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if idx in self.cache:\n",
    "            return self.cache[idx]\n",
    "        \n",
    "        batch = self.batches[idx]\n",
    "        gene_id = batch['GeneId'].values[0]\n",
    "                \n",
    "        y_ji_array = np.array(batch['Y_ji'].tolist()).reshape(-1, 12)\n",
    "        y_ji_tensor = torch.tensor(y_ji_array, dtype=torch.float32)\n",
    "            \n",
    "        data = batch.drop(columns=['GeneId', 'dataset', 'Y_ji'])\n",
    "        tensor_data = torch.tensor(data.values, dtype=torch.float32)\n",
    "    \n",
    "        result = {\n",
    "            'GeneId': batch['GeneId'].values[0],\n",
    "            'Y_ji': y_ji_tensor,\n",
    "        }\n",
    "        for col in data.columns:\n",
    "            result[col] = tensor_data[:, data.columns.get_loc(col)]\n",
    "            \n",
    "        self.cache[idx] = result\n",
    "\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data = pd.DataFrame({\n",
    "    'GeneId': gene_ids,\n",
    "    'Y_ji': [row for row in Y_ji],\n",
    "    'X_ji': X_ji,\n",
    "    'C_j': C_j,\n",
    "    'Z_ji': Z_ji\n",
    "})\n",
    "\n",
    "grouped = data.groupby('GeneId')\n",
    "\n",
    "train_idx, temp_idx = train_test_split(list(grouped.groups.keys()), test_size=0.2, random_state=42)\n",
    "\n",
    "val_idx, test_idx = train_test_split(temp_idx, test_size=0.5, random_state=42)\n",
    "\n",
    "dataset_mapping = {gene_id: 'train' for gene_id in train_idx}\n",
    "dataset_mapping.update({gene_id: 'val' for gene_id in val_idx})\n",
    "dataset_mapping.update({gene_id: 'test' for gene_id in test_idx})\n",
    "\n",
    "data['dataset'] = data['GeneId'].map(dataset_mapping)\n",
    "\n",
    "train_data = data[data['dataset'] == 'train']\n",
    "val_data = data[data['dataset'] == 'val']\n",
    "test_data = data[data['dataset'] == 'test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train data size: 12713808\n",
      "val data size: 1798949\n",
      "test data size: 1669856\n"
     ]
    }
   ],
   "source": [
    "print(\"train data size: \" + str(len(train_data)))\n",
    "print(\"val data size: \" + str(len(val_data)))\n",
    "print(\"test data size: \" + str(len(test_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train # genes: 415\n",
      "val # genes: 52\n",
      "test # genes: 52\n"
     ]
    }
   ],
   "source": [
    "print(\"train # genes: \" + str(len(train_data.groupby('GeneId'))))\n",
    "print(\"val # genes: \" + str(len(val_data.groupby('GeneId'))))\n",
    "print(\"test # genes: \" + str(len(test_data.groupby('GeneId'))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_batches(grouped_data, max_batch_size):\n",
    "    batches = []\n",
    "    for _, group in grouped_data:\n",
    "        # Check if the group size exceeds the max_batch_size\n",
    "        if len(group) > max_batch_size:\n",
    "            # Split the group into smaller batches\n",
    "            for start_idx in range(0, len(group), max_batch_size):\n",
    "                end_idx = start_idx + max_batch_size\n",
    "                batch = group.iloc[start_idx:end_idx]\n",
    "                batches.append(batch)\n",
    "        else:\n",
    "            # If the group size is within the limit, add it as is\n",
    "            batches.append(group)\n",
    "    return batches\n",
    "\n",
    "\n",
    "train_batches = create_batches(train_data.groupby('GeneId'), 64)\n",
    "val_batches = create_batches(val_data.groupby('GeneId'), 64)\n",
    "test_batches = create_batches(test_data.groupby('GeneId'), 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dataset(train_data, batch_size):\n",
    "    batches = create_batches(train_data.groupby('GeneId'), batch_size)\n",
    "    batches = batches[0:10000]\n",
    "    dataset = GeneDataset(batches)\n",
    "    loader = DataLoader(dataset, batch_size=1, num_workers=7, shuffle=False, pin_memory=True)\n",
    "    return loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = GeneDataset(train_batches)\n",
    "val_dataset = GeneDataset(val_batches)\n",
    "test_dataset = GeneDataset(test_batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=1)\n",
    "val_loader = DataLoader(val_dataset, batch_size=1)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model():\n",
    "    \"\"\"\n",
    "    model = nn.Sequential(  # fully-connected, single hidden layer\n",
    "        nn.Conv1d(12, 64, kernel_size=50),\n",
    "        nn.Flatten(start_dim=1, end_dim=-1),\n",
    "        nn.Linear(124864, 2000))\n",
    "    \"\"\"\n",
    "\n",
    "    class LSTMModel(nn.Module):\n",
    "        def __init__(self, input_size, hidden_layer_size, output_size, num_layers):\n",
    "            super(LSTMModel, self).__init__()\n",
    "            self.lstm = nn.LSTM(input_size, hidden_layer_size, num_layers, bidirectional=True, batch_first=True)\n",
    "            self.linear = nn.Linear(2 * hidden_layer_size, output_size)\n",
    "\n",
    "        def forward(self, x):\n",
    "            x, _ = self.lstm(x)\n",
    "            x = self.linear(x)\n",
    "            return x\n",
    "        \n",
    "    model = LSTMModel(12, 100, 1, 1)\n",
    "    \n",
    "    if cuda_available:\n",
    "        if num_gpus > 1:\n",
    "            print(\"Using\", num_gpus, \"GPUs\")\n",
    "            model = torch.nn.DataParallel(model)\n",
    "        model = model.to('cuda')\n",
    "\n",
    "    print(model)\n",
    "    \"\"\"\n",
    "    arr = torch.randn((1,12,2000)).to(device)\n",
    "    print(model(arr).shape)\n",
    "    nparm = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    print(\"Number of parameters: \" + str(nparm))\n",
    "    \"\"\"\n",
    "\n",
    "    first_param_device = next(model.parameters()).device\n",
    "    print(\"Model is on device:\", first_param_device)\n",
    "\n",
    "    return model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CustomLoss, self).__init__()\n",
    "\n",
    "    def forward(self, X_ji, C_j, rho_ji):\n",
    "        #print(rho_ji.shape)\n",
    "        loss = X_ji * rho_ji + C_j * torch.exp(-rho_ji) - X_ji * torch.log(C_j)\n",
    "        return (loss).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nwith torch.no_grad():\\n    for param in model.parameters():\\n        param.zero_()\\n'"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "with torch.no_grad():\n",
    "    for param in model.parameters():\n",
    "        param.zero_()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_optimizer(network, optimizer, learning_rate, momentum):\n",
    "    if optimizer == \"sgd\":\n",
    "        optimizer = optim.SGD(network.parameters(),\n",
    "                              lr=learning_rate, momentum=momentum)\n",
    "    elif optimizer == \"adam\":\n",
    "        optimizer = optim.Adam(network.parameters(),\n",
    "                               lr=learning_rate)\n",
    "    return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_profiler_results(profiler):\n",
    "    # Sort by CUDA memory usage if using CUDA, else sort by CPU memory usage\n",
    "    sort_by_key = \"cuda_time_total\" if torch.cuda.is_available() else \"cpu_time_total\"\n",
    "    print(profiler.key_averages().table(sort_by=sort_by_key, row_limit=10))\n",
    "\n",
    "def train_epoch(model, loader, optimizer, loss_fn, profiler):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for idx, batch in enumerate(loader):\n",
    "        optimizer.zero_grad()\n",
    "        Y_ji_batch = batch['Y_ji'].to(device)\n",
    "        X_ji_batch = batch['X_ji'].to(device)\n",
    "        C_j_batch = batch['C_j'].to(device)\n",
    "        outputs = model(Y_ji_batch)\n",
    "        loss = loss_fn(X_ji_batch, C_j_batch, outputs)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        #print(total_loss)\n",
    "        #profiler.step()\n",
    "    avg_loss = total_loss / len(loader)\n",
    "    return avg_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(profiler):#config=None):\n",
    "    #with wandb.init(config=config):\n",
    "     #   config = wandb.config\n",
    "    model = build_model()\n",
    "    loader = build_dataset(train_data, 2000)\n",
    "    optimizer = build_optimizer(model, \"adam\", 1e-4, 0)\n",
    "    loss_fn = CustomLoss()\n",
    "    \n",
    "    for epoch in range(3):#config.epochs):\n",
    "        print(f'Epoch {epoch+1}')\n",
    "        avg_loss = train_epoch(model, loader, optimizer, loss_fn, profiler)\n",
    "        print(\"loss: \"+ str(avg_loss))\n",
    "        #wandb.log({\"loss\": avg_loss, \"epoch\": epoch})\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTMModel(\n",
      "  (lstm): LSTM(12, 100, batch_first=True, bidirectional=True)\n",
      "  (linear): Linear(in_features=200, out_features=1, bias=True)\n",
      ")\n",
      "Model is on device: cpu\n",
      "Epoch 1\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/6925217.1.comp.q/ipykernel_1204207/4058919901.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mprofile_memory\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m ) as profiler:\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/6925217.1.comp.q/ipykernel_1204207/428164043.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(profiler)\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;31m#config.epochs):\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Epoch {epoch+1}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0mavg_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprofiler\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"loss: \"\u001b[0m\u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mavg_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0;31m#wandb.log({\"loss\": avg_loss, \"epoch\": epoch})\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/6925217.1.comp.q/ipykernel_1204207/4034718371.py\u001b[0m in \u001b[0;36mtrain_epoch\u001b[0;34m(model, loader, optimizer, loss_fn, profiler)\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mX_ji_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'X_ji'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mC_j_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'C_j'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY_ji_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_ji_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mC_j_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/6925217.1.comp.q/ipykernel_1204207/2637187352.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m             \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlstm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    678\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbatch_sizes\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    679\u001b[0m             result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n\u001b[0;32m--> 680\u001b[0;31m                               self.dropout, self.training, self.bidirectional, self.batch_first)\n\u001b[0m\u001b[1;32m    681\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    682\u001b[0m             result = _VF.lstm(input, batch_sizes, hx, self._flat_weights, self.bias,\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "with torch.profiler.profile(\n",
    "    activities=[torch.profiler.ProfilerActivity.CPU, torch.profiler.ProfilerActivity.CUDA],\n",
    "    schedule=torch.profiler.schedule(wait=1, warmup=1, active=3),\n",
    "    on_trace_ready=print_profiler_results,\n",
    "    record_shapes=True,\n",
    "    profile_memory=True\n",
    ") as profiler:\n",
    "    model = train(profiler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#wandb.agent(sweep_id, train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "filename = f\"models/Elongation_Model_{timestamp}.pth\"\n",
    "torch.save(model.state_dict(), filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Linear(num_features, 1, bias=False)\n",
    "model.load_state_dict(torch.load(\"models/Elongation_Model.pth\"))\n",
    "cuda_available = torch.cuda.is_available()\n",
    "print(\"CUDA (GPU support) is available:\", cuda_available)\n",
    "num_gpus = torch.cuda.device_count()\n",
    "print(\"Number of GPUs available:\", num_gpus)\n",
    "if cuda_available:\n",
    "    if num_gpus > 1:\n",
    "        print(\"Using\", num_gpus, \"GPUs\")\n",
    "        model = torch.nn.DataParallel(model)\n",
    "    model = model.to('cuda')\n",
    "\n",
    "first_param_device = next(model.parameters()).device\n",
    "print(\"Model is on device:\", first_param_device)\n",
    "model.double()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = model.weight.data.cpu().numpy()\n",
    "#bias = model.bias.data.cpu().numpy()\n",
    "\n",
    "combined = ', '.join([f'\"{s}\": {f}' for s, f in zip(feature_names, weights[0])])\n",
    "print(combined)\n",
    "\n",
    "#print(\"bias: \" + str(model.bias.data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glm_kappa = [-0.0224536145637661, -0.094592589, -0.023815382, 0.030402922, -0.067234092, -0.032196914, -0.040911478, -0.018557168, -0.033545905, -0.051103287, -0.204434712, 0.015831043]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "GLM K\n",
    "\n",
    "* ctcf: -0.02\n",
    "* h3k36me3: -0.09\n",
    "* h3k4me1: -0.02\n",
    "* h3k79me2: +0.03\n",
    "* h3k9me1: -0.06\n",
    "* h3k9me3: -0.03\n",
    "* h4k20me1: -0.04\n",
    "* sj5: -0.02\n",
    "* sj3: -0.03\n",
    "* dms->stem-loop: -0.05\n",
    "* rpts->low-complex: +0.01\n",
    "* wgbs->DNAm: -0.2\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = range(1, len(loss_hist_train) + 1)\n",
    "plt.plot(epochs, loss_hist_train, label='train_loss')\n",
    "plt.plot(epochs, loss_hist_valid, label='valid_loss')\n",
    "\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_data(glm_zeta, net_zeta):\n",
    "    indices = range(len(glm_zeta))\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(10, 5))\n",
    "    \n",
    "    ax.scatter(indices, net_zeta, color='blue', label='Neural Net Zeta', s=10, alpha=0.5)\n",
    "    ax.scatter(indices, glm_zeta, color='orange', label='GLM Zeta', s=10, alpha=0.5)\n",
    "    \n",
    "    ax.set_title('Neural Net vs GLM Elongation Rate')\n",
    "    ax.set_xlabel('Index')\n",
    "    ax.set_ylabel('Elongation Rate')\n",
    "    ax.legend()\n",
    "    \n",
    "    plt.ylim(0.5, 1.3)\n",
    "\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_batches = create_batches(test_data.groupby('GeneId'), 2000)\n",
    "\n",
    "test_dataset = GeneDataset(test_batches)\n",
    "tstdl = DataLoader(test_dataset, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "\n",
    "data_iter = iter(tstdl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0, 4):\n",
    "    inputs = next(data_iter) \n",
    "    print(\"number of samples: \" + str(len(inputs)))\n",
    "\n",
    "    with torch.no_grad():\n",
    "        y_inputs = inputs['Y_ji'].to(device)\n",
    "        rho_ji = model(y_inputs)\n",
    "        print(rho_ji)\n",
    "\n",
    "    glm_zeta = inputs['Z_ji'][0]\n",
    "    # convert log(Z) outputs to Z\n",
    "    net_zeta = torch.exp(rho_ji.cpu().squeeze())\n",
    "    \n",
    "    plot_data(glm_zeta, net_zeta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net_zeta = []\n",
    "glm_zeta = []\n",
    "with torch.no_grad():\n",
    "    for batch in tstdl:\n",
    "        y_inputs = batch['Y_ji'].to(device)\n",
    "        rho_ji = model(y_inputs)\n",
    "        # convert log(Z) outputs to Z\n",
    "        net_zeta.append(torch.exp(rho_ji.cpu()[0]))\n",
    "        glm_zeta.append(batch['Z_ji'][0])\n",
    "\n",
    "net_zeta = torch.cat(net_zeta, dim=0)\n",
    "glm_zeta = torch.cat(glm_zeta, dim=0)\n",
    "mae = F.l1_loss(net_zeta.squeeze(), glm_zeta)\n",
    "mse = F.mse_loss(net_zeta.squeeze(), glm_zeta)\n",
    "\n",
    "correlation_coefficient = np.corrcoef(glm_zeta, net_zeta.squeeze())[0, 1]\n",
    "print(\"Correlation Coefficient:\", correlation_coefficient)\n",
    "\n",
    "print(f\"Mean Absolute Error: {mae.item():.4f}\")\n",
    "print(f\"Mean Squared Error: {mse.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def density_plot(glm_zeta, net_zeta, gene_id):\n",
    "    sns.kdeplot(x=glm_zeta, y=net_zeta, fill=True, cmap=\"Blues\")\n",
    "            \n",
    "    plt.xlim([min(glm_zeta), max(glm_zeta)])\n",
    "    plt.ylim([min(net_zeta), max(net_zeta)])\n",
    "\n",
    "\n",
    "    plt.xlabel('GLM Elongation Rate')\n",
    "    plt.ylabel('Neural Net Elongation Rate')\n",
    "    plt.title(gene_id)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scatterplot(net_zeta, glm_zeta, gene_id):\n",
    "    indices = range(len(glm_zeta))\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(10, 5))\n",
    "    \n",
    "    min_val = min(min(net_zeta), min(glm_zeta))\n",
    "    max_val = max(max(net_zeta), max(glm_zeta))\n",
    "\n",
    "    plt.xlim(min_val, max_val)\n",
    "    plt.ylim(min_val, max_val)\n",
    "    \n",
    "    ax.scatter(net_zeta, glm_zeta, s=5)\n",
    "    \n",
    "    ax.set_title(gene_id)\n",
    "    ax.set_xlabel('Neural Net Zeta')\n",
    "    ax.set_ylabel('GLM Zeta')\n",
    "    ax.legend()\n",
    "\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_batches2 = create_batches(test_data.groupby('GeneId'), batch_size=2000)\n",
    "\n",
    "test_dataset2 = GeneDataset(test_batches2)\n",
    "tstdl2 = DataLoader(test_dataset2, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_loss = 0\n",
    "loss_fn = CustomLoss()\n",
    "for batch in tstdl2:\n",
    "    gene_id = batch['GeneId'][0]\n",
    "    model.eval()\n",
    "    #print(\"number of samples: \" + str(len(batch)))\n",
    "\n",
    "    with torch.no_grad():\n",
    "        y_inputs = batch['Y_ji'].to(device)\n",
    "        rho_ji = model(y_inputs)\n",
    "        loss = loss_fn(batch['X_ji'], batch['C_j'], rho_ji)\n",
    "    \n",
    "    total_loss += loss.item()\n",
    "\n",
    "    glm_zeta = batch['Z_ji'][0]\n",
    "    # convert log(Z) outputs to Z\n",
    "    net_zeta = torch.exp(rho_ji.cpu().squeeze())\n",
    "        \n",
    "    #density_plot(glm_zeta, net_zeta, gene_id)\n",
    "        \n",
    "    #scatterplot(predicted_zeta, simulated_zeta, gene_id)\n",
    "        \n",
    "    #plot_data(glm_zeta, net_zeta)\n",
    "        \n",
    "print(total_loss / len(tstdl2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 10))\n",
    "\n",
    "sns.scatterplot(x=glm_kappa, y=weights[0])\n",
    "\n",
    "for i in range(len(glm_kappa)):\n",
    "    plt.text(glm_kappa[i], weights[0][i], feature_names[i], fontsize=13, ha='right', va='top')\n",
    "plt.xlabel('GLM Weights')\n",
    "plt.ylabel('Neural Net Weights')\n",
    "\n",
    "max_val = max(np.max(glm_kappa), np.max(weights[0])) + 0.04\n",
    "min_val = min(np.min(glm_kappa), np.min(weights[0])) - 0.04\n",
    "\n",
    "plt.xlim(max_val, min_val)\n",
    "plt.ylim(max_val, min_val)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# profiling code\n",
    "\"\"\"\n",
    "def print_profiler_results(profiler):\n",
    "    print(profiler.key_averages().table(sort_by=\"self_cpu_time_total\", row_limit=10))\n",
    "\n",
    "with torch.profiler.profile(\n",
    "    activities=[torch.profiler.ProfilerActivity.CPU, torch.profiler.ProfilerActivity.CUDA],\n",
    "    schedule=torch.profiler.schedule(wait=1, warmup=1, active=3),\n",
    "    on_trace_ready=print_profiler_results,\n",
    "    record_shapes=True,\n",
    "    profile_memory=True\n",
    ") as profiler:\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f'Epoch {epoch+1}')\n",
    "        model.train()\n",
    "        trndl = DataLoader(train_set, batch_size=batch_size, num_workers=7, shuffle=False, pin_memory=True)\n",
    "        for i, batch in enumerate(trndl):\n",
    "            optimizer.zero_grad()\n",
    "            Y_ji_batch = batch['Y_ji'].to(device)\n",
    "            X_ji_batch = batch['X_ji'].to(device)\n",
    "            C_j_batch = batch['C_j'].to(device)\n",
    "            outputs = model(Y_ji_batch)\n",
    "            loss = loss_fn(X_ji_batch, C_j_batch, outputs)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            loss_hist_train[epoch] += loss.item()\n",
    "            profiler.step()\n",
    "        loss_hist_train[epoch] /= len(trndl)\n",
    "        del trndl\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Anaconda3 (Python 3.7.6)",
   "language": "python",
   "name": "anaconda3_2020.02"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
