{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Testing/Debugging File \"\"\"\n",
    "\n",
    "\"\"\"\n",
    "Restart kernel after running\n",
    "Only need to run once\n",
    "\"\"\"\n",
    "!pip install scikit-learn matplotlib seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.utils.data as td\n",
    "import torch.nn as nn\n",
    "import torch.nn.init as init\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torch.utils.data import DataLoader, Dataset, Subset, BatchSampler\n",
    "\n",
    "froot = './data/k562_samp_epft_norm_test_1.csv'\n",
    "df = pd.read_csv(froot)\n",
    "\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_names = np.array(df.columns)\n",
    "feature_names = column_names[6:-2]\n",
    "num_features = len(feature_names)\n",
    "print(feature_names)\n",
    "num_samples = df.shape[0]\n",
    "\n",
    "# process read counts per gene j, site i\n",
    "X_ji = df['score'].values\n",
    "\n",
    "# process GLM simulated elongation rates\n",
    "Z_ji = df['zeta'].values\n",
    "\n",
    "print(\"Number of Samples: \" + str(num_samples))\n",
    "print(\"Number of Features: \" + str(num_features))\n",
    "\n",
    "#Y_ji is a list of samples containing lists of their feature values\n",
    "    # [   \n",
    "    #   sample_1: [feat_1, feat_2,...,feat_n],\n",
    "    #   sample_2: [feat_1, feat_2,...,feat_n],\n",
    "    # ]\n",
    "\n",
    "Y_ji = df.iloc[:, 6:-2].values\n",
    "Y_ji_shape = Y_ji.shape\n",
    "print(Y_ji.shape)\n",
    "\n",
    "# read depth * initiation rate values per gene j\n",
    "C_j = df['lambda_alphaj'].values\n",
    "\n",
    "gene_ids = df['ensembl_gene_id'].values\n",
    "\n",
    "cuda_available = torch.cuda.is_available()\n",
    "print(\"CUDA (GPU support) is available:\", cuda_available)\n",
    "num_gpus = torch.cuda.device_count()\n",
    "print(\"Number of GPUs available:\", num_gpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GeneDataset(Dataset):\n",
    "    def __init__(self, grouped_data, use_sliding_window=False, window_size=None):\n",
    "        self.grouped_data = grouped_data\n",
    "        self.use_sliding_window = use_sliding_window\n",
    "        self.window_size = window_size\n",
    "        # store windows over sequences\n",
    "        self.segments = []\n",
    "\n",
    "        # use subsequence windows from genes\n",
    "        if self.use_sliding_window and window_size is not None:\n",
    "            self._create_segments()\n",
    "        # use full-length genes\n",
    "        else:\n",
    "            self._prepare_full_genes()\n",
    "    \n",
    "    # create windows over sequences\n",
    "    def _create_segments(self):\n",
    "        for gene_id, group in self.grouped_data:\n",
    "            gene_length = len(group)\n",
    "            for start_idx in range(0, gene_length - self.window_size + 1, self.window_size):\n",
    "                end_idx = start_idx + self.window_size\n",
    "                segment = group.iloc[start_idx:end_idx]\n",
    "                self.segments.append((gene_id, segment))\n",
    "    \n",
    "    def _prepare_full_genes(self):\n",
    "        for gene_id, group in self.grouped_data:\n",
    "            self.segments.append((gene_id, group))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.segments)\n",
    "\n",
    "    # prepare single window or gene\n",
    "    def __getitem__(self, idx):\n",
    "        gene_id, segment = self.segments[idx]\n",
    "        \n",
    "        y_ji_array = np.array(segment['Y_ji'].tolist()).reshape(-1, 12)\n",
    "        y_ji_tensor = torch.tensor(y_ji_array, dtype=torch.float64)\n",
    "        \n",
    "        data = segment.drop(columns=[col for col in ['GeneId', 'dataset', 'Y_ji'] if col in segment.columns])\n",
    "        tensor_data = torch.tensor(data.values, dtype=torch.float64)\n",
    "        \n",
    "        result = {\n",
    "            'GeneId': gene_id,\n",
    "            'Y_ji': y_ji_tensor,\n",
    "            'gene_length': len(segment)\n",
    "        }\n",
    "        for col in data.columns:\n",
    "            result[col] = tensor_data[:, data.columns.get_loc(col)]\n",
    "\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import BatchSampler\n",
    "\n",
    "class GeneIdBatchSampler(BatchSampler):\n",
    "    def __init__(self, dataset, drop_last=False):\n",
    "        self.dataset = dataset\n",
    "        self.drop_last = drop_last\n",
    "        self.batches = self._create_batches()\n",
    "\n",
    "    def _create_batches(self):\n",
    "        # Group indices by GeneId\n",
    "        gene_id_to_indices = {}\n",
    "        for idx in range(len(self.dataset)):\n",
    "            gene_id = self.dataset[idx]['GeneId']\n",
    "            if gene_id not in gene_id_to_indices:\n",
    "                gene_id_to_indices[gene_id] = []\n",
    "            gene_id_to_indices[gene_id].append(idx)\n",
    "\n",
    "        return list(gene_id_to_indices.values())\n",
    "\n",
    "    def __iter__(self):\n",
    "        for batch in self.batches:\n",
    "            yield batch\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data = pd.DataFrame({\n",
    "    'GeneId': gene_ids,\n",
    "    'Y_ji': [row for row in Y_ji],\n",
    "    'X_ji': X_ji,\n",
    "    'C_j': C_j,\n",
    "    'Z_ji': Z_ji\n",
    "})\n",
    "\n",
    "grouped = data.groupby('GeneId')\n",
    "\n",
    "# split by gene into train, val, test sets\n",
    "train_idx, temp_idx = train_test_split(list(grouped.groups.keys()), test_size=0.2, random_state=42)\n",
    "val_idx, test_idx = train_test_split(temp_idx, test_size=0.5, random_state=42)\n",
    "\n",
    "# create dictionary mapping each gene id to its assigned train, val, test dataset labels\n",
    "dataset_mapping = {gene_id: 'train' for gene_id in train_idx}\n",
    "dataset_mapping.update({gene_id: 'val' for gene_id in val_idx})\n",
    "dataset_mapping.update({gene_id: 'test' for gene_id in test_idx})\n",
    "\n",
    "# filter rows based on assigned dataset field\n",
    "data['dataset'] = data['GeneId'].map(dataset_mapping)\n",
    "train_data = data[data['dataset'] == 'train']\n",
    "valid_data = data[data['dataset'] == 'val']\n",
    "test_data = data[data['dataset'] == 'test']\n",
    "\n",
    "print(\"train data size: \" + str(len(train_data)))\n",
    "print(\"val data size: \" + str(len(valid_data)))\n",
    "print(\"test data size: \" + str(len(test_data)) + \"\\n\")\n",
    "\n",
    "train_data = train_data.groupby('GeneId')\n",
    "valid_data = valid_data.groupby('GeneId')\n",
    "test_data = test_data.groupby('GeneId')\n",
    "print(\"train # genes: \" + str(len(train_data)))\n",
    "print(\"val # genes: \" + str(len(valid_data)))\n",
    "print(\"test # genes: \" + str(len(test_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dataset(train_data, use_sliding_window=False, window_size=None):\n",
    "    dataset = GeneDataset(train_data, use_sliding_window, window_size)\n",
    "    batch_sampler = GeneIdBatchSampler(dataset)\n",
    "    loader = DataLoader(dataset, batch_sampler=batch_sampler, shuffle=False, num_workers=7, pin_memory=True)\n",
    "    return loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math \n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def build_model(model_type, num_lstm_layers, bidirectional, hidden_layer_sizes, activation_func,\n",
    "                filter_size, pool_type, pool_size, dropout, window_size, weight_init):\n",
    "    \n",
    "    class LinearModel(nn.Module):\n",
    "        def __init__(self, input_size):\n",
    "            super(LinearModel, self).__init__()\n",
    "            self.name = \"linear\"\n",
    "            self.linear = nn.Linear(input_size, 1, bias=False)\n",
    "\n",
    "        def forward(self, x):\n",
    "            x = self.linear(x)\n",
    "            return x\n",
    "    \n",
    "    class LSTMModel(nn.Module):\n",
    "        def __init__(self, input_size, hidden_layer_sizes, output_size, num_layers, bidirectional):\n",
    "            super(LSTMModel, self).__init__()\n",
    "            self.name = \"lstm\"\n",
    "            self.lstm = nn.LSTM(input_size, hidden_layer_sizes[0], num_layers, bidirectional=bidirectional, batch_first=True)\n",
    "            self.bidirectional_linear = nn.Linear(2 * hidden_layer_sizes[0], output_size)\n",
    "            self.linear = nn.Linear(hidden_layer_sizes[0], output_size)\n",
    "            self.bidirectional = bidirectional\n",
    "\n",
    "        def forward(self, x):\n",
    "            x, _ = self.lstm(x)\n",
    "            if self.bidirectional:\n",
    "                x = self.bidirectional_linear(x)\n",
    "            else:\n",
    "                x = self.linear(x)\n",
    "            return x\n",
    "        \n",
    "    class DenseNet(nn.Module):\n",
    "        def __init__(self, input_size, hidden_layer_sizes, output_size, dropout, activation='relu'):\n",
    "            super(DenseNet, self).__init__()\n",
    "            \n",
    "            self.name = \"dense\"\n",
    "            \n",
    "            layers = []\n",
    "\n",
    "            # Define the input layer\n",
    "            prev_size = input_size\n",
    "\n",
    "            for size in hidden_layer_sizes:\n",
    "                layers.append(nn.Linear(prev_size, size))\n",
    "\n",
    "                if activation.lower() == 'leakyrelu':\n",
    "                    layers.append(nn.LeakyReLU())\n",
    "                elif activation.lower() == 'relu':\n",
    "                    layers.append(nn.ReLU())\n",
    "                elif activation.lower() == 'tanh':\n",
    "                    layers.append(nn.Tanh())\n",
    "                else:\n",
    "                    raise ValueError(\"Unsupported activation function\")\n",
    "\n",
    "                prev_size = size\n",
    "                \n",
    "            layers.append(nn.Dropout(0.5))\n",
    "\n",
    "            layers.append(nn.Linear(prev_size, output_size))\n",
    "\n",
    "            self.layers = nn.Sequential(*layers)\n",
    "\n",
    "        def forward(self, x):\n",
    "            return self.layers(x)\n",
    "\n",
    "    class DeepChromeCNN(nn.Module): \n",
    "        def __init__(self, input_size, hidden_layer_sizes, filter_size, pool_size, dropout, window_size):\n",
    "            super(DeepChromeCNN, self).__init__()\n",
    "            self.name = \"dc_cnn\"\n",
    "            self.conv1d = nn.Conv1d(input_size, hidden_layer_sizes[0], filter_size)\n",
    "            self.relu = nn.ReLU()\n",
    "            self.maxpool1d = nn.MaxPool1d(pool_size)\n",
    "            \n",
    "            self.dropout = nn.Dropout(dropout)\n",
    "            self.linear1_input_size = math.ceil((window_size-filter_size)/pool_size)*hidden_layer_sizes[0]\n",
    "            self.linear1 = nn.Linear(self.linear1_input_size, hidden_layer_sizes[1])\n",
    "            self.linear2 = nn.Linear(hidden_layer_sizes[1], hidden_layer_sizes[2])\n",
    "            self.linear3 = nn.Linear(hidden_layer_sizes[2], window_size)\n",
    "            \n",
    "\n",
    "        def forward(self, x):\n",
    "            x = x.permute(0, 2, 1)\n",
    "            x = self.conv1d(x)\n",
    "            x = self.relu(x)\n",
    "            x = self.maxpool1d(x)\n",
    "            \n",
    "            x = x.view(-1, self.linear1_input_size)\n",
    "            x = self.dropout(x)\n",
    "            x = self.linear1(x)\n",
    "            x = self.relu(x)\n",
    "            x = self.linear2(x)\n",
    "            x = self.relu(x)\n",
    "            x = self.linear3(x)\n",
    "            \n",
    "            return x\n",
    "        \n",
    "    class CNN(nn.Module): \n",
    "        def __init__(self, input_size, hidden_layer_sizes, filter_size, pool_type, pool_size, dropout, window_size):\n",
    "            super(CNN, self).__init__()\n",
    "            self.name = \"cnn\"\n",
    "            self.window_size = window_size\n",
    "            self.num_hidden_layers = len(hidden_layer_sizes)\n",
    "            self.pool_type = pool_type\n",
    "            \n",
    "            in_channels = input_size\n",
    "            self.convs = nn.ModuleList()\n",
    "            for out_channels in hidden_layer_sizes:\n",
    "                self.convs.append(\n",
    "                    nn.Conv1d(in_channels, out_channels, filter_size)\n",
    "                )\n",
    "                in_channels = out_channels\n",
    "                \n",
    "            self.relu = nn.ReLU()\n",
    "            self.maxpool1d = nn.MaxPool1d(pool_size)\n",
    "            self.avgpool1d = nn.AvgPool1d(pool_size)\n",
    "            self.dropout = nn.Dropout(dropout)\n",
    "            self.linear1_input_size = self.get_linear1_input_size(hidden_layer_sizes, window_size, filter_size, pool_size)\n",
    "            self.linear1 = nn.Linear(self.linear1_input_size, window_size)\n",
    "        \n",
    "        def get_linear1_input_size(self, hidden_layer_sizes, window_size, filter_size, pool_size):\n",
    "            num_hidden_layers = len(hidden_layer_sizes)\n",
    "            dim1_size = hidden_layer_sizes[-1]\n",
    "            dim2_size = window_size\n",
    "            for _ in range(num_hidden_layers):\n",
    "                # cnn layer filter shrinks dim (e.g. 100 -> 91)\n",
    "                dim2_size = dim2_size - filter_size + 1\n",
    "                # max pool rounds down for pooling (grouping by pool_size)\n",
    "                dim2_size = math.floor(dim2_size / pool_size)\n",
    "            # linear layer takes flattened [x, y] -> [x * y]\n",
    "            return dim1_size * dim2_size\n",
    "            \n",
    "        def forward(self, x):\n",
    "            x = x.permute(0, 2, 1)\n",
    "            batch_size = x.shape[0]\n",
    "            for conv in self.convs:\n",
    "                x = self.relu(conv(x))\n",
    "                if pool_type == 'max':\n",
    "                    x = self.maxpool1d(x)\n",
    "                else:\n",
    "                    x = self.avgpool1d(x)\n",
    "\n",
    "            x = x.view(-1, self.linear1_input_size)\n",
    "            x = self.dropout(x)\n",
    "            x = self.linear1(x)\n",
    "            \n",
    "            return x\n",
    "    \n",
    "    if model_type == 'lstm':\n",
    "        model = LSTMModel(num_features, hidden_layer_sizes, 1, num_lstm_layers, bidirectional)\n",
    "    elif model_type == 'linear':\n",
    "        model = LinearModel(num_features)\n",
    "    elif model_type == 'dense':\n",
    "        model = DenseNet(num_features, hidden_layer_sizes, 1, activation_func)\n",
    "    elif model_type == 'dc_cnn':\n",
    "        model = DeepChromeCNN(num_features, hidden_layer_sizes, filter_size, pool_size, dropout, window_size)\n",
    "    elif model_type == 'cnn':\n",
    "        model = CNN(num_features, hidden_layer_sizes, filter_size, pool_type, pool_size, dropout, window_size)\n",
    "        \n",
    "    \n",
    "    if cuda_available:\n",
    "        #if num_gpus > 1:\n",
    "            #print(\"Using\", num_gpus, \"GPUs\")\n",
    "            #model = torch.nn.DataParallel(model)\n",
    "        model = model.to('cuda')\n",
    "\n",
    "    print(model)\n",
    "    \n",
    "    \"\"\"\n",
    "    # print # model parameters\n",
    "    arr = torch.randn((1,12,2000)).to(device)\n",
    "    print(model(arr).shape)\n",
    "    nparm = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    print(\"Number of parameters: \" + str(nparm))\n",
    "    \"\"\"\n",
    "\n",
    "    first_param_device = next(model.parameters()).device\n",
    "    print(\"Model is on device:\", first_param_device)\n",
    "    \n",
    "    # expected weights are close to 0 which is why 0 initializing weights converges much quicker\n",
    "    if weight_init == 'zero':\n",
    "        with torch.no_grad():\n",
    "            for param in model.parameters():\n",
    "                param.zero_()\n",
    "    \n",
    "    model.double()\n",
    "\n",
    "    return model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_optimizer(network, optimizer, learning_rate, momentum):\n",
    "    if optimizer == \"sgd\":\n",
    "        optimizer = optim.SGD(network.parameters(),\n",
    "                              lr=learning_rate, momentum=momentum)\n",
    "        \n",
    "    # Adam optimizer adapts the learning rate for each parameter individually\n",
    "    elif optimizer == \"adam\":\n",
    "        optimizer = optim.Adam(network.parameters(),\n",
    "                               lr=learning_rate)\n",
    "    return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_optimizer(network, optimizer, learning_rate, momentum):\n",
    "    if optimizer == \"sgd\":\n",
    "        optimizer = optim.SGD(network.parameters(),\n",
    "                              lr=learning_rate, momentum=momentum)\n",
    "        \n",
    "    # Adam optimizer adapts the learning rate for each parameter individually\n",
    "    elif optimizer == \"adam\":\n",
    "        optimizer = optim.Adam(network.parameters(),\n",
    "                               lr=learning_rate)\n",
    "    return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, loader, optimizer, loss_fn):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for idx, batch in enumerate(loader):\n",
    "        optimizer.zero_grad()\n",
    "        Y_ji_batch = batch['Y_ji'].to(device) \n",
    "        X_ji_batch = batch['X_ji'].to(device)\n",
    "        C_j_batch = batch['C_j'].to(device)\n",
    "        lengths = batch['gene_length'].to(device)\n",
    "        \n",
    "        outputs = model(Y_ji_batch)\n",
    "        \n",
    "        if model.name == 'dc_cnn' or model.name == 'cnn':\n",
    "            rho_ji = outputs\n",
    "        else:\n",
    "            rho_ji = outputs.squeeze(2)\n",
    "        \n",
    "        loss = loss_fn(X_ji_batch, C_j_batch, rho_ji, lengths)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # calculate average loss across all batches\n",
    "        total_loss += loss.item()\n",
    "    avg_train_loss = total_loss / len(loader)\n",
    "    \n",
    "    return avg_train_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CustomLoss, self).__init__()\n",
    "\n",
    "    def forward(self, X_ji, C_j, rho_ji, lengths):\n",
    "        C_j_value = C_j[0]\n",
    "        loss = X_ji * rho_ji + C_j_value * torch.exp(-rho_ji) - X_ji * torch.log(C_j_value)\n",
    "        \n",
    "        # normalize loss by sequence length\n",
    "        loss_sum = loss.sum(dim=1)\n",
    "        normalized_loss = loss_sum / lengths.float()\n",
    "        \n",
    "        # calculate average loss within each batch\n",
    "        return (loss).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model configs\n",
    "model_type = 'lstm'\n",
    "weight_init = None\n",
    "hidden_layer_sizes = [9]\n",
    "activation_func = 'relu'\n",
    "pool_type = 'max'\n",
    "\n",
    "# lstm configs\n",
    "num_lstm_layers = 3\n",
    "bidirectional = True\n",
    "\n",
    "# deep chrome cnn configs\n",
    "filter_size = 10\n",
    "pool_size = 5\n",
    "dropout = 0.5\n",
    "\n",
    "# dataset configs\n",
    "use_sliding_window = True\n",
    "window_size = 1000\n",
    "\n",
    "# optimizer configs\n",
    "learning_rate = 1e-3\n",
    "optimizer_type = 'adam'\n",
    "momentum = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "epochs = 20\n",
    "\n",
    "def train():\n",
    "    model = build_model(model_type, num_lstm_layers, bidirectional, hidden_layer_sizes, \n",
    "                        activation_func, filter_size, pool_type, pool_size, dropout, window_size, weight_init)\n",
    "    \n",
    "    train_loader = build_dataset(train_data, use_sliding_window, window_size)\n",
    "    valid_loader = build_dataset(valid_data, use_sliding_window, window_size)\n",
    "    \n",
    "    optimizer = build_optimizer(model, optimizer_type, learning_rate, momentum)\n",
    "    \n",
    "    loss_fn = CustomLoss()\n",
    "    # track loss curves\n",
    "    loss_neural_net_train = [0] * epochs\n",
    "    loss_neural_net_valid = [0] * epochs\n",
    "    loss_glm_valid = [0] * epochs\n",
    "    \n",
    "    # scheduler to reduce learning rate by half when new validation loss > old validation loss\n",
    "    old_train_loss = float('inf')\n",
    "    scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=1, verbose=True)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        print(f'Epoch {epoch+1}')\n",
    "        \n",
    "        train_loss = train_epoch(model, train_loader, optimizer, loss_fn)\n",
    "        loss_neural_net_train[epoch] = train_loss\n",
    "        print(f\"train loss: {train_loss: .4f}\")\n",
    "        \n",
    "        valid_neural_net_loss, valid_glm_loss, neural_net_zeta, glm_zeta = valid_epoch(model, valid_loader, loss_fn)\n",
    "        loss_neural_net_valid[epoch] = valid_neural_net_loss\n",
    "        loss_glm_valid[epoch] = valid_glm_loss\n",
    "        print(f\"valid neural net loss: {valid_neural_net_loss: .4f}\")\n",
    "        print(f\"valid glm loss: {valid_glm_loss: .4f}\")\n",
    "        \n",
    "        # compute metrics\n",
    "        mae = F.l1_loss(neural_net_zeta.squeeze(), glm_zeta)\n",
    "        mse = F.mse_loss(neural_net_zeta.squeeze(), glm_zeta)\n",
    "        correlation_coefficient = np.corrcoef(glm_zeta, neural_net_zeta.squeeze())[0, 1]\n",
    "        print(\"Correlation Coefficient:\", correlation_coefficient)\n",
    "        print(f\"Mean Absolute Error: {mae.item():.4f}\")\n",
    "        print(f\"Mean Squared Error: {mse.item():.4f}\")\n",
    "        \n",
    "        # reduce learning rate if new loss > old loss\n",
    "        if train_loss > old_train_loss:\n",
    "            optimizer.param_groups[0]['lr'] *= 0.5\n",
    "            print(f\"Reduced learning rate to {optimizer.param_groups[0]['lr']}\")\n",
    "        old_train_loss = train_loss\n",
    "        scheduler.step(train_loss)\n",
    "        \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model parameters\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "filename = f\"models/LSTM_Model.pth\"\n",
    "torch.save(model.state_dict(), filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model state\n",
    "\n",
    "model = build_model(model_type, num_lstm_layers, bidirectional, hidden_layer_sizes, activation_func,\n",
    "                filter_size, pool_type, pool_size, dropout, window_size, weight_init)\n",
    "\n",
    "model.load_state_dict(torch.load(\"models/LSTM_Model.pth\", map_location=torch.device('cpu')))\n",
    "\n",
    "cuda_available = torch.cuda.is_available()\n",
    "print(\"CUDA (GPU support) is available:\", cuda_available)\n",
    "num_gpus = torch.cuda.device_count()\n",
    "print(\"Number of GPUs available:\", num_gpus)\n",
    "if cuda_available:\n",
    "    if num_gpus > 1:\n",
    "        print(\"Using\", num_gpus, \"GPUs\")\n",
    "        model = torch.nn.DataParallel(model)\n",
    "    model = model.to('cuda')\n",
    "\n",
    "first_param_device = next(model.parameters()).device\n",
    "print(\"Model is on device:\", first_param_device)\n",
    "\n",
    "model.double()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = model.linear.weight.data.cpu().numpy()\n",
    "combined = ', '.join([f'\"{s}\": {f}' for s, f in zip(feature_names, weights[0])])\n",
    "print(combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glm_kappa = [-0.0224536145637661, -0.094592589, -0.023815382, 0.030402922, -0.067234092, -0.032196914, -0.040911478, -0.018557168, -0.033545905, -0.051103287, -0.204434712, 0.015831043]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "GLM K\n",
    "\n",
    "* ctcf: -0.02\n",
    "* h3k36me3: -0.09\n",
    "* h3k4me1: -0.02\n",
    "* h3k79me2: +0.03\n",
    "* h3k9me1: -0.06\n",
    "* h3k9me3: -0.03\n",
    "* h4k20me1: -0.04\n",
    "* sj5: -0.02\n",
    "* sj3: -0.03\n",
    "* dms->stem-loop: -0.05\n",
    "* rpts->low-complex: +0.01\n",
    "* wgbs->DNAm: -0.2\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot loss curve\n",
    "\n",
    "epochs = range(1, len(loss_neural_net_train) + 1)\n",
    "plt.plot(epochs, loss_train, label='train_neural_net_loss')\n",
    "plt.plot(epochs, loss_neural_net_valid, label='valid_neural_net_loss')\n",
    "plt.plot(epochs, loss_glm_valid, label='valid_glm_loss')\n",
    "\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_data(glm_zeta, net_zeta):\n",
    "    indices = range(len(glm_zeta))\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(10, 5))\n",
    "    \n",
    "    ax.scatter(indices, net_zeta, color='blue', label='Neural Net Zeta', s=10, alpha=0.5)\n",
    "    ax.scatter(indices, glm_zeta, color='orange', label='GLM Zeta', s=10, alpha=0.5)\n",
    "    \n",
    "    ax.set_title('Neural Net vs GLM Elongation Rate')\n",
    "    ax.set_xlabel('Index')\n",
    "    ax.set_ylabel('Elongation Rate')\n",
    "    ax.legend()\n",
    "    \n",
    "    plt.ylim(0.5, 1.5)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "\n",
    "tstdl = build_dataset(test_data, use_sliding_window, window_size)\n",
    "data_iter = iter(tstdl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute metrics for test dataset\n",
    "\n",
    "loss_fn = CustomLoss()\n",
    "\n",
    "total_net_loss = 0\n",
    "total_glm_loss = 0\n",
    "with torch.no_grad():\n",
    "    for batch in tstdl:\n",
    "        y_inputs = batch['Y_ji'].to(device)\n",
    "        X_ji = batch['X_ji'].to(device)\n",
    "        C_j = batch['C_j'].to(device)\n",
    "        lengths = batch['gene_length'].to(device)\n",
    "        Z_ji = batch['Z_ji'].to(device)\n",
    "        rho_ji = model(y_inputs)\n",
    "        net_loss = loss_fn(X_ji, C_j, rho_ji.squeeze(2), lengths)\n",
    "        glm_loss = loss_fn(X_ji, C_j, torch.log(Z_ji), lengths)\n",
    "        \n",
    "        total_net_loss += net_loss.item()\n",
    "        total_glm_loss += glm_loss.item()\n",
    "\n",
    "\n",
    "\n",
    "print(f\"Neural Net Loss: {total_net_loss/len(tstdl):.4f}\")\n",
    "print(f\"GLM Loss: {total_glm_loss/len(tstdl):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot for subset of genes in test dataset\n",
    "for i in range(0, 4):\n",
    "    inputs = next(data_iter) \n",
    "    print(\"number of samples: \" + str(len(inputs)))\n",
    "\n",
    "    with torch.no_grad():\n",
    "        y_inputs = inputs['Y_ji'].to(device)\n",
    "        rho_ji = model(y_inputs)\n",
    "\n",
    "    glm_zeta = inputs['Z_ji'][0]\n",
    "    # convert log(Z) outputs to Z\n",
    "    net_zeta = torch.exp(rho_ji.cpu().squeeze())\n",
    "    \n",
    "    plot_data(glm_zeta, net_zeta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute metrics for test dataset\n",
    "\n",
    "net_zeta = []\n",
    "glm_zeta = []\n",
    "with torch.no_grad():\n",
    "    for batch in tstdl:\n",
    "        y_inputs = batch['Y_ji'].to(device)\n",
    "        rho_ji = model(y_inputs)\n",
    "        # convert log(Z) outputs to Z\n",
    "        net_zeta.append(torch.exp(rho_ji.cpu()[0]))\n",
    "        glm_zeta.append(batch['Z_ji'][0])\n",
    "\n",
    "net_zeta = torch.cat(net_zeta, dim=0)\n",
    "glm_zeta = torch.cat(glm_zeta, dim=0)\n",
    "mae = F.l1_loss(net_zeta.squeeze(), glm_zeta)\n",
    "mse = F.mse_loss(net_zeta.squeeze(), glm_zeta)\n",
    "\n",
    "correlation_coefficient = np.corrcoef(glm_zeta, net_zeta.squeeze())[0, 1]\n",
    "print(\"Correlation Coefficient:\", correlation_coefficient)\n",
    "\n",
    "print(f\"Mean Absolute Error: {mae.item():.4f}\")\n",
    "print(f\"Mean Squared Error: {mse.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def density_plot(glm_zeta, net_zeta, gene_id):\n",
    "    sns.kdeplot(x=glm_zeta, y=net_zeta, fill=True, cmap=\"Blues\")\n",
    "            \n",
    "    plt.xlim([min(glm_zeta), max(glm_zeta)])\n",
    "    plt.ylim([min(net_zeta), max(net_zeta)])\n",
    "\n",
    "\n",
    "    plt.xlabel('GLM Elongation Rate')\n",
    "    plt.ylabel('Neural Net Elongation Rate')\n",
    "    plt.title(gene_id)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot for all genes in test dataset\n",
    "\n",
    "total_loss = 0\n",
    "loss_fn = CustomLoss()\n",
    "for batch in tstdl:\n",
    "    gene_id = batch['GeneId'][0]\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        y_inputs = batch['Y_ji'].to(device)\n",
    "        rho_ji = model(y_inputs)\n",
    "        \n",
    "    glm_zeta = batch['Z_ji']#[0]\n",
    "    # convert log(Z) outputs to Z\n",
    "    net_zeta = torch.exp(rho_ji.cpu().squeeze())\n",
    "        \n",
    "    density_plot(glm_zeta.flatten(), net_zeta.flatten(), gene_id)\n",
    "                \n",
    "    plot_data(glm_zeta.flatten(), net_zeta.flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot scatterplot of neural net weights and glm weights\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "\n",
    "sns.scatterplot(x=glm_kappa, y=weights[0])\n",
    "\n",
    "for i in range(len(glm_kappa)):\n",
    "    plt.text(glm_kappa[i], weights[0][i], feature_names[i], fontsize=13, ha='right', va='top')\n",
    "plt.xlabel('GLM Weights')\n",
    "plt.ylabel('Neural Net Weights')\n",
    "\n",
    "max_val = max(np.max(glm_kappa), np.max(weights[0])) + 0.04\n",
    "min_val = min(np.min(glm_kappa), np.min(weights[0])) - 0.04\n",
    "\n",
    "plt.xlim(max_val, min_val)\n",
    "plt.ylim(max_val, min_val)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Anaconda3 (Python 3.7.6)",
   "language": "python",
   "name": "anaconda3_2020.02"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
