{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Master File w/ hyperparameter sweeping across multiple architectures\"\"\"\n",
    "# Run CPU-only, GPU code needs further testing\n",
    "\"\"\"\n",
    "Restart kernel after running\n",
    "Only need to run once\n",
    "\"\"\"\n",
    "!pip install scikit-learn matplotlib seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as colors\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.utils.data as td\n",
    "import torch.nn as nn\n",
    "import torch.nn.init as init\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torch.utils.data import DataLoader, Dataset, Subset\n",
    "import wandb\n",
    "\n",
    "\n",
    "froot = './data/k562_samp_epft_norm_test_1.csv'\n",
    "\n",
    "df = pd.read_csv(froot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_names = np.array(df.columns)\n",
    "feature_names = column_names[6:-2]\n",
    "num_features = len(feature_names)\n",
    "#nucleotides = column_names[-6:-2]\n",
    "print(feature_names)\n",
    "#print(nucleotides)\n",
    "num_samples = df.shape[0]\n",
    "\n",
    "# process read counts\n",
    "X_ji = df['score'].values\n",
    "\n",
    "# process GLM simulated elongation rates\n",
    "Z_ji = df['zeta'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of Samples: \" + str(num_samples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of Features: \" + str(num_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Y_ji is a list of samples containing lists of their feature values\n",
    "    # [   \n",
    "    #   sample_1: [feat_1, feat_2,...,feat_n],\n",
    "    #   sample_2: [feat_1, feat_2,...,feat_n],\n",
    "    # ]\n",
    "\n",
    "Y_ji = df.iloc[:, 6:-2].values\n",
    "Y_ji_shape = Y_ji.shape\n",
    "print(Y_ji.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C_j = df['lambda_alphaj'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gene_ids = df['ensembl_gene_id'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "sweep_config = {\n",
    "    'method': 'grid'\n",
    "}\n",
    "metric = {\n",
    "    'name': 'mae',\n",
    "    'goal': 'minimize'   \n",
    "    }\n",
    "\n",
    "sweep_config['metric'] = metric\n",
    "\n",
    "parameters_dict = {\n",
    "    'optimizer': {\n",
    "        'values': ['adam']\n",
    "        },\n",
    "    'learn_rate': {\n",
    "          'values': [1e-3]\n",
    "        },\n",
    "    'momentum': {\n",
    "        'values': [0]#, 0.9]\n",
    "    },\n",
    "    'batch_size': {\n",
    "        'values': [16000000]\n",
    "        },\n",
    "    'model_type': {\n",
    "        'values': ['linear']\n",
    "    },\n",
    "    'bidirectional': {\n",
    "        'values': [False]#, True]\n",
    "    },\n",
    "    'hidden_layer_size': {\n",
    "        'values': [100]#, 50, 200, 500]\n",
    "    },\n",
    "    'num_layers': {\n",
    "        'values': [1]#, 2]\n",
    "    }\n",
    "}\n",
    "\n",
    "parameters_dict.update({\n",
    "    'epochs': {\n",
    "        'value': 20}\n",
    "    })\n",
    "\n",
    "sweep_config['parameters'] = parameters_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create sweep with ID: m3emo6dd\n",
      "Sweep URL: https://wandb.ai/elongation-net/elongation-net/sweeps/m3emo6dd\n"
     ]
    }
   ],
   "source": [
    "sweep_id = wandb.sweep(sweep_config, project=\"elongation-net\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cuda_available = torch.cuda.is_available()\n",
    "print(\"CUDA (GPU support) is available:\", cuda_available)\n",
    "num_gpus = torch.cuda.device_count()\n",
    "print(\"Number of GPUs available:\", num_gpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GeneDataset(Dataset):\n",
    "    def __init__(self, batches):\n",
    "        self.batches = batches\n",
    "        self.cache = {}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.batches)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if idx in self.cache:\n",
    "            return self.cache[idx]\n",
    "        \n",
    "        batch = self.batches[idx]\n",
    "        gene_id = batch['GeneId'].values[0]\n",
    "                \n",
    "        y_ji_array = np.array(batch['Y_ji'].tolist()).reshape(-1, 12)\n",
    "        y_ji_tensor = torch.tensor(y_ji_array, dtype=torch.float32)\n",
    "            \n",
    "        data = batch.drop(columns=['GeneId', 'dataset', 'Y_ji'])\n",
    "        tensor_data = torch.tensor(data.values, dtype=torch.float32)\n",
    "    \n",
    "        result = {\n",
    "            'GeneId': batch['GeneId'].values[0],\n",
    "            'Y_ji': y_ji_tensor,\n",
    "        }\n",
    "        for col in data.columns:\n",
    "            result[col] = tensor_data[:, data.columns.get_loc(col)]\n",
    "            \n",
    "        self.cache[idx] = result\n",
    "\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data = pd.DataFrame({\n",
    "    'GeneId': gene_ids,\n",
    "    'Y_ji': [row for row in Y_ji],\n",
    "    'X_ji': X_ji,\n",
    "    'C_j': C_j,\n",
    "    'Z_ji': Z_ji\n",
    "})\n",
    "\n",
    "grouped = data.groupby('GeneId')\n",
    "\n",
    "train_idx, temp_idx = train_test_split(list(grouped.groups.keys()), test_size=0.2, random_state=42)\n",
    "\n",
    "val_idx, test_idx = train_test_split(temp_idx, test_size=0.5, random_state=42)\n",
    "\n",
    "dataset_mapping = {gene_id: 'train' for gene_id in train_idx}\n",
    "dataset_mapping.update({gene_id: 'val' for gene_id in val_idx})\n",
    "dataset_mapping.update({gene_id: 'test' for gene_id in test_idx})\n",
    "\n",
    "data['dataset'] = data['GeneId'].map(dataset_mapping)\n",
    "\n",
    "train_data = data[data['dataset'] == 'train']\n",
    "valid_data = data[data['dataset'] == 'val']\n",
    "test_data = data[data['dataset'] == 'test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"train data size: \" + str(len(train_data)))\n",
    "print(\"val data size: \" + str(len(valid_data)))\n",
    "print(\"test data size: \" + str(len(test_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"train # genes: \" + str(len(train_data.groupby('GeneId'))))\n",
    "print(\"val # genes: \" + str(len(valid_data.groupby('GeneId'))))\n",
    "print(\"test # genes: \" + str(len(test_data.groupby('GeneId'))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_batches(grouped_data, max_batch_size):\n",
    "    batches = []\n",
    "    for _, group in grouped_data:\n",
    "        # Check if the group size exceeds the max_batch_size\n",
    "        if len(group) > max_batch_size:\n",
    "            # Split the group into smaller batches\n",
    "            for start_idx in range(0, len(group), max_batch_size):\n",
    "                end_idx = start_idx + max_batch_size\n",
    "                batch = group.iloc[start_idx:end_idx]\n",
    "                batches.append(batch)\n",
    "        else:\n",
    "            # If the group size is within the limit, add it as is\n",
    "            batches.append(group)\n",
    "    return batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dataset(train_data, batch_size):\n",
    "    batches = create_batches(train_data.groupby('GeneId'), batch_size)\n",
    "    dataset = GeneDataset(batches)\n",
    "    loader = DataLoader(dataset, batch_size=1, num_workers=7, shuffle=False, pin_memory=True)\n",
    "    return loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(model_type, num_layers, hidden_layer_size, bidirectional):\n",
    "    \"\"\"\n",
    "    model = nn.Sequential(  # fully-connected, single hidden layer\n",
    "        nn.Conv1d(12, 64, kernel_size=50),\n",
    "        nn.Flatten(start_dim=1, end_dim=-1),\n",
    "        nn.Linear(124864, 2000))\n",
    "    \"\"\"\n",
    "\n",
    "    class LSTMModel(nn.Module):\n",
    "        def __init__(self, input_size, hidden_layer_size, output_size, num_layers, bidirectional):\n",
    "            super(LSTMModel, self).__init__()\n",
    "            self.lstm = nn.LSTM(input_size, hidden_layer_size, num_layers, bidirectional=bidirectional, batch_first=True)\n",
    "            self.linear = nn.Linear(2 * hidden_layer_size, output_size)\n",
    "\n",
    "        def forward(self, x):\n",
    "            x, _ = self.lstm(x)\n",
    "            x = self.linear(x)\n",
    "            return x\n",
    "    \n",
    "    if model_type == 'lstm':\n",
    "        model = LSTMModel(num_features, hidden_layer_size, 1, num_layers, bidirectional)\n",
    "    elif model_type == 'linear':\n",
    "        model = nn.Linear(num_features, 1, bias=False)\n",
    "    \n",
    "    if cuda_available:\n",
    "        if num_gpus > 1:\n",
    "            print(\"Using\", num_gpus, \"GPUs\")\n",
    "            model = torch.nn.DataParallel(model)\n",
    "        model = model.to('cuda')\n",
    "\n",
    "    print(model)\n",
    "    \"\"\"\n",
    "    arr = torch.randn((1,12,2000)).to(device)\n",
    "    print(model(arr).shape)\n",
    "    nparm = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    print(\"Number of parameters: \" + str(nparm))\n",
    "    \"\"\"\n",
    "\n",
    "    first_param_device = next(model.parameters()).device\n",
    "    print(\"Model is on device:\", first_param_device)\n",
    "\n",
    "    return model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CustomLoss, self).__init__()\n",
    "\n",
    "    def forward(self, X_ji, C_j, rho_ji):\n",
    "        C_j_value = C_j[0]\n",
    "        X_ji = X_ji.squeeze(0)\n",
    "        rho_ji = rho_ji.squeeze(0).squeeze(1)\n",
    "        loss = X_ji * rho_ji + C_j_value * torch.exp(-rho_ji) - X_ji * torch.log(C_j_value)\n",
    "        return (loss).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "with torch.no_grad():\n",
    "    for param in model.parameters():\n",
    "        param.zero_()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_optimizer(network, optimizer, learning_rate, momentum):\n",
    "    if optimizer == \"sgd\":\n",
    "        optimizer = optim.SGD(network.parameters(),\n",
    "                              lr=learning_rate, momentum=momentum)\n",
    "    # Adam optimizer adapts the learning rate for each parameter individually\n",
    "    elif optimizer == \"adam\":\n",
    "        optimizer = optim.Adam(network.parameters(),\n",
    "                               lr=learning_rate)\n",
    "    return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def valid_epoch(model, loader, loss_fn):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    net_zeta = []\n",
    "    glm_zeta = []\n",
    "    with torch.no_grad():\n",
    "        for idx, batch in enumerate(loader):\n",
    "            Y_ji_batch = batch['Y_ji'].to(device)\n",
    "            X_ji_batch = batch['X_ji'].to(device)\n",
    "            C_j_batch = batch['C_j'].to(device)\n",
    "            outputs = model(Y_ji_batch)\n",
    "            loss = loss_fn(X_ji_batch, C_j_batch, outputs)\n",
    "            total_loss += loss.item()\n",
    "            net_zeta.append(torch.exp(outputs.cpu()[0]))\n",
    "            glm_zeta.append(batch['Z_ji'][0])\n",
    "    avg_loss = total_loss / len(loader)\n",
    "    net_zeta = torch.cat(net_zeta, dim=0)\n",
    "    glm_zeta = torch.cat(glm_zeta, dim=0)\n",
    "    return avg_loss, net_zeta, glm_zeta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, loader, optimizer, loss_fn):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for idx, batch in enumerate(loader):\n",
    "        optimizer.zero_grad()\n",
    "        Y_ji_batch = batch['Y_ji'].to(device)\n",
    "        X_ji_batch = batch['X_ji'].to(device)\n",
    "        C_j_batch = batch['C_j'].to(device)\n",
    "        outputs = model(Y_ji_batch)\n",
    "        loss = loss_fn(X_ji_batch, C_j_batch, outputs)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    avg_train_loss = total_loss / len(loader)\n",
    "    return avg_train_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(config=None):\n",
    "    with wandb.init(config=config):\n",
    "        config = wandb.config\n",
    "        model = build_model(config.model_type, config.num_layers, config.hidden_layer_size, config.bidirectional)\n",
    "        train_loader = build_dataset(train_data, config.batch_size)\n",
    "        valid_loader = build_dataset(valid_data, config.batch_size)\n",
    "        optimizer = build_optimizer(model, config.optimizer, config.learn_rate, config.momentum)\n",
    "        loss_fn = CustomLoss()\n",
    "        loss_hist_train = [0] * config.epochs\n",
    "        loss_hist_valid = [0] * config.epochs\n",
    "        best_valid_loss = float('inf')\n",
    "\n",
    "        for epoch in range(config.epochs):\n",
    "            print(f'Epoch {epoch+1}')\n",
    "            train_loss = train_epoch(model, train_loader, optimizer, loss_fn)\n",
    "            loss_hist_train[epoch] = train_loss\n",
    "            print(\"train loss: \"+ str(train_loss))\n",
    "            valid_loss, net_zeta, glm_zeta = valid_epoch(model, valid_loader, loss_fn)\n",
    "            loss_hist_valid[epoch] = valid_loss\n",
    "            print(\"valid loss: \"+ str(valid_loss))\n",
    "            mae = F.l1_loss(net_zeta.squeeze(), glm_zeta)\n",
    "            mse = F.mse_loss(net_zeta.squeeze(), glm_zeta)\n",
    "            correlation_coefficient = np.corrcoef(glm_zeta, net_zeta.squeeze())[0, 1]\n",
    "            print(\"Correlation Coefficient:\", correlation_coefficient)\n",
    "            print(f\"Mean Absolute Error: {mae.item():.4f}\")\n",
    "            print(f\"Mean Squared Error: {mse.item():.4f}\")\n",
    "            wandb.log({\"epoch\": epoch, \"train_loss\": train_loss, \"valid_loss\": valid_loss, \"corr_coeff\": correlation_coefficient,\n",
    "                      \"mae\": mae.item(), \"mse\": mse.item()})\n",
    "            if valid_loss > best_valid_loss:\n",
    "                print(\"Early stopping due to increasing validation loss.\")\n",
    "                break\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.agent(sweep_id, train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "\"\"\"\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "filename = f\"models/Elongation_Model_{timestamp}.pth\"\n",
    "torch.save(model.state_dict(), filename)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Linear(num_features, 1, bias=False)\n",
    "model.load_state_dict(torch.load(\"models/Elongation_Model.pth\"))\n",
    "cuda_available = torch.cuda.is_available()\n",
    "print(\"CUDA (GPU support) is available:\", cuda_available)\n",
    "num_gpus = torch.cuda.device_count()\n",
    "print(\"Number of GPUs available:\", num_gpus)\n",
    "if cuda_available:\n",
    "    if num_gpus > 1:\n",
    "        print(\"Using\", num_gpus, \"GPUs\")\n",
    "        model = torch.nn.DataParallel(model)\n",
    "    model = model.to('cuda')\n",
    "\n",
    "first_param_device = next(model.parameters()).device\n",
    "print(\"Model is on device:\", first_param_device)\n",
    "model.double()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = model.weight.data.cpu().numpy()\n",
    "#bias = model.bias.data.cpu().numpy()\n",
    "\n",
    "combined = ', '.join([f'\"{s}\": {f}' for s, f in zip(feature_names, weights[0])])\n",
    "print(combined)\n",
    "\n",
    "#print(\"bias: \" + str(model.bias.data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glm_kappa = [-0.0224536145637661, -0.094592589, -0.023815382, 0.030402922, -0.067234092, -0.032196914, -0.040911478, -0.018557168, -0.033545905, -0.051103287, -0.204434712, 0.015831043]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "GLM K\n",
    "\n",
    "* ctcf: -0.02\n",
    "* h3k36me3: -0.09\n",
    "* h3k4me1: -0.02\n",
    "* h3k79me2: +0.03\n",
    "* h3k9me1: -0.06\n",
    "* h3k9me3: -0.03\n",
    "* h4k20me1: -0.04\n",
    "* sj5: -0.02\n",
    "* sj3: -0.03\n",
    "* dms->stem-loop: -0.05\n",
    "* rpts->low-complex: +0.01\n",
    "* wgbs->DNAm: -0.2\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = range(1, len(loss_hist_train) + 1)\n",
    "plt.plot(epochs, loss_hist_train, label='train_loss')\n",
    "plt.plot(epochs, loss_hist_valid, label='valid_loss')\n",
    "\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_data(glm_zeta, net_zeta):\n",
    "    indices = range(len(glm_zeta))\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(10, 5))\n",
    "    \n",
    "    ax.scatter(indices, net_zeta, color='blue', label='Neural Net Zeta', s=10, alpha=0.5)\n",
    "    ax.scatter(indices, glm_zeta, color='orange', label='GLM Zeta', s=10, alpha=0.5)\n",
    "    \n",
    "    ax.set_title('Neural Net vs GLM Elongation Rate')\n",
    "    ax.set_xlabel('Index')\n",
    "    ax.set_ylabel('Elongation Rate')\n",
    "    ax.legend()\n",
    "    \n",
    "    plt.ylim(0.5, 1.3)\n",
    "\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_batches = create_batches(test_data.groupby('GeneId'), 2000)\n",
    "\n",
    "test_dataset = GeneDataset(test_batches)\n",
    "tstdl = DataLoader(test_dataset, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "\n",
    "data_iter = iter(tstdl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0, 4):\n",
    "    inputs = next(data_iter) \n",
    "    print(\"number of samples: \" + str(len(inputs)))\n",
    "\n",
    "    with torch.no_grad():\n",
    "        y_inputs = inputs['Y_ji'].to(device)\n",
    "        rho_ji = model(y_inputs)\n",
    "        print(rho_ji)\n",
    "\n",
    "    glm_zeta = inputs['Z_ji'][0]\n",
    "    # convert log(Z) outputs to Z\n",
    "    net_zeta = torch.exp(rho_ji.cpu().squeeze())\n",
    "    \n",
    "    plot_data(glm_zeta, net_zeta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net_zeta = []\n",
    "glm_zeta = []\n",
    "with torch.no_grad():\n",
    "    for batch in tstdl:\n",
    "        y_inputs = batch['Y_ji'].to(device)\n",
    "        rho_ji = model(y_inputs)\n",
    "        # convert log(Z) outputs to Z\n",
    "        net_zeta.append(torch.exp(rho_ji.cpu()[0]))\n",
    "        glm_zeta.append(batch['Z_ji'][0])\n",
    "\n",
    "net_zeta = torch.cat(net_zeta, dim=0)\n",
    "glm_zeta = torch.cat(glm_zeta, dim=0)\n",
    "mae = F.l1_loss(net_zeta.squeeze(), glm_zeta)\n",
    "mse = F.mse_loss(net_zeta.squeeze(), glm_zeta)\n",
    "\n",
    "correlation_coefficient = np.corrcoef(glm_zeta, net_zeta.squeeze())[0, 1]\n",
    "print(\"Correlation Coefficient:\", correlation_coefficient)\n",
    "\n",
    "print(f\"Mean Absolute Error: {mae.item():.4f}\")\n",
    "print(f\"Mean Squared Error: {mse.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def density_plot(glm_zeta, net_zeta, gene_id):\n",
    "    sns.kdeplot(x=glm_zeta, y=net_zeta, fill=True, cmap=\"Blues\")\n",
    "            \n",
    "    plt.xlim([min(glm_zeta), max(glm_zeta)])\n",
    "    plt.ylim([min(net_zeta), max(net_zeta)])\n",
    "\n",
    "\n",
    "    plt.xlabel('GLM Elongation Rate')\n",
    "    plt.ylabel('Neural Net Elongation Rate')\n",
    "    plt.title(gene_id)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scatterplot(net_zeta, glm_zeta, gene_id):\n",
    "    indices = range(len(glm_zeta))\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(10, 5))\n",
    "    \n",
    "    min_val = min(min(net_zeta), min(glm_zeta))\n",
    "    max_val = max(max(net_zeta), max(glm_zeta))\n",
    "\n",
    "    plt.xlim(min_val, max_val)\n",
    "    plt.ylim(min_val, max_val)\n",
    "    \n",
    "    ax.scatter(net_zeta, glm_zeta, s=5)\n",
    "    \n",
    "    ax.set_title(gene_id)\n",
    "    ax.set_xlabel('Neural Net Zeta')\n",
    "    ax.set_ylabel('GLM Zeta')\n",
    "    ax.legend()\n",
    "\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_batches2 = create_batches(test_data.groupby('GeneId'), batch_size=2000)\n",
    "\n",
    "test_dataset2 = GeneDataset(test_batches2)\n",
    "tstdl2 = DataLoader(test_dataset2, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_loss = 0\n",
    "loss_fn = CustomLoss()\n",
    "for batch in tstdl2:\n",
    "    gene_id = batch['GeneId'][0]\n",
    "    model.eval()\n",
    "    #print(\"number of samples: \" + str(len(batch)))\n",
    "\n",
    "    with torch.no_grad():\n",
    "        y_inputs = batch['Y_ji'].to(device)\n",
    "        rho_ji = model(y_inputs)\n",
    "        loss = loss_fn(batch['X_ji'], batch['C_j'], rho_ji)\n",
    "    \n",
    "    total_loss += loss.item()\n",
    "\n",
    "    glm_zeta = batch['Z_ji'][0]\n",
    "    # convert log(Z) outputs to Z\n",
    "    net_zeta = torch.exp(rho_ji.cpu().squeeze())\n",
    "        \n",
    "    #density_plot(glm_zeta, net_zeta, gene_id)\n",
    "        \n",
    "    #scatterplot(predicted_zeta, simulated_zeta, gene_id)\n",
    "        \n",
    "    #plot_data(glm_zeta, net_zeta)\n",
    "        \n",
    "print(total_loss / len(tstdl2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 10))\n",
    "\n",
    "sns.scatterplot(x=glm_kappa, y=weights[0])\n",
    "\n",
    "for i in range(len(glm_kappa)):\n",
    "    plt.text(glm_kappa[i], weights[0][i], feature_names[i], fontsize=13, ha='right', va='top')\n",
    "plt.xlabel('GLM Weights')\n",
    "plt.ylabel('Neural Net Weights')\n",
    "\n",
    "max_val = max(np.max(glm_kappa), np.max(weights[0])) + 0.04\n",
    "min_val = min(np.min(glm_kappa), np.min(weights[0])) - 0.04\n",
    "\n",
    "plt.xlim(max_val, min_val)\n",
    "plt.ylim(max_val, min_val)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# profiling code\n",
    "\"\"\"\n",
    "def print_profiler_results(profiler):\n",
    "    print(profiler.key_averages().table(sort_by=\"self_cpu_time_total\", row_limit=10))\n",
    "\n",
    "with torch.profiler.profile(\n",
    "    activities=[torch.profiler.ProfilerActivity.CPU, torch.profiler.ProfilerActivity.CUDA],\n",
    "    schedule=torch.profiler.schedule(wait=1, warmup=1, active=3),\n",
    "    on_trace_ready=print_profiler_results,\n",
    "    record_shapes=True,\n",
    "    profile_memory=True\n",
    ") as profiler:\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f'Epoch {epoch+1}')\n",
    "        model.train()\n",
    "        trndl = DataLoader(train_set, batch_size=batch_size, num_workers=7, shuffle=False, pin_memory=True)\n",
    "        for i, batch in enumerate(trndl):\n",
    "            optimizer.zero_grad()\n",
    "            Y_ji_batch = batch['Y_ji'].to(device)\n",
    "            X_ji_batch = batch['X_ji'].to(device)\n",
    "            C_j_batch = batch['C_j'].to(device)\n",
    "            outputs = model(Y_ji_batch)\n",
    "            loss = loss_fn(X_ji_batch, C_j_batch, outputs)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            loss_hist_train[epoch] += loss.item()\n",
    "            profiler.step()\n",
    "        loss_hist_train[epoch] /= len(trndl)\n",
    "        del trndl\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Anaconda3 (Python 3.7.6)",
   "language": "python",
   "name": "anaconda3_2020.02"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
