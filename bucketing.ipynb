{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Testing/Debugging File \"\"\"\n",
    "\n",
    "\"\"\"\n",
    "Restart kernel after running\n",
    "Only need to run once\n",
    "\"\"\"\n",
    "!pip install scikit-learn matplotlib seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.utils.data as td\n",
    "import torch.nn as nn\n",
    "import torch.nn.init as init\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torch.utils.data import DataLoader, Dataset, Subset, Sampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seqnames                         1\n",
      "start                      1002760\n",
      "end                        1002760\n",
      "strand                           +\n",
      "ensembl_gene_id    ENSG00000187608\n",
      "score                          0.0\n",
      "ctcf                      -0.07771\n",
      "h4k20me1                 -0.429997\n",
      "h3k79me2                   -0.2804\n",
      "h3k4me1                  -0.217665\n",
      "h3k9me3                  -0.333359\n",
      "h3k36me3                 -0.801406\n",
      "sj5                      -0.039619\n",
      "sj3                      -0.059131\n",
      "rpts                     -0.187111\n",
      "wgbs                           0.0\n",
      "lambda_alphaj             0.026377\n",
      "zeta                      1.133344\n",
      "A                                0\n",
      "T                                0\n",
      "G                                1\n",
      "C                                0\n",
      "dataset                      train\n",
      "Name: 0, dtype: object\n",
      "['ctcf' 'h4k20me1' 'h3k79me2' 'h3k4me1' 'h3k9me3' 'h3k36me3' 'sj5' 'sj3'\n",
      " 'rpts' 'wgbs']\n",
      "Number of Samples: 136927782\n",
      "Number of Features: 10\n"
     ]
    }
   ],
   "source": [
    "filename = './data/k562_datasets.pkl'\n",
    "\n",
    "with open(filename, 'rb') as file:\n",
    "    combined_datasets = pickle.load(file)\n",
    "\n",
    "train_data = combined_datasets['train']\n",
    "valid_data = combined_datasets['valid']\n",
    "test_data = combined_datasets['test']\n",
    "\n",
    "print(train_data.iloc[0])\n",
    "\n",
    "column_names = np.array(train_data.columns)\n",
    "feature_names = column_names[6:16]\n",
    "num_features = len(feature_names)\n",
    "print(feature_names)\n",
    "num_samples = train_data.shape[0]\n",
    "nucleotides = ['A', 'T', 'G', 'C']\n",
    "num_seq_features = 4\n",
    "\n",
    "print(\"Number of Samples: \" + str(num_samples))\n",
    "print(\"Number of Features: \" + str(num_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA (GPU support) is available: True\n",
      "Number of GPUs available: 1\n"
     ]
    }
   ],
   "source": [
    "cuda_available = torch.cuda.is_available()\n",
    "print(\"CUDA (GPU support) is available:\", cuda_available)\n",
    "num_gpus = torch.cuda.device_count()\n",
    "print(\"Number of GPUs available:\", num_gpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GeneDataset(Dataset):\n",
    "    def __init__(self, dataframe):\n",
    "        self.dataframe = dataframe\n",
    "        self.gene_ids = dataframe['ensembl_gene_id'].unique()\n",
    "        self.genes = dataframe.groupby('ensembl_gene_id')\n",
    "        self.cache = {}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.gene_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        gene_id = self.gene_ids[idx]\n",
    "        gene = self.genes.get_group(gene_id)\n",
    "                \n",
    "        if gene_id in self.cache:\n",
    "            return self.cache[gene_id]\n",
    " \n",
    "        result = {\n",
    "            'GeneId': gene_id,\n",
    "            'Seq_Name': gene['seqnames'].iloc[0],\n",
    "            'Start': gene['start'],\n",
    "            'End': gene['end'],\n",
    "            'Strand': gene['strand'],\n",
    "            \n",
    "            # epigenomic features per gene j, site i\n",
    "            'Y_ji':  torch.tensor(gene[feature_names].values, dtype=torch.float64),\n",
    "            \n",
    "            # read counts per gene j, site i\n",
    "            'X_ji': torch.tensor(gene['score'].values, dtype=torch.float64),\n",
    "            \n",
    "            # read depth * initiation rate values per gene j\n",
    "            'C_j': torch.tensor(gene['lambda_alphaj'].iloc[0], dtype=torch.float64),\n",
    "            \n",
    "            # GLM elongation rate predictions per gene j, site i\n",
    "            'Z_ji': torch.tensor(gene['zeta'].values, dtype=torch.float64),\n",
    "            \n",
    "            # one-hot encoded sequences\n",
    "            'N_ji': torch.tensor(gene[nucleotides].values, dtype=torch.float64), \n",
    "            'Length': len(gene)\n",
    "        }\n",
    "    \n",
    "        self.cache[gene_id] = result\n",
    "\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import BatchSampler\n",
    "\n",
    "class GeneBatchSampler(Sampler):\n",
    "    def __init__(self, dataset, batch_size, bucket_size):\n",
    "        self.dataset = dataset\n",
    "        self.batch_size = batch_size\n",
    "        self.bucket_size = bucket_size\n",
    "\n",
    "        lengths = torch.tensor([dataset[i]['Length'] for i in range(len(dataset))])\n",
    "        max_length = lengths.max().item()\n",
    "        min_length = lengths.min().item()\n",
    "        self.n_buckets = ((max_length - min_length) // bucket_size) + 1\n",
    "        self.boundaries = torch.arange(min_length, max_length + bucket_size, step=bucket_size)\n",
    "        self.bucket_indices = torch.bucketize(lengths, self.boundaries, right=True)\n",
    "\n",
    "        # Efficient grouping of indices into buckets using PyTorch operations\n",
    "        self.buckets = [torch.where(self.bucket_indices == i + 1)[0].tolist() for i in range(self.n_buckets)]\n",
    "\n",
    "    def __iter__(self):\n",
    "        for bucket in self.buckets:\n",
    "            for batch in BatchSampler(torch.utils.data.SubsetRandomSampler(bucket), self.batch_size, drop_last=False):\n",
    "                yield batch\n",
    "\n",
    "    # calculate number of batches created\n",
    "    def __len__(self):\n",
    "        # Include partial batch in calculation\n",
    "        return sum((len(bucket) + self.batch_size - 1) // self.batch_size for bucket in self.buckets)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "def collate_fn(batch):\n",
    "    GeneIds, Seq_Names, Start, End, Strand, C_j, Lengths = zip(*[(item['GeneId'], item['Seq_Name'], item['Start'], item['End'], item['Strand'], item['C_j'], item['Length']) for item in batch])\n",
    "    \n",
    "    Y_ji = pad_sequence([item['Y_ji'] for item in batch], batch_first=True, padding_value=0.0)\n",
    "    X_ji = pad_sequence([item['X_ji'] for item in batch], batch_first=True, padding_value=0.0)\n",
    "    Z_ji = pad_sequence([item['Z_ji'] for item in batch], batch_first=True, padding_value=1.0)\n",
    "    N_ji = pad_sequence([item['N_ji'] for item in batch], batch_first=True, padding_value=-1)\n",
    "    \n",
    "    longest_seq_length = torch.arange(X_ji.size(1)).unsqueeze(0)\n",
    "    seq_lengths = torch.tensor(Lengths).unsqueeze(-1) \n",
    "    mask = longest_seq_length < seq_lengths\n",
    "    \n",
    "    return {\n",
    "        'GeneId': GeneIds,\n",
    "        'Seq_Name': Seq_Names,\n",
    "        'Start': Start,\n",
    "        'End': End,\n",
    "        'Strand': Strand,\n",
    "        'Y_ji': Y_ji,\n",
    "        'X_ji': X_ji,\n",
    "        'C_j': torch.stack(C_j).unsqueeze(1),\n",
    "        'Z_ji': Z_ji,\n",
    "        'N_ji': N_ji,\n",
    "        'Mask': mask,\n",
    "        'Length': len(X_ji[0])\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dataset(data, batch_size, bucket_size):\n",
    "    dataset = GeneDataset(data)\n",
    "    batch_sampler = GeneBatchSampler(dataset, batch_size, bucket_size)\n",
    "    loader = DataLoader(dataset, batch_sampler=batch_sampler, num_workers=7, collate_fn=collate_fn)#, pin_memory=True)\n",
    "    return loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math \n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def build_model(model_type, num_lstm_layers, bidirectional, hidden_layer_sizes, activation_func,\n",
    "                filter_size, pool_type, pool_size, dropout, window_size, weight_init):\n",
    "    \n",
    "    class EpigeneticLinearModel(nn.Module):\n",
    "        def __init__(self, input_size):\n",
    "            super(LinearModel, self).__init__()\n",
    "            self.name = \"linear\"\n",
    "            self.linear = nn.Linear(input_size, 1, bias=False)\n",
    "\n",
    "        def forward(self, Y_ji):\n",
    "            x = self.linear(Y_ji)\n",
    "            return x.squeeze(-1)\n",
    "    \n",
    "    class LinearModel(nn.Module):\n",
    "        def __init__(self, input_size):\n",
    "            super(LinearModel, self).__init__()\n",
    "            self.name = \"linear\"\n",
    "            self.linear = nn.Linear(input_size, 1, bias=False)\n",
    "\n",
    "        def forward(self, Y_ji, N_ji):\n",
    "            x = torch.cat((Y_ji, N_ji), axis=-1)\n",
    "            x = self.linear(x)\n",
    "            return x.squeeze(-1)\n",
    "    \n",
    "    class LSTMModel(nn.Module):\n",
    "        def __init__(self, input_size, hidden_layer_sizes, output_size, num_layers, bidirectional):\n",
    "            super(LSTMModel, self).__init__()\n",
    "            self.name = \"lstm\"\n",
    "            self.lstm = nn.LSTM(input_size, hidden_layer_sizes[0], num_layers, bidirectional=bidirectional, batch_first=True)\n",
    "            self.bidirectional_linear = nn.Linear(2 * hidden_layer_sizes[0], output_size)\n",
    "            self.linear = nn.Linear(hidden_layer_sizes[0], output_size)\n",
    "            self.bidirectional = bidirectional\n",
    "\n",
    "        def forward(self, Y_ji, N_ji):\n",
    "            x = torch.cat((Y_ji, N_ji), axis=-1)\n",
    "            x, _ = self.lstm(x)\n",
    "            if self.bidirectional:\n",
    "                x = self.bidirectional_linear(x)\n",
    "            else:\n",
    "                x = self.linear(x)\n",
    "            return x.squeeze(-1)\n",
    "        \n",
    "    class CNN(nn.Module):\n",
    "        def __init__(self, num_features, hidden_layer_sizes, filter_size, dropout):\n",
    "            super(CNN, self).__init__()\n",
    "            self.name = \"cnn\"\n",
    "\n",
    "            # Calculate padding to maintain sequence length\n",
    "            padding = filter_size // 2  # Assuming filter_size is odd\n",
    "\n",
    "            self.y_convs = nn.ModuleList()\n",
    "            y_in_channels = num_features\n",
    "            self.n_convs = nn.ModuleList()\n",
    "            n_in_channels = num_seq_features\n",
    "\n",
    "            for out_channels in hidden_layer_sizes:\n",
    "                self.y_convs.append(\n",
    "                    nn.Conv1d(y_in_channels, out_channels, filter_size, stride=1, padding=padding)\n",
    "                )\n",
    "                y_in_channels = out_channels\n",
    "                self.n_convs.append(\n",
    "                    nn.Conv1d(n_in_channels, out_channels, filter_size, stride=1, padding=padding)\n",
    "                )\n",
    "                n_in_channels = out_channels\n",
    "\n",
    "            self.relu = nn.ReLU()\n",
    "            self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "            # Final convolutional layer to map to a single output channel\n",
    "            # Since the output needs to be (batch_size, seq_len), we map the final features to 1\n",
    "            self.final_conv = nn.Conv1d(hidden_layer_sizes[-1] * 2, 1, 1)  # 1x1 convolution\n",
    "\n",
    "        def forward(self, Y_ji, N_ji):\n",
    "            #x = torch.cat((Y_ji, N_ji), axis=-1)\n",
    "            Y_ji = Y_ji.permute(0, 2, 1)  # Change x to [batch, features, seq_len] to match Conv1d input\n",
    "            N_ji = N_ji.permute(0, 2, 1)\n",
    "            for conv in self.y_convs:\n",
    "                Y_ji = conv(Y_ji)\n",
    "                Y_ji = self.relu(Y_ji)\n",
    "                Y_ji = self.dropout(Y_ji)\n",
    "            \n",
    "            for conv in self.n_convs:\n",
    "                N_ji = conv(N_ji)\n",
    "                N_ji = self.relu(N_ji)\n",
    "                N_ji = self.dropout(N_ji)\n",
    "\n",
    "            x = torch.cat((Y_ji, N_ji), 1)\n",
    "            x = self.final_conv(x)\n",
    "            # After final_conv, x is of shape [batch, 1, seq_len]\n",
    "            x = x.squeeze(1)  # Remove the middle dimension to get [batch, seq_len]\n",
    "\n",
    "            return x\n",
    "    \n",
    "    if model_type == 'lstm':\n",
    "        model = LSTMModel(num_features + num_seq_features, hidden_layer_sizes, 1, num_lstm_layers, bidirectional)\n",
    "    elif model_type == 'epigenetic_linear':\n",
    "        model = EpigeneticLinearModel(num_features)\n",
    "    elif model_type == 'linear':\n",
    "        model = LinearModel(num_features + num_seq_features)\n",
    "    elif model_type == 'cnn':\n",
    "        model = CNN(num_features, hidden_layer_sizes, filter_size, dropout)\n",
    "        \n",
    "    \n",
    "    if cuda_available:\n",
    "        if num_gpus > 1:\n",
    "            print(\"Using\", num_gpus, \"GPUs\")\n",
    "            model = torch.nn.DataParallel(model)\n",
    "        model = model.to('cuda')\n",
    "\n",
    "    print(model)\n",
    "\n",
    "    first_param_device = next(model.parameters()).device\n",
    "    print(\"Model is on device:\", first_param_device)\n",
    "    \n",
    "    # expected weights are close to 0 which is why 0 initializing weights converges much quicker\n",
    "    if weight_init == 'zero':\n",
    "        with torch.no_grad():\n",
    "            for param in model.parameters():\n",
    "                param.zero_()\n",
    "    \n",
    "    model.double()\n",
    "\n",
    "    return model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_optimizer(network, optimizer, learning_rate, momentum):\n",
    "    if optimizer == \"sgd\":\n",
    "        optimizer = optim.SGD(network.parameters(),\n",
    "                              lr=learning_rate, momentum=momentum)\n",
    "        \n",
    "    # Adam optimizer adapts the learning rate for each parameter individually\n",
    "    elif optimizer == \"adam\":\n",
    "        optimizer = optim.Adam(network.parameters(),\n",
    "                               lr=learning_rate)\n",
    "    return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def valid_epoch(model, loader, loss_fn):\n",
    "    model.eval()\n",
    "    total_neural_net_loss = 0\n",
    "    total_glm_loss = 0\n",
    "    neural_net_zeta = []\n",
    "    glm_zeta = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for idx, batch in enumerate(loader):\n",
    "            Y_ji_batch = batch['Y_ji'].to(device)\n",
    "            N_ji_batch = batch['N_ji'].to(device)\n",
    "            X_ji_batch = batch['X_ji'].to(device)\n",
    "            C_j_batch = batch['C_j'].to(device)\n",
    "            Z_ji_batch = batch['Z_ji'].to(device)\n",
    "            mask = batch['Mask'].to(device)\n",
    "            \n",
    "            outputs = model(Y_ji_batch, N_ji_batch)\n",
    "            \n",
    "            neural_net_loss = loss_fn(X_ji_batch, C_j_batch, outputs, mask)\n",
    "            glm_loss = loss_fn(X_ji_batch, C_j_batch, torch.log(Z_ji_batch), mask)\n",
    "\n",
    "            total_neural_net_loss +=  neural_net_loss.item()\n",
    "            total_glm_loss += glm_loss.item()\n",
    "            \n",
    "            # store all predictions in list\n",
    "            masked_net_outputs = torch.exp(outputs.cpu()[0]) * mask.cpu()\n",
    "            masked_net_outputs_list = masked_net_outputs.flatten().tolist()\n",
    "            neural_net_zeta.extend(masked_net_outputs_list)\n",
    "            masked_Z_ji = batch['Z_ji'][0] * mask.cpu()\n",
    "            masked_Z_ji_list = masked_Z_ji.flatten().tolist()\n",
    "            glm_zeta.extend(masked_Z_ji_list)\n",
    "    \n",
    "    # calculate average loss across all batches\n",
    "    avg_neural_net_loss = total_neural_net_loss / len(loader)\n",
    "    avg_glm_loss = total_glm_loss / len(loader)\n",
    "    \n",
    "    return avg_neural_net_loss, avg_glm_loss, neural_net_zeta, glm_zeta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, loader, optimizer, loss_fn):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for idx, batch in enumerate(loader):\n",
    "        optimizer.zero_grad()\n",
    "        Y_ji_batch = batch['Y_ji'].to(device) \n",
    "        X_ji_batch = batch['X_ji'].to(device)\n",
    "        C_j_batch = batch['C_j'].to(device)\n",
    "        N_ji_batch = batch['N_ji'].to(device)\n",
    "        mask = batch['Mask'].to(device)\n",
    "                \n",
    "        outputs = model(Y_ji_batch, N_ji_batch)\n",
    "        \n",
    "        loss = loss_fn(X_ji_batch, C_j_batch, outputs, mask)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # calculate average loss across all batches\n",
    "        total_loss += loss.item()\n",
    "    avg_train_loss = total_loss / len(loader)\n",
    "    \n",
    "    return avg_train_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CustomLoss, self).__init__()\n",
    "\n",
    "    def forward(self, X_ji, C_j, rho_ji, mask):\n",
    "        loss = (X_ji * rho_ji + C_j * torch.exp(-rho_ji) - X_ji * torch.log(C_j)) * mask\n",
    "        \n",
    "        loss = loss.sum()\n",
    "        #non_padded_elements = mask.sum()\n",
    "        \n",
    "        #avg_loss = loss / non_padded_elements\n",
    "            \n",
    "        return loss#avg_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model configs\n",
    "model_type = 'cnn'\n",
    "weight_init = None\n",
    "hidden_layer_sizes = [16,32]#[16]\n",
    "activation_func = 'relu'\n",
    "pool_type = 'max'\n",
    "\n",
    "# lstm configs\n",
    "num_lstm_layers = 1\n",
    "bidirectional = False\n",
    "\n",
    "# deep chrome cnn configs\n",
    "filter_size = 7\n",
    "pool_size = 5\n",
    "dropout = 0.5\n",
    "\n",
    "# dataset configs\n",
    "use_sliding_window = True\n",
    "window_size = 1000\n",
    "\n",
    "# optimizer configs\n",
    "learning_rate = 1e-4\n",
    "optimizer_type = 'adam'\n",
    "momentum = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics(neural_net_zeta, glm_zeta):\n",
    "    neural_net_zeta = torch.tensor(neural_net_zeta, dtype=torch.float64)\n",
    "    glm_zeta = torch.tensor(glm_zeta, dtype=torch.float64)\n",
    "\n",
    "    mse = F.mse_loss(neural_net_zeta, glm_zeta)\n",
    "    mae = F.l1_loss(neural_net_zeta, glm_zeta)\n",
    "    correlation_coefficient = np.corrcoef(neural_net_zeta, glm_zeta)[0, 1]\n",
    "\n",
    "    print(f\"MSE: {mse.item()}\")\n",
    "    print(f\"MAE: {mae.item()}\")\n",
    "    print(\"Correlation Coefficient:\", correlation_coefficient)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "epochs = 20\n",
    "\n",
    "def train():\n",
    "    model = build_model(model_type, num_lstm_layers, bidirectional, hidden_layer_sizes, \n",
    "                        activation_func, filter_size, pool_type, pool_size, dropout, window_size, weight_init)\n",
    "    \n",
    "    train_loader = build_dataset(train_data, batch_size=10, bucket_size=2000)\n",
    "    valid_loader = build_dataset(valid_data, batch_size=10, bucket_size=2000)\n",
    "    \n",
    "    optimizer = build_optimizer(model, optimizer_type, learning_rate, momentum)\n",
    "    \n",
    "    loss_fn = CustomLoss()\n",
    "    # track loss curves\n",
    "    loss_neural_net_train = [0] * epochs\n",
    "    loss_neural_net_valid = [0] * epochs\n",
    "    loss_glm_valid = [0] * epochs\n",
    "    \n",
    "    # scheduler to reduce learning rate by half when new validation loss > old validation loss\n",
    "    old_train_loss = float('inf')\n",
    "    scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=1, verbose=True)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        print(f'Epoch {epoch+1}')\n",
    "        \n",
    "        train_loss = train_epoch(model, train_loader, optimizer, loss_fn)\n",
    "        loss_neural_net_train[epoch] = train_loss\n",
    "        print(f\"train loss: {train_loss: .4f}\")\n",
    "        \n",
    "        valid_neural_net_loss, valid_glm_loss, neural_net_zeta, glm_zeta = valid_epoch(model, valid_loader, loss_fn)\n",
    "        loss_neural_net_valid[epoch] = valid_neural_net_loss\n",
    "        loss_glm_valid[epoch] = valid_glm_loss\n",
    "        print(f\"valid neural net loss: {valid_neural_net_loss: .4f}\")\n",
    "        print(f\"valid glm loss: {valid_glm_loss: .4f}\")\n",
    "        \n",
    "        # compute metrics\n",
    "        mae = calculate_metrics(neural_net_zeta, glm_zeta)\n",
    "        \n",
    "        # reduce learning rate if new loss > old loss\n",
    "        if train_loss > old_train_loss:\n",
    "            optimizer.param_groups[0]['lr'] *= 0.5\n",
    "            print(f\"Reduced learning rate to {optimizer.param_groups[0]['lr']}\")\n",
    "        old_train_loss = train_loss\n",
    "        scheduler.step(train_loss)\n",
    "        \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/7033500.1.gpu.q/ipykernel_1976402/2115540189.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/7033500.1.gpu.q/ipykernel_1976402/2126732145.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     model = build_model(model_type, num_lstm_layers, bidirectional, hidden_layer_sizes, \n\u001b[0;32m----> 6\u001b[0;31m                         activation_func, filter_size, pool_type, pool_size, dropout, window_size, weight_init)\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mtrain_loader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbucket_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/7033500.1.gpu.q/ipykernel_1976402/191286972.py\u001b[0m in \u001b[0;36mbuild_model\u001b[0;34m(model_type, num_lstm_layers, bidirectional, hidden_layer_sizes, activation_func, filter_size, pool_type, pool_size, dropout, window_size, weight_init)\u001b[0m\n\u001b[1;32m    110\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Using\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_gpus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"GPUs\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m             \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataParallel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 112\u001b[0;31m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cuda'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mto\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    850\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_complex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    851\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 852\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    853\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    854\u001b[0m     def register_backward_hook(\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    528\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    529\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 530\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    531\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    532\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    528\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    529\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 530\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    531\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    532\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    550\u001b[0m                 \u001b[0;31m# `with torch.no_grad():`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    551\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 552\u001b[0;31m                     \u001b[0mparam_applied\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    553\u001b[0m                 \u001b[0mshould_use_set_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    554\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mshould_use_set_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mconvert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    848\u001b[0m                 return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None,\n\u001b[1;32m    849\u001b[0m                             non_blocking, memory_format=convert_to_format)\n\u001b[0;32m--> 850\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_complex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    851\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    852\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1."
     ]
    }
   ],
   "source": [
    "model = train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model parameters\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "filename = f\"models/LSTM_Model.pth\"\n",
    "torch.save(model.state_dict(), filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model state\n",
    "\n",
    "model = build_model(model_type, num_lstm_layers, bidirectional, hidden_layer_sizes, activation_func,\n",
    "                filter_size, pool_type, pool_size, dropout, window_size, weight_init)\n",
    "\n",
    "model.load_state_dict(torch.load(\"models/LSTM_Model.pth\", map_location=torch.device('cpu')))\n",
    "\n",
    "cuda_available = torch.cuda.is_available()\n",
    "print(\"CUDA (GPU support) is available:\", cuda_available)\n",
    "num_gpus = torch.cuda.device_count()\n",
    "print(\"Number of GPUs available:\", num_gpus)\n",
    "if cuda_available:\n",
    "    if num_gpus > 1:\n",
    "        print(\"Using\", num_gpus, \"GPUs\")\n",
    "        model = torch.nn.DataParallel(model)\n",
    "    model = model.to('cuda')\n",
    "\n",
    "first_param_device = next(model.parameters()).device\n",
    "print(\"Model is on device:\", first_param_device)\n",
    "\n",
    "model.double()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = model.linear.weight.data.cpu().numpy()\n",
    "combined = ', '.join([f'\"{s}\": {f}' for s, f in zip(feature_names, weights[0])])\n",
    "print(combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glm_kappa = [-0.0224536145637661, -0.094592589, -0.023815382, 0.030402922, -0.067234092, -0.032196914, -0.040911478, -0.018557168, -0.033545905, -0.051103287, -0.204434712, 0.015831043]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "GLM K\n",
    "\n",
    "* ctcf: -0.02\n",
    "* h3k36me3: -0.09\n",
    "* h3k4me1: -0.02\n",
    "* h3k79me2: +0.03\n",
    "* h3k9me1: -0.06\n",
    "* h3k9me3: -0.03\n",
    "* h4k20me1: -0.04\n",
    "* sj5: -0.02\n",
    "* sj3: -0.03\n",
    "* dms->stem-loop: -0.05\n",
    "* rpts->low-complex: +0.01\n",
    "* wgbs->DNAm: -0.2\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot loss curve\n",
    "\n",
    "epochs = range(1, len(loss_neural_net_train) + 1)\n",
    "plt.plot(epochs, loss_train, label='train_neural_net_loss')\n",
    "plt.plot(epochs, loss_neural_net_valid, label='valid_neural_net_loss')\n",
    "plt.plot(epochs, loss_glm_valid, label='valid_glm_loss')\n",
    "\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_data(glm_zeta, net_zeta):\n",
    "    indices = range(len(glm_zeta))\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(10, 5))\n",
    "    \n",
    "    ax.scatter(indices, net_zeta, color='blue', label='Neural Net Zeta', s=10, alpha=0.5)\n",
    "    ax.scatter(indices, glm_zeta, color='orange', label='GLM Zeta', s=10, alpha=0.5)\n",
    "    \n",
    "    ax.set_title('Neural Net vs GLM Elongation Rate')\n",
    "    ax.set_xlabel('Index')\n",
    "    ax.set_ylabel('Elongation Rate')\n",
    "    ax.legend()\n",
    "    \n",
    "    plt.ylim(0.5, 1.5)\n",
    "\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "\n",
    "tstdl = build_dataset(test_data, use_sliding_window, window_size)\n",
    "data_iter = iter(tstdl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute metrics for test dataset\n",
    "\n",
    "loss_fn = CustomLoss()\n",
    "\n",
    "total_net_loss = 0\n",
    "total_glm_loss = 0\n",
    "with torch.no_grad():\n",
    "    for batch in tstdl:\n",
    "        y_inputs = batch['Y_ji'].to(device)\n",
    "        X_ji = batch['X_ji'].to(device)\n",
    "        C_j = batch['C_j'].to(device)\n",
    "        lengths = batch['gene_length'].to(device)\n",
    "        Z_ji = batch['Z_ji'].to(device)\n",
    "        rho_ji = model(y_inputs)\n",
    "        net_loss = loss_fn(X_ji, C_j, rho_ji.squeeze(2), lengths)\n",
    "        glm_loss = loss_fn(X_ji, C_j, torch.log(Z_ji), lengths)\n",
    "        \n",
    "        total_net_loss += net_loss.item()\n",
    "        total_glm_loss += glm_loss.item()\n",
    "\n",
    "\n",
    "\n",
    "print(f\"Neural Net Loss: {total_net_loss/len(tstdl):.4f}\")\n",
    "print(f\"GLM Loss: {total_glm_loss/len(tstdl):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot for subset of genes in test dataset\n",
    "for i in range(0, 4):\n",
    "    inputs = next(data_iter) \n",
    "    print(\"number of samples: \" + str(len(inputs)))\n",
    "\n",
    "    with torch.no_grad():\n",
    "        y_inputs = inputs['Y_ji'].to(device)\n",
    "        rho_ji = model(y_inputs)\n",
    "\n",
    "    glm_zeta = inputs['Z_ji'][0]\n",
    "    # convert log(Z) outputs to Z\n",
    "    net_zeta = torch.exp(rho_ji.cpu().squeeze())\n",
    "    \n",
    "    plot_data(glm_zeta, net_zeta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute metrics for test dataset\n",
    "\n",
    "net_zeta = []\n",
    "glm_zeta = []\n",
    "with torch.no_grad():\n",
    "    for batch in tstdl:\n",
    "        y_inputs = batch['Y_ji'].to(device)\n",
    "        rho_ji = model(y_inputs)\n",
    "        # convert log(Z) outputs to Z\n",
    "        net_zeta.append(torch.exp(rho_ji.cpu()[0]))\n",
    "        glm_zeta.append(batch['Z_ji'][0])\n",
    "\n",
    "net_zeta = torch.cat(net_zeta, dim=0)\n",
    "glm_zeta = torch.cat(glm_zeta, dim=0)\n",
    "mae = F.l1_loss(net_zeta.squeeze(), glm_zeta)\n",
    "mse = F.mse_loss(net_zeta.squeeze(), glm_zeta)\n",
    "\n",
    "correlation_coefficient = np.corrcoef(glm_zeta, net_zeta.squeeze())[0, 1]\n",
    "print(\"Correlation Coefficient:\", correlation_coefficient)\n",
    "\n",
    "print(f\"Mean Absolute Error: {mae.item():.4f}\")\n",
    "print(f\"Mean Squared Error: {mse.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def density_plot(glm_zeta, net_zeta, gene_id):\n",
    "    sns.kdeplot(x=glm_zeta, y=net_zeta, fill=True, cmap=\"Blues\")\n",
    "            \n",
    "    plt.xlim([min(glm_zeta), max(glm_zeta)])\n",
    "    plt.ylim([min(net_zeta), max(net_zeta)])\n",
    "\n",
    "\n",
    "    plt.xlabel('GLM Elongation Rate')\n",
    "    plt.ylabel('Neural Net Elongation Rate')\n",
    "    plt.title(gene_id)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot for all genes in test dataset\n",
    "\n",
    "total_loss = 0\n",
    "loss_fn = CustomLoss()\n",
    "for batch in tstdl:\n",
    "    gene_id = batch['GeneId'][0]\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        y_inputs = batch['Y_ji'].to(device)\n",
    "        rho_ji = model(y_inputs)\n",
    "        \n",
    "    glm_zeta = batch['Z_ji']#[0]\n",
    "    # convert log(Z) outputs to Z\n",
    "    net_zeta = torch.exp(rho_ji.cpu().squeeze())\n",
    "        \n",
    "    density_plot(glm_zeta.flatten(), net_zeta.flatten(), gene_id)\n",
    "                \n",
    "    plot_data(glm_zeta.flatten(), net_zeta.flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot scatterplot of neural net weights and glm weights\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "\n",
    "sns.scatterplot(x=glm_kappa, y=weights[0])\n",
    "\n",
    "for i in range(len(glm_kappa)):\n",
    "    plt.text(glm_kappa[i], weights[0][i], feature_names[i], fontsize=13, ha='right', va='top')\n",
    "plt.xlabel('GLM Weights')\n",
    "plt.ylabel('Neural Net Weights')\n",
    "\n",
    "max_val = max(np.max(glm_kappa), np.max(weights[0])) + 0.04\n",
    "min_val = min(np.min(glm_kappa), np.min(weights[0])) - 0.04\n",
    "\n",
    "plt.xlim(max_val, min_val)\n",
    "plt.ylim(max_val, min_val)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Anaconda3 (Python 3.7.6)",
   "language": "python",
   "name": "anaconda3_2020.02"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
